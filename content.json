{"pages":[{"title":"","text":"google-site-verification: google1926343a7854156d.html","link":"/google1926343a7854156d.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"About Me","text":"Ｈello, 我是義洋 Frank Lin， 目前研究所主要方向以虛擬化技術與深度學習應用等，並持續關注kubeflow專案，嘗試於kubernetes進行分散式DL管理/訓練，以及檔案儲存系統、調度分配節點資源技術等。 平時積極參與社群相關活動，學習不同領域技術，並希望透過部落格文章記錄學習筆記，此地用來分享學習過程相關技術文章，歡迎大家對文章進行交流討論。 生活體悟 https://medium.com/@frank.yylin 閒暇之中的小興趣，習慣透過鏡頭記錄生活的每一刻 flickr.com/photos/100498847@N04","link":"/about/index.html"}],"posts":[{"title":"2018 年終回顧總結","text":"2018年 是一個重新思考人生階段的下一步。我每年自己都會透過回顧一遍今年完成的專案，看看過去任務筆記，但總是沒有完全實踐列出來的清單，經歷太多外務與繁忙的瑣事，才發現換過來苦了自己缺少非常多需要努力的地方。我從過去足跡中規劃好「年度目標」，希望更專注於學習，踏實逐步讓目標成真。 1. 成為掌舵者 kubernetes &amp; kubeflow由於實驗室研究方向規劃，從轉入虛擬化容器開始，希望進行DL分散式訓練研究，剛好學校有足夠的實體主機，而我因此開始接觸 Kubernetes。初期真的遇到非常多部署問題與環境因素，感謝從COSCUP2017認識的 kairen 大大，在學習上幫助我非常多，讓我能從不同面向學習解決問題，也透過參與cloud native taiwan user group 社群讓我體會到，獲得學習的成長，並有機會被邀請分享自己所學習的議題領域。 接觸 kubernetes 後發現Google整合一項專案kubeflow 為 Deep Learning 工作流更快速部署訓練流程，而開始研究後有機會透過分享議程與社群朋友交流，後續我更參加上海的 KubeCon 2018，並與kubeflow 社區 Github維護者們做互動，過程真的是是獲益非常多。 2. 國際論文報告經驗回顧今年6月底，參加釜山IEEE/ASIC (SNPD2018)會議論文報告，完成去年我在ITRI實習，進行Deep Learning benchmark相關實驗研究，最終研究成果成功投稿上至SNPD2018 Conference發表Paper。 第一次參與國際研討會，大會主題圍繞著人工智慧、平行分散式、大數據與資訊安全等，很意外這場並不是很大，各廳小型Room，輪著報告者演講，首場報告非常緊張，到會場還差一點遲到，最終用努力趕出來的英文講稿成功完成報告，英文需要多努力！ 很榮幸這次有實驗室兩位夥伴一同陪伴，完成國際論文報告初體驗，但碩士論文還是要繼續努力✈️，也很難得有機會走踏釜山行。 3. COSCUP 昔日組員竟然成為攝影組組長任職組長扎實的體悟籌備 COSCUP2018 經驗，最終「COSCUP x GNOME.Asia x openSUSE.Asia 2018」總出圖3832張 】，並與攝影組23位志工組員，經過「#大人物攝影 #會前記者會 #場佈 #Welcome Party | 活動當日 #15軌議程 #BOF #即時出圖」奮鬥淬煉，真的認為是一個很難得並吸取多項經驗成長的一個歷程。 ▍疑！當上組長 還記得我在看攝影展時遇到 Huang Tooth，多次聊聊後就默默接下這重責大任，清楚知道後續這整年籌備想必是需要經歷不同凡響的挑戰，但我踏出行動嘗試這次的挑戰，還是要非常感謝牙牙過程中不斷協助。 ▍籌備Conference 除了大學經歷多個團隊經驗中，對我要馬上參與籌備千人Conference活動籌備是很大的挑戰。以往自己都是擔任默默的組員，但是這次因為有了責任，因此有著不同心態，很感謝總招 Li-Han Nfsnfs Chen與核心組長群們\b\b大家一年之中，經歷多次籌會準備與努力。其中很大體會「帶團隊\b過程中，隨時與跨組各Sync彼此進度，並且與自己組員回報討論規劃，並持續跟緊最新資訊是很重要的」，很感謝自己當遇到問題，有勇敢提問的精神！過程其實很擔心是否有遺漏重要的事情。 ▍最後，感謝攝影組 攝影組團隊給了我這次團隊經驗，很辛苦大家這次協助，由於人手關係，比較不好意思大家要花比往年更多的體力趕場，其中還有CPG好手強力的支援，給予我很大的出圖協助，還有超級感謝活動中協助 魏筠芳經理。 大家腦中最難忘的可能是就是我們的班表「15軌」填坑填寫到快昏倒，要印到B4才能清楚看到各軌狀況，會再思考怎麼做會更好，也很感謝這次招募近來的新人，第一次就參與不一樣的挑戰，希望你們有對「攝影組」留下很好的回憶。 #COSCUP2018 攝影組 #COSCUP2018 總招與各組組長 4. 透過撰寫技術文章筆記，來記錄自己學習的內容社群影響發現原來有一種企劃「一週一文章」讓我有動力下定決心要來寫文章，而從9月開始終於搞定自己的部落格定位，開始撰寫文章，從學習中開始整理筆記並彙整文章來分享，但原本是想撰寫在Medium分享文章，但是沒有用 Markdown語法 格式撰寫起來還是太麻煩，但是會思考後續應該怎麼區分文章類別與行態。 5. 從社群中交流/分享/學習感謝自己有機會，能持續參與台灣各大社群，最重要是 Cloud Native Taiwan User Group社群，當我聽到一句話「一個人一天只有24小時，如果每個人都能交流分享，獲得的就能比自己努力還多」深刻感受社群交流學習的力量！，因此有機會聆聽不同人的開發經驗與研究領域。而我有機會從《SDN x Cloud Native Meetup #4 》第一次站上舞台分享交流Kubeflow研究，讓自己從社群的「聽眾身分」轉換為社群中的「演講者」，後續更有機會踏上與社群大大合作在廣宣學堂分享付費課程，後續還分享KubeCon參與經驗，很感謝有機會從社群交流與體悟，明年繼續努力有機會就分享。 第一次社群分享的簡報：[slide]Kubeflow Machine Learning Toolkit for Kubernetes (SDN x Cloud Native Meetup #4) 與 Kairen大大合作-廣宣學堂：[slide]Kubeflow:容器機器學習平台實戰 LT短講5分鐘-分享KubeCon上海2018：[slide]kubeCon經驗分享-搭上KubeCon大船遇見 Kubeflow Community大牛們 6. 人生首場 KubeCon&amp;CloudNativeCon China 2018很榮幸今年有機會參與首屆在上海的KubeCon-China2018年度Conference，能與各項kubernetes開源專案維護者互動交流，是非常榮幸的一件事，此趟旅行還認識幾位kubeflow社區 maintainer兼研究生 @gaocegege與 @ChanYiLin讓我重新抓住碩論方向與核心，已計畫有時間就前往學習交流。 # 結論趕在最後一天拼完回顧真的是不容易。2018年經過不同身份與體悟，讓我達成了不同的里程碑，明年2019持續朝著自己規劃的目標前進，堅持每一步，碩論加油！","link":"/2018/12/31/2018-retrospect-prospect/"},{"title":"安裝NVIDIA顯卡驅動時可能遇到的問題","text":"本篇主要介紹如何解決Ubuntu環境安裝NVIDIA GPU Driver後，會出現「循環登入的問題」，當裝完驅動重啟後，輸入登錄密碼之後，桌面一閃就退回到登錄界面了，然後就陷入到了輸入密碼登錄、彈出的循環，這時就需要進行顯卡驅動程序的卸載重裝。 卸載方法如下:首先在登錄介面進入到Linux的shell ie tty model，同時按下Ctrl+Alt+F1 （F1~F6其中一個就可以），然後輸入使用者輸入使用者帳號密碼，成功進入到shell，開始卸載NVIDIA驅動： 卸載乾淨所有安裝過的nvidia驅動12$ sudo apt-get remove --purge nvidia-*$ sudo apt-get autoremove 並透過直接下驅動解除安裝 1$ sudo nvidia-uninstall 再次檢查，透過 dpkg 檢查是否有非透過 apt-get 安裝的需要移除12$ sudo dpkg -l 'nvidia'$ sudo dpkg --remove nvidia-{name} 完成步驟後重啟系統 1$ reboot 重新安裝 NVIDIA Driver官網連結：NVIDIA Driver 下載連結 這邊測試環境GPU 為 GTX 1080 再次透過登入介面Ctrl+Alt+F1切換到tty1執行 1234567# 關閉X server$ sudo service lightdm stop $ sudo ./NVIDIA-Linux-x86_64-410.78.run -no-x-check -no-nouveau-check -no-opengl-files# -no-x-check 安裝時關閉X Server ;# -no-nouveau-check 安装驅動時禁用Nouveau# -no-opengl-files 安装時只裝驅動檔案，不安裝Opengl 安裝好後即，再次重啟電腦reboot 1sudo service lightdm restart 重啟後即可透過UI介面登入環境，就可以解決循環登入的問題。 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：123456789101112131415161718$ sudo nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.87 Driver Version: 390.87 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 108... Off | 00000000:02:00.0 On | N/A || 26% 40C P8 16W / 250W | 328MiB / 11173MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 837 C python 157MiB || 0 1250 G /usr/lib/xorg/Xorg 159MiB |+-----------------------------------------------------------------------------+ 補充：安裝NVIDIA顯卡驅動是可能遇到的問題：出現An error occurred while p erforming the step : &quot; Building kernel modules &quot;這個問題: 問題原因 Linix系統的內核是在不斷更新的，而安裝的NVIDIA驅動是之前下載好的，沒有更新，因此安裝過程中無法創建內核。 解決方法 這時候從NVIDIA下載驅動對應新版本的驅動，並安裝執行以上步驟即可。 相關狀況參考資料 |ubuntu 16.04 循环登录Ubuntu 16.04 安裝 CUDA + cuDNN + nvidia driver 的踩雷心得 (非安裝步驟詳解)安裝 NVIDIA Docker 2 來讓容器使用 GPU","link":"/2018/12/01/install-nvidia-driver-problem/"},{"title":"人生首場 KubeCon&CloudNativeCon China 2018","text":"此篇主要分享參與KubeCon China2018會後心得，剛好透過社群分享再次的回顧自己聆聽議程與參與國際會議論壇的體悟。 很榮幸今年有機會參與首屆在上海的KubeCon-China2018年度Conference，難得在最後的學生時期有機會踏入國際會議論壇，而此次我主要目的聆聽議程為Deep Learning ＆ Scheduling 議程為主，最關注的還是我目前研究方向「kubeflow」後續發展與有機會與社群的contributors &amp; maintainers互動，真的非常興奮。 會後剛好來到11月社群Cloud Native Taiwan User Group定期Meetup時間，感謝有機會被邀請並以Lightning talk方式分享，滿滿心得剛好有機會透過這樣的方式分享。 btw. 剛好留意Kubernetes發布KubeCon China 2018 Workshop 活動kubernetes發佈介紹：New Contributor Workshop Shanghai 剛好會場攝影師有拍到我和一同參與的夥伴XD 簡報分享這邊直接放上，我在Cloud Native Taiwan User Group Meetup 12分享的內容，後續有時間會另外補上自己更深入的體驗與交流分享。 kubeCon經驗分享-搭上KubeCon大船遇見 Kubeflow Community大牛們","link":"/2018/12/14/kubecon-china-first/"},{"title":"Parameter Server 學習筆記","text":"本篇文章主要介紹分散式概念，對於參數伺服器(Parameter Server)架構釐清，並與主流常見 機器學習(Machine Learning) 框架使用分散式運算實際應用場景探討。 概念：在過去十年中深度學習技術已成功應用於許多領域，例如語音辨識、影像分析、物件偵測等。 大規模數據上跑機器學習任務是過去至今，資料中心系統架構面臨的主要挑戰之一，要如何有效解決運算需求成長，能否承載大型神經網絡與巨量數據資料資源請求挑戰： 要訪問這些巨量的參數超過單個機器容納的頻寬能力(Bandwidth)支持 很多ML算法是序列性的，同步過程會影響性能損失 在分佈式中，容錯能力是非常重要的。很多情況下，算法都是部署到雲環境中的（這種環境下，機器是不可靠的，並且job也是有可能被搶占的）； 為了解決這些問題，大神們提出了一種新的架構- Parameter Server(簡稱PS)。 Paramter Server架構設計:1. Paramter Server 整體架構PS架構主要包括兩大部分，一個參數伺服器群（Parameter Server group）和多個工作(Worker)群: 在PS中，每個Parameter實際上都只負責分到的部分參數（Servers共同維持一個全局的共享參數），而每個Worker 也只分到部分數據和處理任務 每個子節點都只維護自己分配到的參數，自己部分更新之後，將計算結果（例如：梯度）傳回到主節點，進行全局的更新（比如All reduce平均操作之類的），主節點再向子節點傳送新的參數； Parameter Server 架構圖: 架構概念解釋： Server節點可以跟其他Server節點溝通，每個Server負責自己分到的參數，Server group共同維持所有參數的更新。 Server manager node 負責維護一些元數據(metadata)的一致性，比如各個節點的狀態，參數的分配情況等。 Worker節點之間無交流，只能跟自己對應的Server進行溝通。 每個Worker group有一個 task scheduler，負責向 Worker 分配任務，並且監控 Worker的運行情況。當有新的Worker加入或者退出，task scheduler負責重新分配任務。 training data 會被 split 多個部分，一個 Worker 在本地將一部分訓練數據存儲在本地統計數據中。 範例: Parameter Server 示意圖 上圖中，每個子節點都只維護自己分配到的參數（圖中的黑色），自己部分更新之後，將計算結果（例如：梯度）傳回到主節點，進行全局的更新（比如平均操作之類的），主節點再向子節點傳送新的參數。 Ref: [Parameter Server 详解-仙道菜] 2. Parameter Server 架構優勢1. 高效溝通(Efficient communication) 非同步(Asynchronous)訓鍊，使得計算不會被拖累，不需要停下來等一些機器(Worker)執行完一個iteration，這大幅減少延遲時間。 2. 彈性一致性模型(Flexible consistency models) 允許用戶自定義一致性: 比如Sequential（序列式的，即完全同步）、Eventual（完全不同步的）和Bounded Delay（有條件的限制，可以允許用戶在限制的次數內異步，比如限制為3 次，如果某個節點已經超前了其他節點四次迭代了，那麼要停下等待同步。在整個訓練的過程中，Delay 可能是動態的，即 delay 的參數在訓練過程中可以變大或變小）。 3. 擴展性強 (Elastic Scalability) 使用了一個分佈式 hash 表使得新的server 節點可以隨時動態的插入到集合中；因此，新增一個節點不需要重新運行系統。 4. 錯誤容忍 (Fault Tolerance and Durability) 我們都知道，節點故障是不可避免的，特別是在大規模商用伺服器集群中。從非災難性機器故障中恢復，只需要1秒，而且不需要中斷計算。Vector clocks保證了經歷故障之後還是能運行良好。 5. 易用性 (Ease of Use) 全局共享的參數可以被表示成各種形式：vector、matrices或者相應的 sparse 類型，這方便機器學習算法的開發。並且提供的線性代數數據類型都具有高性能的多線程庫。 3. Parameter Server 關鍵概念1. (key, value)，Range Push and Pull Parameter Server中，參數都是可以被表示成(key, value)的集合，比如一個最小化損失函數的問題，key 就是feature ID，而value 就是它的權值。對於稀疏參數，不存在的key，就可以認為是0。 把參數表示成kv (key, value)， 行式更自然、易於理解、更易於編程解； Workers 跟 Servers 之間通過 push 跟 pull 來溝通 : Worker 通過 push 將計算好的梯度發送到 Server，然後通過 pull 從Server 更新參數。 為了提高計算性能和頻寬效率，Parameter Server 允許用戶使用 Range Push 跟 Range Pull 操作（使用區間更新的方式，那麼可以進行如下操作： w.push(R, dest)w.pull(R, dest) 發送和接送特定 Range中的 w 。 2. Key-value vectors 賦予每個 key 所對應的 value 一個向量概念或矩陣概念。 3. User-Defined Functions on the Server 伺服器端更新參數的時候還有計算正則項，這樣的操作可以由使用者自定義。 4. Asychronous Tasks and Dependency同步(Synchronous)與非同步(Asynchronous)差異： 如上圖，如果 iter1 需要在 iter0 computation，進行 push 跟 pull 都完成後才能開始;Synchronous 將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。 好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的Work計算完成時間。 補充Asychronous 架構說明： Ref: [Paper][Online job scheduling in Distributed Machine Learning Clusters] Fig.2: Asynchronous Training Workflow 系統中非訓練工作流程，作業中不同 Worker 的訓練進度不同步，並且每次參數伺服器在接收到來自 Worker 的梯度時，每次更新其參數。 在上面Fig.2 圖中，參數伺服器使用「new weight = old weight − stepsize × gradient」然後將更新的權重發送回 Worker，處理完整個數據塊後，Worker 將繼續訓練分配給它的下一個數據塊。 Asychronous 的優點是能夠提高系統的使用效率（節省任務等待的過程），但是它的缺點就是容易降低算法的收斂速率。 考慮到使用者使用的時候會有不同的情况，Parameter Server 為使用者提供了多種任務依賴的方式： Sequential： 这里其实是 synchronous task，任务之间是有顺序的，只有上一个任务完成，才能开始下一个任务；Eventual： 跟 sequential 相反，所有任务之间没有顺序，各自独立完成自己的任务，Bounded Delay： 这是sequential 跟 eventual 之间的trade-off，可以设置一个 τ 作为最大的延时时间。也就是说，只有 &gt;τ 之前的任务都被完成了，才能开始一个新的任务；极端的情况： τ=0，情况就是 Sequential； τ=∞，情况就是 Eventual； 5. User-Defined Filters（使用者自定義過濾）在Worker節點這一端對梯度進行過濾，如果梯度並不是影響那麼大，就不佔用網絡去更新，等累積一段時間之後再去做更新。 對於機器學習優化問題比如梯度下降來說，並不是每次計算的梯度對於最終優化都是有價值的，使用者可以通過自定義的規則過濾一些不必要的傳遞，而再進一步壓縮頻寬花費： 發送很小的梯度值是低效率的： 因此可以自定義設置，只在梯度值較大的時候發送 更新接近最優情況的值是低效的： 所以只在非最優的情況下發送，可通過KKT來判斷 4. Paramter Server架構實現Vector Clock 為參數伺服器中的每個參數添加一個時間戳記，來跟蹤參數的更新和防止重複發送數據。基於此，溝通中的梯度更新數據中也應該有時間戳，防止重複更新。 如果每個參數都有一個時間戳，那麼參數眾多，時間戳也眾多。幸好 Parameter server在 push 跟 pull 的時候，都是 rang-based，這就帶來了一個好處：這個range裡面的參數共享的是同一個時間戳，這顯然可以大大降低了空間複雜度。 Messages Message是節點間交互的主要格式。一條 message 包括：時間戳，len(range)對kv. $[vc(R),(k1,v1),…,(kp,vp)]kj∈Randj∈{1,…p}$ 這是parameter server 中最基本的溝通格式，不僅僅是共享參數才有，task 的 message也是這樣的格式，只要把這裡的(key, value) 改成(task ID, 參數/返回值)。 由於機器學習問題通常都需要很高的網絡頻寬，因此信息的壓縮是必須的。 key的壓縮：因為訓練數據通常在分配之後都不會發生改變，因此worker沒有必要每次都發送相同的key，只需要接收方在第一次接收的時候緩存起來就行了。第二次，worker不再需要同時發送key和value，只需要發送value和key list的hash就行。這樣瞬間減少了一般的通信量。 value的壓縮：假設參數時稀疏的，那麼就會有大量的0存在。因此，為了進一步壓縮，我們只需要發送非0值。parameter server使用Snappy快速壓縮庫來壓縮數據、高效去除0值。 key的壓縮和value的壓縮可以同時進行。 Replication and Consistency參數伺服器集群中每個節點都負責不同區域的參數，那麼，類似於hash table，使用hash ring進行實現，key和server id都插入到hash ring上。 備份和一致性使用類似hadoop 的chain 備份方式，對於一個master 節點，如果有更新，先更新它，然後再去更新備份的伺服器。在更新的時候，由於機器學習算法的特點，可以將多次梯度聚合之後再去更新備份伺服器，從而減少帶寬。 Server Management由於key 的range 特性，當參數伺服器集群中增加一個節點時，步驟如下： Server Manager 節點給新節點分配一個key range，這可能會導致其他節點上的key range 切分。 新節點從其他節點上將屬於它的key range 數據取過來，然後也將slave 信息取過來。 Server Manager廣播節點變動，其他節點得知消息後將不屬於自己key range 的數據刪掉 在第二步，從其他節點上取數據的時候，其他節點上的操作也分為兩步，第一複製數據，這可能也會導致key range 的切分。第二是不再接受和這些數據有關的消息，而是進行轉發，轉發到新節點。 在第三步，收到廣播信息後，節點會刪除對應區間的數據，然後，掃描所有的和R有關發送出去的還沒收到回复的消息，當這些消息回复時，轉發到新節點。 節點的離開與節點的加入是類似 Worker Management添加工作節點比添加參數伺服器節點要簡單一些，步驟如下： task scheduler給新節點分配一些數據 節點從網絡文件系統中載入數據，然後從伺服器端拉取參數 task scheduler廣播變化，其他節點 free掉一些訓練數據 當一個節點離開的時候，task scheduler可能會尋找一個替代，但恢復節點是十分耗時的工作，同時，損失一些數據對最後的結果可能影響並不是很大。所以，系統會讓用戶進行選擇，是恢復節點還是不做處理。這種機制甚至可以允許用戶刪掉跑的最慢的節點來提升速度。 5. 何時使用分散式深度學習? 分散式的深度學習並不總是最佳的選擇，需要視情況而定： 進行分散式訓練，由於同步、資料和參數與網路傳輸等，分散式系統相比單機訓練要多不少額外的必要開銷。 可能導致網路模型的訓練時間過長： 神經網路太大 資料量太大 上圖其參考選擇任務合適對應訓練環境場景，其中都要考慮到 「網路傳輸」與「總計算量」 總結 一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。 系統性能跟算法收斂速率之間是存在一個平衡（trade-off） 的情況，你需要同時考慮 算法對於參數非一致性的敏感度 訓練數據特徵之間的關聯度 節點硬碟存儲容量 什麼時候DL訓練任務，真的需要使用分散式深度學習 Further Readings [Distributed System] Intro-Distributed-Deep-Learning References [Distributed System] 小沙文的博客 - Parameter Server 学习 [[Distributed System] #周末识堂# 参数服务器 parameter server] [Distributed System]Parameter Server 详解 [[Distributed System] raincoffee - Parameter Server] [Paper] Scaling Distributed Machine Learning with the Parameter Server [Paper] Large Scale Distributed Deep Networks","link":"/2019/01/09/parameter-server-concept/"},{"title":"透過 Zabbix API 監測Trigger狀態","text":"此篇主要說明如何透過，Zabbix提供的觸發器(Trigger)來監測CPU超標狀態，並透過python直接呼叫Zabbix API獲取所需要的狀態資訊，並協助監測遷移。 1. 新建Zabbix Trigger觸發器本節你會學習如何配置一個觸發器（trigger） 首先登入你Zabbix網頁並輸入帳號密碼 範例：1http://localhost/zabbix/ 進入Zabbix網站後，開始配置觸發器（trigger） 前往配置（Configuration）→ 主機（Hosts）→ 選擇你要進行配置觸發器（trigger）的節點 右側可以選擇要設置的環境，這邊Group我們選擇Compute節點群 接續會顯示Compute節點中可以觀察的三台Node主機 → 這邊我們直接點選node-1.domain.tld預備測試的節點進入設置 進入節點點擊上方的觸發器（Triggers），然後即可點擊創建觸發器（Create trigger），這將會向我們顯示一個觸發器定義表單 找到’新增主機（New host）’，點擊旁邊的觸發器（Triggers），然後點擊創建觸發器（Create trigger）。這將會向我們展現一個觸發器定義表單。 2. 設置Zabbix Trigger表達式由於我們需要持續觀察Computer節點上面CPU狀態，Trigger設定這邊可以直接透過語法進行觀察如下: 需要連續三分鐘CPU使用率平均值超過80%觸發報警 名稱（Name） 可以輸入: “CPU load too high on ‘node-1.domain.tld’ for 3 minutes”作為值。這個值會作為觸發器的名稱被現實在列表和其他地方。 表達式（Expression）1{node-1.domain.tld:system.cpu.util[,idle].max(3m)}&lt;20 Trigger 觀察主要透過system.cpu.util[,idle]狀態顯示，反向思考如果CPU能使用空閒小於20%，即CPU佔用超過80%立即觸發報警 把Trigger表達式打在Expression中 可以限制Trigger Severity警告提式: 這邊直接設定為High警告 完成後，點擊添加（Add）。新的觸發器將會顯示在觸發器列表中。 3. 顯示觸發器狀態 創建好的Trigger即可馬上列表出目前狀態 如果要查看Trigger目前監控狀態可以透過，前往監控（Monitoring） → 觸發器（Triggers），3分鐘後（我們需要等待3分鐘以評估這個觸發器的3分鐘平均值），觸發器會在這裡顯示。應該會有一個綠色的’OK’在’狀態（Status）’列中閃爍。 接下來可以透過存放於Compute node-1.domain.tld相關VM，來提升CPU使用率，達到Compute超標，這邊範例測試為超標CPU 50%表示為警告。 一般情況，綠色為system.cpu.util[,idle]狀態幾乎是很空閒的 超標狀態，可以明顯看到右側綠色能使用的system.cpu.util[,idle]明顯剩餘50%以下 從Zabbix 觀察Trigger狀態後，此處出現一個閃爍的紅色’PROBLEM’,顯然，這說明了CPU負載已經超過了你在觸發器裡定義的閾值。 4. 透過Python獲得Zabbix API 獲取Trigger超標狀態 參考 GitHub: 連結 此部分主要透過API方式取得Trigger.get欄位超標狀態，如果出現Trigger顯示為紅色閃爍的紅色PROBLEM，即可抓取到status:0的狀態，如果沒有超標則無法獲得資訊。 執行狀態可以直接獲得超標狀態123$ python zabbix-get-trigger.py Trigger message list: [ CPU load too high on 'node-1.domain.tld' for 3 minutes limit 50% ] view :status 0 若無超標及無法抓到Trigger列表的狀態123$ python zabbix-get-trigger.py No Trigger load high problem in list. 參考資料: [原创]Python利用Zabbix API定时报告存在报警的机器（更新：针对zabbix3.x） Zabbix Trigger表達式實例","link":"/2018/11/29/zabbix-api-trigger/"},{"title":"如何自行編譯 HPL-GPU 來測試 Benchmarks","text":"構建 NVIDIA CUDA Linpack 環境執行環境非常困難，網路上訊息非常少，然後linkpack測試更新版本已經有一段時間，記錄實作主要參考「Hybrid HPL(GPU版HPL)安装教程」與「AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる」兩個文章教學，並嘗試運行現在環境支援的版本，部署過程記錄。 環境部署資訊Linpack 部署的版本資訊： Mpich: v3.2.1 Openmpi: v1.10.3 Intel MKL: l_mkl_2019.0.117 Linpack: hpl-2.0_FERMI_v15 實驗環境安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體機器： Role vCPU RAM Extra Device ubuntu 8 16G GTX 1060 6G 事前準備 測試 Linkpack 之前，需要確保以下條件達成：確認環境是否安裝以下NVIDIA driver、CUDA、Intel MKL`Openmpi、mpich2`，並設定好環境變數。 安裝 NVIDIA驅動和CUDA tookit由於CUDA toolkit中，安裝時已包含了NVIDIA Driver，可一併安裝 1234$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：123456789101112131415161718192021222324252627282930$ lsmod | grep nvidianvidia_uvm 790528 0nvidia_drm 40960 2nvidia_modeset 1089536 3 nvidia_drmdrm_kms_helper 167936 1 nvidia_drmdrm 360448 5 nvidia_drm,drm_kms_helpernvidia 14032896 96 nvidia_modeset,nvidia_uvmipmi_msghandler 45056 2 nvidia,ipmi_devintf$ cat /usr/local/cuda/version.txtCUDA Version 9.2.148$ nvidia-smiTue Oct 2 18:15:47 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 396.44 Driver Version: 396.44 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 On | N/A || 39% 31C P8 7W / 120W | 52MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1603 G /usr/lib/xorg/Xorg 49MiB |+-----------------------------------------------------------------------------+ 準備LinpackLink : https://developer.nvidia.com/rdp/assets/cuda-accelerated-linpack-linux64從上面的連結，登入CUDA註冊開發者會員，下載linpack for Linux64版本，這裡下載到的版本為hpl-2.0_FERMI_v15.tgz。 參考連結Hybrid HPL(GPU版HPL)安装教程AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる 安裝INTEL MKL透過連結註冊帳號https://software.intel.com/en-us/qualify-for-free-software 註冊後，它會向您發送序列號於信箱，以便進行安裝準備。 這邊是下載最新l_mkl_2019.0.117.tgz版本 下載取得l_mkl_2019.0.117.tgz後，即可透過install.sh運行安裝。 12$ tar zxvf l_mkl_2019.0.117.tgz$ cd l_mkl_2019.0.117 Intel mkl的安裝很簡單的，每一步也都有說明，按Enter繼續下一步預設設定安裝即可，安裝到某一步會要求輸入序列號，申請30天試用版所給的那個序列號。123456789101112131415161718192021222324$ sh ./install.sh--------------------------------------------------------------------------------Initializing, please wait...--------------------------------------------------------------------------------Welcome--------------------------------------------------------------------------------Welcome to the Intel(R) Math Kernel Library 2019 for Linux*--------------------------------------------------------------------------------You will complete the following steps: 1. Welcome 2. License Agreement 3. Options 4. Installation 5. Complete----------------------------------------------------------------------------------------------------------------------------------------------------------------Press \"Enter\" key to continue or \"q\" to quit:License Agreement-------------------------------------------------------------------------------- 確認後會安裝一些套件，這裡就可以看到MKL預設情況下，會安裝在/opt/intel下面。123456789101112131415161718------------------------Options &gt; Pre-install Summary--------------------------------------------------------------------------------Install location: /opt/intelComponent(s) selected: Intel Math Kernel Library 2019 for C/C++ 2.6GB Intel MKL core libraries for C/C++ Intel TBB threading support GNU* C/C++ compiler support Intel Math Kernel Library 2019 for Fortran 2.6GB Intel MKL core libraries for Fortran GNU* Fortran compiler support Fortran 95 interfaces for BLAS and LAPACK Install space required: 2.8GB 編譯完成後，即會顯示安裝資訊。 123456789101112------------------------Complete--------------------------------------------------------------------------------Thank you for installing Intel(R) Math Kernel Library 2019 for Linux*.If you have not done so already, please register your product with IntelRegistration Center to create your support account and take full advantage ofyour product purchase.Your support account gives you access to free product updates and upgradesas well as Priority Customer support at the Online Service Centerhttps://supporttickets.intel.com. 完整安裝過程於Gist。 安装mpich2123456$ wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gztar zxvf mpich-3.2.1.tar.gz$ cd mpich-3.2.1./configure -prefix=/home/username/mpich$ make$ make install 配置環境打開/etc/environment1$ vim /etc/environment 將自己的路徑添加到PATH最後，注意別忘了冒號“：”，添加後的PATH如下1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存退出，在終端輸入source /etc/environment再輸入echo $PATH測試發現已經更新，環境變量配置成功。 本文来自 ForTheDreamSMS 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/baidu_34045013/article/details/78237842?utm_source=copy 安裝參考:ubuntu16.04安裝配置mpich2 安裝openmpi1234567$ wget -c https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.3.tar.gz$ tar zxvf openmpi-1.10.3.tar.gz$ cd openmpi-1.10.3$ ./configure --prefix=/opt/openmpi$ make$ sudo make install 安裝make和make instal需要一段時間，等待完成即可，openmpi環境配置會在後面統一設定。 參考 : OpenMPI設定叢集環境 配置環境變量首先更改環境變量PATH： 1sudo vim /etc/environment 在PATH變量加上/usr/local/cuda-9.2/bin,前面要有分號，後面沒有，修改後例如下面這樣：1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存文件，然後再執行：source /etc/environment完成後，可以執行echo $PATH,查看是否修改成功 接著還需更改ldconfig12cd /etc/ld.so.conf.d/sudo vim hpl.conf 輸入如下內容12345/usr/local/cuda-9.2/lib64/lib/opt/intel/mkl/lib/intel64/opt/intel/lib/intel64/home/ubuntu/hpl/src/cuda 最後一行/home/使用者/hpl/src/cuda是編譯HPL時才需要改的，在這裡一併修改。這個目錄就是編譯hpl時，hpl的路徑。 添加上述內容並保存後，執行1sudu ldconfig 可以輸入下面命令進行檢驗，有輸出內容就對了1sudo ldconfig -v | grep cuda 接著還要執行Intel MKL的環境變量設置腳本1234export LD_LIBRARY_PATH=/opt/intel/mkl/lib/intel64:/opt/intel/compilers_and_libraries/linux/lib/intel64:/home/ubuntu/hpl/src/cuda:/opt/openmpi/libexport PATH=/opt/openmpi/bin:$PATHsource /opt/intel/compilers_and_libraries_2019.0.117/linux/mkl/bin/mklvars.sh intel64 請確認以上路徑與當前環境上所有套件的路徑是否對應存在，再執行1source ~/.bashrc 這樣，環境變量就設置好了。最好echo $PATH查看下是否多了一行intel的信息，如果沒有配置成功的話，在編譯HPL時會提示/usr/bin/ld: cannot find -liomp5的錯誤。 開始編譯Linpack benchmark for CUDA這邊將hpl-2.0_FERMI_v15.tgz解壓縮放置主目錄下hpl文件夾，可以依照自己設定的路徑對應編譯。 12345$ tar -xvf hpl-2.0_FERMI_v15.tgz –C ~/hpl$ cd ~/hpl$ lsbin BUGS COPYRIGHT CUDA_LINPACK_README.txt HISTORY include INSTALL lib Make.CUDA Makefile makes Make.top man README setup src testing TODO TUNING www 編譯Make.CUDA編輯配置這時還需要編輯Make.CUDA測試環境參考連結，需更改Make.CUDA中的TOPdir為hpl的目錄。 123456103 TOPdir = /home/ubuntu/hpl132 LAdir = /opt/intel/mkl/lib/intel64133 LAMP5dir = /opt/intel/compilers_and_libraries/linux/lib/intel64134 LAinc = -I/opt/intel/mkl/include 接著可以開始編譯了12cd ~/hplmake arch=CUDA 如果沒有提示錯誤，就是編譯成功了。 編譯完成後，還需要修改~/hpl/bin/CUDA/run_linpack中的HPL_DIR為你hpl的路徑 1HPL_DIR=/home/ubuntu/hpl 修改完成後就可以開始測試了。 測試之前建議把HPL.dat的參數改小一點，N改成8000，這樣所需的測試時間少。也先把P，Q，PxQ都改成1，保證可以執行測試: 1$ mpirun -n 1 ./run_linpack 輸出結果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ mpirun -n 1 ./run_linpack================================================================================HPLinpack 2.0 -- High-Performance Linpack benchmark -- September 10, 2008Written by A. Petitet and R. Clint Whaley, Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V : Wall time / encoded variant.N : The order of the coefficient matrix A.NB : The partitioning blocking factor.P : The number of process rows.Q : The number of process columns.Time : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N : 25000 30000NB : 768 1024 1280 1536PMAP : Row-major process mappingP : 1Q : 1PFACT : LeftNBMIN : 2NDIV : 2RFACT : LeftBCAST : 1ringDEPTH : 1SWAP : Spread-roll (long)L1 : no-transposed formU : no-transposed formEQUIL : yesALIGN : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed: ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be 1.110223e-16- Computational tests pass if scaled residuals are less than 16.0================================================================================T/V N NB P Q Time Gflops--------------------------------------------------------------------------------WR10L2L2 25000 768 1 1 43.07 2.419e+02--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)= 0.0040802 ...... PASSED================================================================================ 完整環境配置與設置有放到Gits，可以提供參考。 #補充-直接使用 Docker測試HPL GPU: 參考連結","link":"/2018/10/23/build-hpl-gpu/"},{"title":"透過 Microk8s 快速部署 Kubeflow","text":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。 什麼是microk8s簡單來說，microk8s設計為快速輕巧Kubernetes最新版本安裝與主機隔離但不通過虛擬機。通過在單個快照包snap中打包 Kubernetes、Docker.io、iptables和CNI的所有上游二進製文件來實現此隔離。snap是一個應用程序容器，您可以將其想像為Docker容器的輕量級版本。它使用了許多相同的底層技術進行隔離，而不會有網絡隔離的所帶來的開銷。而minikube需使用虛擬化工具來環境隔離創建K8S集群，想必會快照更加耗時。 安裝MultipassMac OS X. (本篇文章以Mac環境進行)) 本機Mac OS 安裝程序安裝Multipass Linux OS 透過snap使用指令安裝 1$ sudo snap install multipass --beta --classic 啟動Ubuntu虛擬機部署前要先下載cloud-init文件1wget https://bit.ly/2tOfMUA -O kubeflow.init 這邊查看一下文件內容，主要是配置kubeflow環境並去執行kubeflow 已經寫好script能更快速部署，有需要修正自己的環境可以修改後執行123456789101112# cloud-init for kubeflowpackage_update: truepackage_upgrade: trueruncmd: - mkdir /kubeflow - wget https://bit.ly/2tp2aOo -O /kubeflow/install-kubeflow-pre-micro.sh - chmod a+x /kubeflow/install-kubeflow-pre-micro.sh - wget https://bit.ly/2tndL0g -O /kubeflow/install-kubeflow.sh - chmod a+x /kubeflow/install-kubeflow.sh - printf \"\\n\\nexport KUBECONFIG=/snap/microk8s/current/client.config\\n\\n\" &gt;&gt; /home/multipass/.bashrc 啟動Multipass VM123456$ multipass launch bionic -n kubeflow -m 8G -d 40G -c 4 --cloud-init kubeflow.initRetrieving image: 66%Retrieving image: 87%Verifying image: -Launched: kubeflow 預設備至Multipass為Kubeflow部署創建的VM上的最低建議設置，這邊是可以自由地調整根據主機的能力和工作負載需求。 依據環境狀況應該很快速就建立好單節點集群，這邊可以查看一下123$ multipass list Name State IPv4 Releasekubeflow RUNNING 192.168.64.2 Ubuntu 18.04 LTS 如果部署有問題隨時都可以透過multipass -h查看 接下來就mount本地文件與multipass配置1$ multipass mount . kubeflow:/multipass 安裝kubernetes接下來我們就可以進入vm，安裝由microk8s驅動的kubernetes，以及部署Kubeflow所需的其他工具。 透過multipass進入vm12345678910111213141516171819202122$ multipass shell kubeflowWelcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.15.0-34-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Mon Sep 17 09:03:39 CST 2018 System load: 0.63 Processes: 153 Usage of /: 2.9% of 38.60GB Users logged in: 0 Memory usage: 2% IP address for enp0s2: 192.168.64.2 Swap usage: 0% Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.multipass@kubeflow:~$ 安裝microk8s、etc1234567891011121314151617$ sudo /kubeflow/install-kubeflow-pre-micro.shSaving to: ‘ksonnet.tar.gz’ksonnet.tar.gz 100%[============================================================================================&gt;] 14.77M 934KB/s in 44s2018-09-17 09:22:01 (342 KB/s) - ‘ksonnet.tar.gz’ saved [15491217/15491217]ks_0.11.0_linux_amd64/CHANGELOG.mdks_0.11.0_linux_amd64/CODE-OF-CONDUCT.mdks_0.11.0_linux_amd64/CONTRIBUTING.mdks_0.11.0_linux_amd64/LICENSEks_0.11.0_linux_amd64/README.mdks_0.11.0_linux_amd64/ksChecking kube-system status until all pods are running (2 not running)Checking kube-system status until all pods are running (3 not running)Before running install-kubeflow.sh, please 'export GITHUB_TOKEN=&lt;your token&gt;' 這邊提供GitHub Token是為了避免ksonnet部署反覆部署可能會造成GitHub限速問題，可以參考建立Token。 更換自己的Token1$ export GITHUB_TOKEN=972702e4fb348d3dba52f5dd3b99d7ae83e857ab kubeflow 部署在vm環境中執行kubeflow script，就可以快速完成kubeflow部署，而這邊會有Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.狀況就依照環境是否把所有應該生成的Pod建立完成，就執行結束。 1234567891011121314$ multipass@kubeflow:~$ /kubeflow/install-kubeflow.shnamespace/kubeflow createdINFO Using context \"microk8s\" from kubeconfig file \"/snap/microk8s/current/client.config\"INFO Creating environment \"default\" with namespace \"default\", pointing to cluster at address \"http://127.0.0.1:8080\"INFO Generating ksonnet-lib data at path '/home/multipass/my-kubeflow/lib/v1.11.1'INFO Retrieved 22 filesINFO Retrieved 5 filesINFO Retrieved 5 files···Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.JupyterHub Port: 31808 以上就完成microk8s+kubeflow部署 部署完成後環境完成後檢查 Kubeflow 元件部署結果： 12345678910multipass@kubeflow:~$ kubectl -n kubeflow get po -o wideNAME READY STATUS RESTARTS AGE IP NODEambassador-68954d75f4-2n6lv 2/2 Running 4 4h 10.1.1.55 kubeflowambassador-68954d75f4-7dgvl 2/2 Running 4 4h 10.1.1.56 kubeflowambassador-68954d75f4-bgs8h 2/2 Running 5 4h 10.1.1.44 kubeflowmxnet-operator-f46557c4f-wmntn 1/1 Running 2 3h 10.1.1.52 kubeflowspartakus-volunteer-d54b65666-klrfk 1/1 Running 2 4h 10.1.1.45 kubeflowtf-hub-0 1/1 Running 2 4h 10.1.1.47 kubeflowtf-job-dashboard-784cdcbb4f-j2cjx 1/1 Running 2 4h 10.1.1.51 kubeflowtf-job-operator-85b46d47b7-xfwmp 1/1 Running 2 4h 10.1.1.48 kubeflow 運行JupyterHub測試透過查看vm狀態中，完成後連接 http://IP:Port，並輸入任意帳號密碼進行登入。 Spawner options可以自行配置Jupyter Notebook 資源狀態 Image: 預設會有多種映像檔可以使用其他參數可調整 CPU、Memory、GPU資源限制 按下Spawn kubeflow就會啟動一個pod配置資源給Jupyter notebook，部署過程時Image下載需要花一點時間。 配置過程中有時會後有狀況，可能需要多嘗試 完成後即可在透過Jupyter Notebook 編寫Model進行DL訓練 Kubernetes Addonsmicrok8s在Kubernetes安裝了一個準系統。這代表只要安裝和運行api-server、controller-manager、scheduler、kubelet，cni、kube-proxy。可以使用該microk8s.enable命令運行kube-dns和Dashboard等附加服務。 1microk8s.enable dns dashboard 使用該disable命令隨時禁用這些插件1microk8s.disable dashboard dns 12345678$ microk8s.kubectl cluster-infoKubernetes master is running at http://127.0.0.1:8080Heapster is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/heapster/proxyKubeDNS is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyGrafana is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxyInfluxDB is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:http/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 停止並重新啟動microk8 (測試環境為Mac) 在 Linx 環境指令都是偷過snap enable/disable 來管理microk8s Mac\b\b環境可以透過multipass -h查詢可執行指令 暫時關閉vm運行1multipass stop kubeflow 啟動vm運行1multipass stop kubeflow 查看multipass目前vm狀態12Name State IPv4 Releasekubeflow STOPPED -- Ubuntu 18.04 LTS 刪除microk8s透過multipass list 檢查instances狀態 如果只下delete只有instances狀態刪除，是可以透過recover恢復已刪除的instances123multipass delete kubeflowName State IPv4 Releasekubeflow DELETED -- Not Available 所以我們要透過purge來清除永久清除所有已刪除的instances1multipass purge 相關文章:[Kubernetes] A local Kubernetes with microk8s[kubeflow] Microk8s for Kubeflow 本篇文章主介紹microk8s部署kubeflow，更多相關kubeflow 實作會在後續文章陸續介紹","link":"/2018/09/17/microk8s-deploy-kubeflow/"},{"title":"透過microk8s更輕巧建立單節點kubernetes叢集（含GPU測試）","text":"先前文章參考：透過 Microk8s 快速部署 Kubeflow 此篇想重新介紹透過 microk8s，近期剛好看到Using GPGPUs with Kubernetes這邊文章介紹，想起之前在實驗kubeflow時候有於Mac上執行過Microk8s，這次重新測試於單機 Linux 環境並測試相關 Addons 與GPU環境中實驗。 什麼是 microk8s?123456# What is microk8s? _ _ ___ _ __ ___ (_) ___ _ __ ___ | | _( _ ) ___| '_ ` _ \\| |/ __| '__/ _ \\| |/ / _ \\/ __|| | | | | | | (__| | | (_) | &lt; (_) \\__ \\|_| |_| |_|_|\\___|_| \\___/|_|\\_\\___/|___/ 由 Ubuntu 子公司 Canonica 推出的 microK8s， 這是一種快速建置Kubernetes單節點的快照包，目前支援可安裝在42種Linux系統上，只要任何支持Snap Linux發行都可以透過 MicroK8s 快速部署。藉由小磁區和內存佔用，MicroK8s提供了一種在幾秒鐘內部署Kubernetes最新上游版本的有效解決方案，無論是在個人主機、伺服器、雲服務設備上，都能快速執行測試。 為了進一步加速Kubernetes採用並簡化常見的開發人員使用情境，MicroK8s包括越來越多的Add-on服務，包括容器映像檔儲存、存儲傳遞和本機GPGPU支援，所有這些服務都可直接通過單個命令啟用。對於資料科學家和機器學習工程師而言，單節點測試環境有GPGPU支持，簡化實驗過程中環境應用上相關的工作流程。 實驗環境 測試機器為Ubuntu 16.04並包含一張GTX 1060 GPU卡 已經事先安裝完成CUDA Version 10.0.130、Nvidia Driver 410.79 Installing snap on Ubuntu (若環境沒有安裝)12$ sudo apt update$ sudo apt install snapd 123456$ snap versionsnap 2.36.1snapd 2.36.1series 16ubuntu 18.04kernel 4.15.0-39-generic 部署 microk8s最快的入門方法是通過單擊snap store按鈕直接從快照存儲中安裝microk8s。 這邊還是使用CLI命令列介面安裝microk8s，並讓它更新到最新的穩定上游Kubernetes版本： 123$ sudo snap install microk8s --classicmicrok8s v1.13.0 from Canonical✓ installed 若要指定不同kubernetes版本，我們可以在安裝過程選擇版本。例如要指定v1.12 version： 123$ sudo snap install microk8s --classic --channel=1.12/stablemicrok8s (1.12/stable) v1.12.3 from Canonical✓ installed 可以透過snap info microk8s看查看當前發布的穩定版、測試版等多個版本: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859$ snap info microk8sname: microk8ssummary: Kubernetes for workstations and appliancespublisher: Canonical✓contact: https://github.com/ubuntu/microk8slicense: unsetdescription: | MicroK8s is a small, fast, secure, single node Kubernetes that installs on just about any Linux box. Use it for offline development, prototyping, testing, or use it on a VM as a small, cheap, reliable k8s for CI/CD. It's also a great k8s for appliances - develop your IoT apps for k8s and deploy them to MicroK8s on your boxes.commands: - microk8s.config - microk8s.disable - microk8s.docker - microk8s.enable - microk8s.inspect - microk8s.istioctl - microk8s.kubectl - microk8s.reset - microk8s.start - microk8s.status - microk8s.stopservices: microk8s.daemon-apiserver: simple, enabled, active microk8s.daemon-apiserver-kicker: simple, enabled, active microk8s.daemon-controller-manager: simple, enabled, active microk8s.daemon-docker: simple, enabled, active microk8s.daemon-etcd: simple, enabled, active microk8s.daemon-kubelet: simple, enabled, active microk8s.daemon-proxy: simple, enabled, active microk8s.daemon-scheduler: simple, enabled, activesnap-id: EaXqgt1lyCaxKaQCU349mlodBkDCXRcgtracking: stablerefresh-date: today at 13:44 CSTchannels: stable: v1.13.0 (340) 204MB classic candidate: v1.13.0 (340) 204MB classic beta: v1.13.0 (340) 204MB classic edge: v1.13.1 (350) 229MB classic 1.13/stable: v1.13.0 (340) 204MB classic 1.13/candidate: v1.13.0 (340) 204MB classic 1.13/beta: v1.13.0 (340) 204MB classic 1.13/edge: v1.13.1 (351) 229MB classic 1.12/stable: v1.12.3 (336) 226MB classic 1.12/candidate: v1.12.3 (336) 226MB classic 1.12/beta: v1.12.3 (336) 226MB classic 1.12/edge: v1.12.3 (336) 226MB classic 1.11/stable: v1.11.5 (322) 219MB classic 1.11/candidate: v1.11.5 (322) 219MB classic 1.11/beta: v1.11.5 (322) 219MB classic 1.11/edge: v1.11.5 (322) 219MB classic 1.10/stable: v1.10.11 (321) 175MB classic 1.10/candidate: v1.10.11 (321) 175MB classic 1.10/beta: v1.10.11 (321) 175MB classic 1.10/edge: v1.10.11 (321) 175MB classicinstalled: v1.13.0 (340) 204MB classic 下指令操作環境請使用User執行，使用root user執行會command not found。 通過microk8s.status檢查 microk8s目前狀態：123456789101112$ microk8s.statusmicrok8s is runningaddons:ingress: disableddns: disabledmetrics-server: disabledistio: disabledgpu: disabledstorage: disableddashboard: disabledregistry: disabled 訪問Kubernetes為了避免與kubectl已安裝在原環境導致發生衝突並避免覆蓋任何現有的Kubernetes配置文件，MicroK8s添加了一個microk8s.kubectl命令，配置為專門訪問新的MicroK8s安裝。 1234$ microk8s.kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-testing Ready &lt;none&gt; 2m30s v1.13.0 microk8s.config12345678910111213141516171819$ microk8s.configapiVersion: v1clusters:- cluster: server: http://192.0.0.10:8080 name: microk8s-clustercontexts:- context: cluster: microk8s-cluster user: admin name: microk8scurrent-context: microk8skind: Configpreferences: {}users:- name: admin user: username: admin microk8s.docker12$ microk8s.docker -vDocker version 17.03.2-ce, build f5ec1e2 microk8s.start / stop1234$ microk8s.stopStopped.$ microk8s.startStarted. 若無與原生kubectl衝突影響，這邊可以透過alias指定名稱省去輸入指令時間。 123$ sudo snap alias microk8s.kubectl kubectlAdded: - microk8s.kubectl as kubectl 解除alias設定 1$ snap unalias kubectl 啟動 microk8s.enable默認情況下，我們在Kubernetes上游獲得了一個準系統。可以使用以下microk8s.enable命令運行其他服務，例如kube-dns和儀表板： 最重要的插件列表: dns：部署kube dns。其他人可能需要此插件，因此我們建議您始終啟用它。 儀表板：部署kubernetes儀表板以及grafana和Influxdb。 storage：創建默認存儲類。此存儲類使用指向主機上目錄的hostpath-provisioner。 ingress：創建入口控制器。 gpu：通過啟用nvidia-docker運行時和nvidia-device-plugin-daemonset，將GPU暴露給microk8s。需要已在主機系統上安裝NVIDIA驅動程序。 istio：部署核心Istio服務。您可以使用該microk8s.istioctl命令來管理部署。 registry：部署docker私有註冊表並在localhost：32000上公開它。存儲插件將作為此插件的一部分啟用。 12345678910111213141516$ microk8s.enable dashboardEnabling dashboardsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard createdservice/monitoring-grafana createdservice/monitoring-influxdb createdservice/heapster createddeployment.extensions/monitoring-influxdb-grafana-v4 createdserviceaccount/heapster createdconfigmap/heapster-config createdconfigmap/eventer-config createddeployment.extensions/heapster-v1.5.2 createddashboard enabled 123456789101112$ microk8s.kubectl -n kube-system get pod,svc -o wide Sat Dec 15 14:48:45 2018NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/heapster-v1.5.2-64874f6bc6-nt72k 4/4 Running 0 94s 10.1.1.17 k8s-testing &lt;none&gt; &lt;none&gt;pod/kubernetes-dashboard-654cfb4879-xm8kp 1/1 Running 0 2m26s 10.1.1.13 k8s-testing &lt;none&gt; &lt;none&gt;pod/monitoring-influxdb-grafana-v4-6679c46745-4kv5s 2/2 Running 0 2m25s 10.1.1.14 k8s-testing &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/heapster ClusterIP 10.152.183.62 &lt;none&gt; 80/TCP 2m25s k8s-app=heapsterservice/kubernetes-dashboard ClusterIP 10.152.183.54 &lt;none&gt; 443/TCP 2m26s k8s-app=kubernetes-dashboardservice/monitoring-grafana ClusterIP 10.152.183.188 &lt;none&gt; 80/TCP 2m26s k8s-app=influxGrafanaservice/monitoring-influxdb ClusterIP 10.152.183.117 &lt;none&gt; 8083/TCP,8086/TCP 2m25s k8s-app=influxGrafana 可以使用disable命令隨時停用這些插件 1$ microk8s.disable dashboard dns 可隨時透過microk8s.status你可以看到當前啟用與可用插件列表。 k8s barebone: api-server controller-manager scheduler kubelet cni kube-proxy 除了 Dashboard 和 private registry，還可以選擇GPU和Istio服務： DNS：部署kube dns。 Dasnboard：部署kubernetes Dashboard 以及 grafana 和 Influxdb。 storage：創建默認存儲區。此存儲使區指向主機上目錄的hostpath-provisioner。 ingress gpu：通過啟用nvidia-docker運行時和nvidia-device-plugin-daemonset，將GPU掛給microk8s。需要已在主機系統上安裝NVIDIA驅動程序。 istio：部署核心Istio服務。您可以使用該microk8s.istioctl命令來管理部署。 registry 啟用GPU microk8s.enable gpu123456789101112131415$ microk8s.enable gpuEnabling NVIDIA GPUNVIDIA kernel module detectedEnabling DNSApplying manifestservice/kube-dns createdserviceaccount/kube-dns createdconfigmap/kube-dns createddeployment.extensions/kube-dns createdRestarting kubeletDNS is enabledApplying manifestdaemonset.extensions/nvidia-device-plugin-daemonset createdNVIDIA is enabled 檢查enable後nvidia-divice-plugin狀態： 123456789101112$ microk8s.statusmicrok8s is runningaddons:ingress: disableddns: enabledmetrics-server: disabledistio: disabledgpu: enabledstorage: disableddashboard: enabledregistry: disabled 123456789101112131415$ microk8s.kubectl -n kube-system get pod,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/heapster-v1.5.2-64874f6bc6-wkxtm 4/4 Running 4 14m 10.1.1.25 k8s-testing &lt;none&gt; &lt;none&gt;pod/kube-dns-6ccd496668-gv9pm 3/3 Running 0 38s 10.1.1.26 k8s-testing &lt;none&gt; &lt;none&gt;pod/kubernetes-dashboard-654cfb4879-wbzkz 1/1 Running 1 14m 10.1.1.23 k8s-testing &lt;none&gt; &lt;none&gt;pod/monitoring-influxdb-grafana-v4-6679c46745-n4j74 2/2 Running 3 14m 10.1.1.24 k8s-testing &lt;none&gt; &lt;none&gt;pod/nvidia-device-plugin-daemonset-w828l 1/1 Running 0 32s 10.1.1.27 k8s-testing &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/heapster ClusterIP 10.152.183.11 &lt;none&gt; 80/TCP 14m k8s-app=heapsterservice/kube-dns ClusterIP 10.152.183.10 &lt;none&gt; 53/UDP,53/TCP 38s k8s-app=kube-dnsservice/kubernetes-dashboard NodePort 10.152.183.201 &lt;none&gt; 443:32560/TCP 14m k8s-app=kubernetes-dashboardservice/monitoring-grafana ClusterIP 10.152.183.191 &lt;none&gt; 80/TCP 14m k8s-app=influxGrafanaservice/monitoring-influxdb ClusterIP 10.152.183.68 &lt;none&gt; 8083/TCP,8086/TCP 14m k8s-app=influxGrafana 測試GPU節點是否可以正常運作這邊簡易部署gpu-pod測試節點divice pligin 可以正常使用 12345678910111213141516cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: gpu-podspec: restartPolicy: Never containers: - image: nvidia/cuda name: cuda command: [\"nvidia-smi\"] resources: limits: nvidia.com/gpu: 1EOFpod \"gpu-pod\" created 1234$ kubectl get pod -o wide Sat Dec 15 15:20:54 2018NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESgpu-pod 0/1 Completed 0 3m29s 10.1.1.28 k8s-testing &lt;none&gt; &lt;none&gt; 查看Pod log 狀態：123456789101112131415161718$ kubectl logs gpu-podSat Dec 15 07:18:53 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.79 Driver Version: 410.79 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 Off | N/A || 37% 25C P8 5W / 120W | 0MiB / 6077MiB | 1% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 刪除 microk8s在移除microk8s之前，請先使用microk8s.reset停止所有正在運行的pod 12$ microk8s.reset$ snap remove microk8s 相關參考[Github/microk8s][Using GPGPUs with Kubernetes][install a local kubernetes with microk8s][MicroK8sを使ってみる]","link":"/2018/12/14/snap-microk8s-deploy-kubernetes/"},{"title":"快速部署 Kubeflow v0.3 容器機器學習平台","text":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。 Kubeflow目標不是在於重建其他服務，而是提供一個最佳開發系統，來部署到任何集群中，有效確保ML在集群之間移動性，並輕鬆將任務擴展任何集群。 由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠運行部署 Kubeflow。 在不同基礎設施上協助實現Machine learning 工作流程「更加簡單」與「便於攜帶」、「可擴展性」的部署，在不同基礎設施上(Local Laptop、GPU Server cluster、Cloud Production Cluster) 簡述Kubelfow工作流程： 下載Kubeflow shell script 和 配置文件 自定義 ksonnet部署所需的JSON組件 執行shell script將容器部署到所選擇的Kubernetes Enveroment 開始執行機器學習訓練 我們透過Kubeflow工具管理，可以解決ML環境配置困難的問題，其中在Kubeflow提供Operator，對不同的機器學習框架，擴展的 Kubernetes API 進行自動建立、管理與配置應用程式容器實例，並提供訓練在不同環境和服務，包含數據準備、模型培訓、預測服務和服務管理。 Kubernetes 有提供不同集群部署支援，包括單節點Minikube部署、 透過Hypervisor部署的Multipass &amp; Microk8s、Cloud。無論你在哪裡運行Kubernetes，你都能夠運行Kubeflow。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Destop，測試環境為實體機器部署HA Cluster： IP Address Role vCPU RAM Extra Device 172.22.132.98 VIP 172.22.132.81 k8s-m1 8 16G GTX 1060 6G 172.22.132.82 k8s-m2 8 16G GTX 1060 6G 172.22.132.83 k8s-m3 8 16G GTX 1060 6G 172.22.132.84 k8s-g1 8 16G GTX 1060 6G 172.22.132.97 k8s-g2 8 16G GTX 1060 6G 172.22.132.86 k8s-g3 8 16G GTX 1060 6G 172.22.132.94 k8s-g11 8 16G GTX 1060 6G kubernetes部署可以參考 開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集教學 本次部署使用版本 kubernetes v1.11.2 Master x 3, Worker x 4 ksonnet 0.13.0 kubeflow v0.3.0 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點確認已經安裝 NVIDIA driver、CUDA、Docker、NVIDIA Docker，以下為實驗環境版本資訊： CUDA Version: 10.0.130 Driver Version: 410.48 Docker Version: 18.03.0-ce NVIDIA Docker: 2.0.3 安裝 NFS設置Kubeflow需要Volumes來存儲數據，建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： 1234567891011# 在 master 執行，這邊nfs-server建立在k8s-m1節點$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data# 可以在 `nfs-data`資料夾建立多個pv資料夾，提供kubeflow儲存使用$ cd /nfs-data$ mkdir user1 user2 ... $ echo \"/nfs-data *(rw,sync,no_root_squash,no_subtree_check)\" | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 這邊也可以使用其他存儲數據方式: Kubernetes: Local Volume 安裝 KsonnetKsonnet 是一個命令工具，可以更輕鬆地管理由多個組件組成的複雜部署，簡化編寫和部署 Kubernetes 配置。 123456789$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.0/ks_0.13.0_linux_amd64.tar.gz$ tar xvf ks_0.13.0_linux_amd64.tar.gz$ sudo cp ks_0.13.0_linux_amd64 /usr/local/bin/$ chmod +x ks$ ks versionksonnet version: 0.13.0jsonnet version: v0.11.2client-go version: kubernetes-1.10.4 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。部署版本選擇為kubeflow v0.3.0。 下載 Kubeflow 腳本下載GitHub中Kubeflow設置所需配置： 12345678910# 建立配置指定資料夾路徑，可自行更換名稱$ KUBEFLOW_SRC=mykfsrc $ mkdir ${KUBEFLOW_SRC}$ cd ${KUBEFLOW_SRC}# 部署kubeflow版本標籤，可自行選擇其他分支 (Upload v0.3.4)$ export KUBEFLOW_TAG=v0.3.0 # 執行shell script$ curl https://raw.githubusercontent.com/kubeflow/kubeflow/${KUBEFLOW_TAG}/scripts/download.sh | bash 下載後目錄下會多kubeflow、scripts資料夾: 123$ tree -L 1 ├── kubeflow└── scripts 執行腳本配置和部署Kubeflow123456789101112131415161718# 退回上一層建立對應路徑$ cd ../$ KUBEFLOW_REPO=$(pwd)/mykfsrc# ${KFAPP} 為配置 kubeflow 儲存目錄的名稱，執行init時將創建此目錄# ksonnet應用程式將在 ${KFAPP}/ks_app目錄中創建$ KFAPP=mykfapp$ ${KUBEFLOW_REPO}/scripts/kfctl.sh init ${KFAPP} --platform none# 安裝 Kubeflow 套件至應用程式目錄$ cd ${KFAPP}$ ${KUBEFLOW_REPO}/scripts/kfctl.sh generate k8s# 定義 Namespace 為kubeflow來管理相關資源$ kubectl create ns kubeflow# 部署 Kubeflow$ ${KUBEFLOW_REPO}/scripts/kfctl.sh apply k8s ！部署預設為default，這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 完成後檢查 Kubeflow 元件部署結果：12345678910111213141516171819$ kubectl -n kubeflow get po -o wide ambassador-868d5dbdc4-5fgdw 3/3 Running 0 7m 10.244.2.90 k8s-g3 &lt;none&gt;ambassador-868d5dbdc4-qcp5z 3/3 Running 1 7m 10.244.5.78 k8s-g11 &lt;none&gt;ambassador-868d5dbdc4-x9bfc 3/3 Running 0 7m 10.244.3.96 k8s-g2 &lt;none&gt;argo-ui-84464bd59c-hmhmd 1/1 Running 0 7m 10.244.4.89 k8s-g1 &lt;none&gt;centraldashboard-c76877875-nqbhg 1/1 Running 0 7m 10.244.3.97 k8s-g2 &lt;none&gt;modeldb-backend-58969447f6-z2xmf 1/1 Running 1 11s 10.244.3.102 k8s-g2 &lt;none&gt;modeldb-db-57b855f5b7-dw5s6 1/1 Running 0 7m 10.244.5.79 k8s-g11 &lt;none&gt;modeldb-frontend-769d5bdd66-njkqs 1/1 Running 0 7m 10.244.3.99 k8s-g2 &lt;none&gt;spartakus-volunteer-5bfd5876fb-b86h5 1/1 Running 0 7m 10.244.2.91 k8s-g3 &lt;none&gt;studyjob-controller-56588dc6f9-shg72 1/1 Running 0 7m 10.244.3.101 k8s-g2 &lt;none&gt;tf-hub-0 1/1 Running 0 7m 10.244.4.86 k8s-g1 &lt;none&gt;tf-job-dashboard-7777b6bf-qltfc 1/1 Running 0 7m 10.244.3.98 k8s-g2 &lt;none&gt;tf-job-operator-v1alpha2-5c5b4dcfdf-x5rkv 1/1 Running 0 7m 10.244.4.87 k8s-g1 &lt;none&gt;vizier-core-5584ccbd8-l2bxr 0/1 CrashLoopBackOff 4 7m 10.244.5.80 k8s-g11 &lt;none&gt;vizier-db-547967c899-mf988 0/1 Pending 0 7m &lt;none&gt; &lt;none&gt; &lt;none&gt;vizier-suggestion-grid-8547dbb55b-k8tw6 1/1 Running 0 7m 10.244.4.90 k8s-g1 &lt;none&gt;vizier-suggestion-random-d7c5cd68b-7p9nz 1/1 Running 0 7m 10.244.3.100 k8s-g2 &lt;none&gt;workflow-controller-5c95f95f58-d9plp 1/1 Running 0 7m 10.244.4.88 k8s-g1 &lt;none&gt; 可以看到vizier狀態為Pending，失敗的原因是PersistentVolume未分配給vizer-db（實際是MySQL）。所以vizier-db正在等待並且未能啟動vizier-core。 這邊透過使用kubectl describe命令檢查原因: 123456789$ kubectl -n kubeflow describe pod vizier-db-547967c899-mf988Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m (x417 over 12m) default-scheduler pod has unbound PersistentVolumeClaims (repeated 4 times)$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Pending 15m 這邊我們建立一個 NFS PV 來提供給 vizier-db 使用：1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: vizier-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv1EOF 請更換自己的 nfs-server IP 再次查看 PV &amp; PVC 狀態，就可以看到vizier-db獲得 20Gi 的 ersistentVolume 空間 1234567$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEvizier-pv 20Gi RWO Retain Bound kubeflow/vizier-db 16s$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Bound vizier-pv 20Gi RWO 20m 其他儲存方式參考：Kubernetes: Local Volumeの検証 暫時問題：modeldb-backend 會不定時 Crash Kubeflow UI查看Service中，可以看到kubeflow v0.3.0版本中，附帶了許多Web UI: Argo UI Central UI for navigation JupyterHub Katib TFJobs Dashboard 這邊我們透過Service查看一下目前pod狀態： 123456789101112131415161718$ kubectl -n kubeflow get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador ClusterIP 10.98.216.151 &lt;none&gt; 80/TCP 7m service=ambassadorambassador-admin ClusterIP 10.102.20.172 &lt;none&gt; 8877/TCP 6m service=ambassadorargo-ui NodePort 10.102.36.121 &lt;none&gt; 80:32530/TCP 6m app=argo-uicentraldashboard ClusterIP 10.99.255.3 &lt;none&gt; 80/TCP 6m app=centraldashboardk8s-dashboard ClusterIP 10.110.209.78 &lt;none&gt; 443/TCP 6m k8s-app=kubernetes-dashboardmodeldb-backend ClusterIP 10.102.179.187 &lt;none&gt; 6543/TCP 6m app=modeldb,component=backendmodeldb-db ClusterIP 10.102.143.161 &lt;none&gt; 27017/TCP 6m app=modeldb,component=dbmodeldb-frontend ClusterIP 10.100.76.92 &lt;none&gt; 3000/TCP 6m app=modeldb,component=frontendstatsd-sink ClusterIP 10.98.167.5 &lt;none&gt; 9102/TCP 7m service=ambassadortf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 6m app=tf-hubtf-hub-lb ClusterIP 10.106.99.192 &lt;none&gt; 80/TCP 6m app=tf-hubtf-job-dashboard ClusterIP 10.100.66.145 &lt;none&gt; 80/TCP 6m name=tf-job-dashboardvizier-core NodePort 10.105.154.126 &lt;none&gt; 6789:30678/TCP 6m app=vizier,component=corevizier-db ClusterIP 10.100.32.162 &lt;none&gt; 3306/TCP 6m app=vizier,component=dbvizier-suggestion-grid ClusterIP 10.105.68.247 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-gridvizier-suggestion-random ClusterIP 10.110.164.76 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-random 這時候就可以透過kubeflow UI，能引導到 登入JupyterHub，但這邊需要修改 Kubernetes Service，透過以下指令進行： 123456789101112# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc ambassador...sessionAffinity: None type: NodePortstatus: loadBalancer: {}...#修改後就可以透過NodePort接上Port進入顯示Jupyter NotebookNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador NodePort 10.98.216.151 &lt;none&gt; 80:32752/TCP 15m service=ambassador 完成後連接 http://Master_IP:Port即可看到kubeflow UI。 可以透過上方連結查看Jupyter Hub、TFjob Dashboard、Kubernetes Dashboard 等。 測試 Jupyter Notebook這邊需要再次先建立一個 NFS PV 來提供給 Kubeflow Jupyter使用： 1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: notebook-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv2EOF NFS PV 建立好後這邊UI點選Jupyter Notebook登入測試，並輸入任意帳號密碼進行登入： Spawner Optinos 這邊可以輸入Notebook Pod節點資源，包含Image、CPU、Memory、GPU數量，最後點選Spawn來完成建立 Server，如下圖所示： kubeflow v0.3版本開始加入，在部署Jupyter Hub時的Event log方便了解部署狀態 部署完後即可直接操作擁有GPU的Jupyter Notebook 補充: 透過TFJobs Dashboard \b\b顯示TFJob狀態可以透過TensorFlow Training (TFJob)定義TFjob，即可從TFJobs Dashboard查看Job資訊，也可以直接透過Dashboard建立/刪除TFJob。 這邊透單測試Tensorflow Benchmark GPU測試 範例 查看 PS狀態: 查看 Worker狀態: 查看GPU資源狀態可以透過執行以下指令確認是否GPU可被分配資源：123456789$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUk8s-g1 1k8s-g11 1k8s-g2 1k8s-g3 1k8s-m1 &lt;none&gt;k8s-m2 &lt;none&gt;k8s-m3 &lt;none&gt; 後續有陸續關注，關於 kubeflow CLI的專案kubeflow/Arena，主要供資料科學家運行和監控機器學習培訓工作並以簡單的方式檢查其結果。目前它支持單獨/分佈式TensorFlow訓練。部署基於Kubernetes，helm和Kubeflow。 同時，最終用戶需要GPU資源和節點管理。Arena還提供top檢查Kubernetes集群中可用GPU資源的命令。 123456789101112$ arena top node NAME IPADDRESS ROLE GPU(Total) GPU(Allocated)k8s-g1 172.22.132.84 &lt;none&gt; 1 0k8s-g11 172.22.132.94 &lt;none&gt; 1 0k8s-g2 172.22.132.97 &lt;none&gt; 1 0k8s-g3 172.22.132.86 &lt;none&gt; 1 1k8s-m1 172.22.132.81 master 0 0k8s-m2 172.22.132.82 master 0 0k8s-m3 172.22.132.83 master 0 0-----------------------------------------------------------------------------------------Allocated/Total GPUs In Cluster:1/5 (20%) 透過CLI方式來 watch -n1 arena top node就能很清楚知道節點資源使用狀況。 移除 kubeflow這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 1kubectl -n kubeflow delete namespace kubeflow 相關文章:[kubeflow] Local Volumeを使ったKubeflowの動作検証[kubeflow] Microk8s for Kubeflow #! 更多相關kubeflow Components實作會在後續文章陸續介紹","link":"/2018/10/23/kubeflow-deploy-v0.3.0/"},{"title":"監控工具 Zabbix API 使用記錄","text":"此篇主要記錄為，在Openstack環境下觀察第三方監控工具Zabbix使用API獲取歷史數據。 Zabbix API 監控架構 透過 Python zabbix api &amp; CURL1. user.login方法獲取 zabbix server 的認證結果curl 命令：123curl -i -X POST -H 'Content-Type:application/json' -d '{\"jsonrpc\":\"2.0\",\"method\":\"user.login\",\"params\":{\"user\":\"admin\",\"password\":\"zabbix\"},\"auth\":null,\"id\":0}' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令運行結果：1{\"jsonrpc\":\"2.0\",\"result\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\":0} or Python 腳本：123456789101112131415161718192021222324252627282930313233$ vim auth.py #/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = {\"Content-Type\":\"application/json\"}# auth user and passworddata = json.dumps({ \"jsonrpc\": \"2.0\", \"method\": \"user.login\", \"params\": { \"user\": \"admin\", \"password\": \"zabbix\"},\"id\": 0})# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# auth and get authidtry: result = urllib2.urlopen(request)except URLError as e: print \"Auth Failed, Please Check Your Name AndPassword:\",e.codeelse: response = json.loads(result.read()) result.close()print\"Auth Successful. The Auth ID Is:\",response['result'] Python 腳本運行結果：12$ python auth.pyAuth Successful. The Auth ID Is: df432b915671b77035d5e26f85c8bf5c 2. hostgroup.get方法獲取所有主機組 IDcurl 命令：1$ curl -i -X POST -H 'Content-Type:application/json' -d '{\"jsonrpc\": \"2.0\",\"method\":\"hostgroup.get\",\"params\":{\"output\":[\"groupid\",\"name\"]},\"auth\":\"1d152c36140c245665ec8dc717675fb1\",\"id\": 0}' http://10.111.200.8/zabbix/api_jsonrpc.php curl 執行結果：1{\"jsonrpc\":\"2.0\",\"result\":[{\"groupid\":\"7\",\"name\":\"Ceph Cluster\"},{\"groupid\":\"6\",\"name\":\"Ceph MONs\"},{\"groupid\":\"10\",\"name\":\"Ceph OSDs\"},{\"groupid\":\"11\",\"name\":\"Computes\"},{\"groupid\":\"9\",\"name\":\"Controllers\"},{\"groupid\":\"5\",\"name\":\"Discovered hosts\"},{\"groupid\":\"2\",\"name\":\"Linux servers\"},{\"groupid\":\"12\",\"name\":\"Load Balancers\"},{\"groupid\":\"8\",\"name\":\"ManagedByPuppet\"},{\"groupid\":\"1\",\"name\":\"Templates\"}],\"id\":0} or python 腳本：1234567891011121314151617181920212223242526272829303132333435363738394041$ vim get_hostgroup_list.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = {\"Content-Type\":\"application/json\"}# request jsondata = json.dumps({ \"jsonrpc\":\"2.0\", \"method\":\"hostgroup.get\", \"params\":{ \"output\":[\"groupid\",\"name\"], }, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,})# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) #print response for group in response['result']: print \"Group ID:\",group['groupid'],\"\\tGroupName:\",group['name'] python 腳本執行結果：123456789101112$ python get_hostgroup_list.py Number Of Hosts: 10Group ID: 7 GroupName: Ceph ClusterGroup ID: 6 GroupName: Ceph MONsGroup ID: 10 GroupName: Ceph OSDsGroup ID: 11 GroupName: ComputesGroup ID: 9 GroupName: ControllersGroup ID: 5 GroupName: Discovered hostsGroup ID: 2 GroupName: Linux serversGroup ID: 12 GroupName: Load BalancersGroup ID: 8 GroupName: ManagedByPuppetGroup ID: 1 GroupName: Templates 3. host.get方法獲取單個主機組下所有的主機 IDcurl 命令：1$ curl -i -X POST -H'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"method\":\"host.get\",\"params\":{\"output\":[\"hostid\",\"name\"],\"groupids\":\"11\"},\"auth\":\"1d152c36140c245665ec8dc717675fb1\",\"id\": 0}' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令執行結果：1234{\"jsonrpc\":\"2.0\",\"result\":[{\"hostid\":\"54\",\"name\":\"node-1.domain.tld\"},{\"hostid\":\"55\",\"name\":\"node-3.domain.tld\"},{\"hostid\":\"56\",\"name\":\"node-2.domain.tld\"}],\"id\":0} or python 腳本：123456789101112131415161718192021222324252627282930313233343536373839#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = {\"Content-Type\":\"application/json\"}# request jsondata = json.dumps({ \"jsonrpc\":\"2.0\", \"method\":\"host.get\", \"params\":{ \"output\":[\"hostid\",\"name\"], \"groupids\":\"11\", }, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,})# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本執行結果：12345$ python get_group_one.pyNumber Of Hosts: 3Host ID: 54 HostName: node-1.domain.tldHost ID: 55 HostName: node-3.domain.tldHost ID: 56 HostName: node-2.domain.tld 4. itemsid.get方法獲取單個主機下所有的監控項 ID根據標題 3 中獲取到的所有主機 id 與名稱，找到你想要獲取的主機 id ，獲取它下面的所有 items curl 命令：這邊選擇觀察 Host ID: 54 HostName: node-1.domain.tld1$ curl -i -X POST -H 'Content-Type:application/json' -d '{\"jsonrpc\":\"2.0\",\"method\":\"item.get\",\"params\":{\"output\":\"itemids\",\"hostids\":\"54\"},\"auth\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0}' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令執行結果：1{\"jsonrpc\":\"2.0\",\"result\":[{\"itemid\":\"572\"},{\"itemid\":\"573\"},{\"itemid\":\"574\"},{\"itemid\":\"416\"},{\"itemid\":\"417\"},{\"itemid\":\"418\"},{\"itemid\":\"419\"},{\"itemid\":\"420\"},{\"itemid\":\"421\"},{\"itemid\":\"422\"},{\"itemid\":\"423\"},{\"itemid\":\"458\"},{\"itemid\":\"459\"},{\"itemid\":\"460\"},{\"itemid\":\"3527\"},{\"itemid\":\"3541\"},{\"itemid\":\"3555\"},{\"itemid\":\"3528\"},{\"itemid\":\"3542\"},{\"itemid\":\"3556\"},{\"itemid\":\"3519\"},{\"itemid\":\"3533\"},{\"itemid\":\"3547\"},{\"itemid\":\"3517\"},{\"itemid\":\"3531\"},{\"itemid\":\"3545\"},{\"itemid\":\"3522\"},{\"itemid\":\"3536\"},{\"itemid\":\"3550\"},{\"itemid\":\"3518\"},{\"itemid\":\"3532\"},{\"itemid\":\"3546\"},{\"itemid\":\"3516\"},{\"itemid\":\"3530\"},{\"itemid\":\"3544\"},{\"itemid\":\"3524\"},{\"itemid\":\"3538\"},{\"itemid\":\"3552\"},{\"itemid\":\"3515\"},{\"itemid\":\"3529\"},{\"itemid\":\"3543\"},{\"itemid\":\"3520\"},{\"itemid\":\"3534\"},{\"itemid\":\"3548\"},{\"itemid\":\"3523\"},{\"itemid\":\"3537\"},{\"itemid\":\"3551\"},{\"itemid\":\"3525\"},{\"itemid\":\"3539\"},{\"itemid\":\"3553\"},{\"itemid\":\"3521\"},{\"itemid\":\"3535\"},{\"itemid\":\"3549\"},{\"itemid\":\"3526\"},{\"itemid\":\"3540\"},{\"itemid\":\"3554\"},{\"itemid\":\"4982\"},{\"itemid\":\"4986\"},{\"itemid\":\"4990\"},{\"itemid\":\"4154\"},{\"itemid\":\"4159\"},{\"itemid\":\"4164\"},{\"itemid\":\"4741\"},{\"itemid\":\"4745\"},{\"itemid\":\"4749\"},{\"itemid\":\"4888\"},{\"itemid\":\"4892\"},{\"itemid\":\"4896\"},{\"itemid\":\"4649\"},{\"itemid\":\"4653\"},{\"itemid\":\"4657\"},{\"itemid\":\"4863\"},{\"itemid\":\"4867\"},{\"itemid\":\"4871\"},{\"itemid\":\"4290\"},{\"itemid\":\"4294\"},{\"itemid\":\"4298\"},{\"itemid\":\"4481\"},{\"itemid\":\"4485\"},{\"itemid\":\"4489\"},{\"itemid\":\"4433\"},{\"itemid\":\"4437\"},{\"itemid\":\"4441\"},{\"itemid\":\"4933\"},{\"itemid\":\"4937\"},{\"itemid\":\"4941\"},{\"itemid\":\"4763\"},{\"itemid\":\"4767\"},{\"itemid\":\"4771\"},{\"itemid\":\"4981\"},{\"itemid\":\"4985\"},{\"itemid\":\"4989\"},{\"itemid\":\"4152\"},{\"itemid\":\"4157\"},{\"itemid\":\"4162\"},{\"itemid\":\"4740\"},{\"itemid\":\"4744\"},{\"itemid\":\"4748\"},{\"itemid\":\"4886\"},{\"itemid\":\"4890\"},{\"itemid\":\"4894\"},{\"itemid\":\"4651\"},{\"itemid\":\"4655\"},{\"itemid\":\"4659\"},{\"itemid\":\"4861\"},{\"itemid\":\"4865\"},{\"itemid\":\"4869\"},{\"itemid\":\"4292\"},{\"itemid\":\"4296\"},{\"itemid\":\"4300\"},{\"itemid\":\"4484\"},{\"itemid\":\"4488\"},{\"itemid\":\"4492\"},{\"itemid\":\"4436\"},{\"itemid\":\"4440\"},{\"itemid\":\"4444\"},{\"itemid\":\"4936\"},{\"itemid\":\"4940\"},{\"itemid\":\"4944\"},{\"itemid\":\"4766\"},{\"itemid\":\"4770\"},{\"itemid\":\"4774\"},{\"itemid\":\"4984\"},{\"itemid\":\"4988\"},{\"itemid\":\"4992\"},{\"itemid\":\"4151\"},{\"itemid\":\"4156\"},{\"itemid\":\"4161\"},{\"itemid\":\"4739\"},{\"itemid\":\"4743\"},{\"itemid\":\"4747\"},{\"itemid\":\"4887\"},{\"itemid\":\"4891\"},{\"itemid\":\"4895\"},{\"itemid\":\"4652\"},{\"itemid\":\"4656\"},{\"itemid\":\"4660\"},{\"itemid\":\"4862\"},{\"itemid\":\"4866\"},{\"itemid\":\"4870\"},{\"itemid\":\"4289\"},{\"itemid\":\"4293\"},{\"itemid\":\"4297\"},{\"itemid\":\"4483\"},{\"itemid\":\"4487\"},{\"itemid\":\"4491\"},{\"itemid\":\"4435\"},{\"itemid\":\"4439\"},{\"itemid\":\"4443\"},{\"itemid\":\"4935\"},{\"itemid\":\"4939\"},{\"itemid\":\"4943\"},{\"itemid\":\"4765\"},{\"itemid\":\"4769\"},{\"itemid\":\"4773\"},{\"itemid\":\"4983\"},{\"itemid\":\"4987\"},{\"itemid\":\"4991\"},{\"itemid\":\"4155\"},{\"itemid\":\"4160\"},{\"itemid\":\"4165\"},{\"itemid\":\"4742\"},{\"itemid\":\"4746\"},{\"itemid\":\"4750\"},{\"itemid\":\"4885\"},{\"itemid\":\"4889\"},{\"itemid\":\"4893\"},{\"itemid\":\"4650\"},{\"itemid\":\"4654\"},{\"itemid\":\"4658\"},{\"itemid\":\"4864\"},{\"itemid\":\"4868\"},{\"itemid\":\"4872\"},{\"itemid\":\"4291\"},{\"itemid\":\"4295\"},{\"itemid\":\"4299\"},{\"itemid\":\"4482\"},{\"itemid\":\"4486\"},{\"itemid\":\"4490\"},{\"itemid\":\"4434\"},{\"itemid\":\"4438\"},{\"itemid\":\"4442\"},{\"itemid\":\"4934\"},{\"itemid\":\"4938\"},{\"itemid\":\"4942\"},{\"itemid\":\"4764\"},{\"itemid\":\"4768\"},{\"itemid\":\"4772\"},{\"itemid\":\"4153\"},{\"itemid\":\"4158\"},{\"itemid\":\"4163\"},{\"itemid\":\"3569\"},{\"itemid\":\"3583\"},{\"itemid\":\"3597\"},{\"itemid\":\"3570\"},{\"itemid\":\"3584\"},{\"itemid\":\"3598\"},{\"itemid\":\"3561\"},{\"itemid\":\"3575\"},{\"itemid\":\"3589\"},{\"itemid\":\"3559\"},{\"itemid\":\"3573\"},{\"itemid\":\"3587\"},{\"itemid\":\"3564\"},{\"itemid\":\"3578\"},{\"itemid\":\"3592\"},{\"itemid\":\"3560\"},{\"itemid\":\"3574\"},{\"itemid\":\"3588\"},{\"itemid\":\"3558\"},{\"itemid\":\"3572\"},{\"itemid\":\"3586\"},{\"itemid\":\"3566\"},{\"itemid\":\"3580\"},{\"itemid\":\"3594\"},{\"itemid\":\"3557\"},{\"itemid\":\"3571\"},{\"itemid\":\"3585\"},{\"itemid\":\"3562\"},{\"itemid\":\"3576\"},{\"itemid\":\"3590\"},{\"itemid\":\"3565\"},{\"itemid\":\"3579\"},{\"itemid\":\"3593\"},{\"itemid\":\"3567\"},{\"itemid\":\"3581\"},{\"itemid\":\"3595\"},{\"itemid\":\"3563\"},{\"itemid\":\"3577\"},{\"itemid\":\"3591\"},{\"itemid\":\"3568\"},{\"itemid\":\"3582\"},{\"itemid\":\"3596\"},{\"itemid\":\"4994\"},{\"itemid\":\"4998\"},{\"itemid\":\"5002\"},{\"itemid\":\"4169\"},{\"itemid\":\"4174\"},{\"itemid\":\"4179\"},{\"itemid\":\"4753\"},{\"itemid\":\"4757\"},{\"itemid\":\"4761\"},{\"itemid\":\"4900\"},{\"itemid\":\"4904\"},{\"itemid\":\"4908\"},{\"itemid\":\"4661\"},{\"itemid\":\"4665\"},{\"itemid\":\"4669\"},{\"itemid\":\"4875\"},{\"itemid\":\"4879\"},{\"itemid\":\"4883\"},{\"itemid\":\"4302\"},{\"itemid\":\"4306\"},{\"itemid\":\"4310\"},{\"itemid\":\"4493\"},{\"itemid\":\"4497\"},{\"itemid\":\"4501\"},{\"itemid\":\"4445\"},{\"itemid\":\"4449\"},{\"itemid\":\"4453\"},{\"itemid\":\"4945\"},{\"itemid\":\"4949\"},{\"itemid\":\"4953\"},{\"itemid\":\"4775\"},{\"itemid\":\"4779\"},{\"itemid\":\"4783\"},{\"itemid\":\"4993\"},{\"itemid\":\"4997\"},{\"itemid\":\"5001\"},{\"itemid\":\"4167\"},{\"itemid\":\"4172\"},{\"itemid\":\"4177\"},{\"itemid\":\"4752\"},{\"itemid\":\"4756\"},{\"itemid\":\"4760\"},{\"itemid\":\"4898\"},{\"itemid\":\"4902\"},{\"itemid\":\"4906\"},{\"itemid\":\"4663\"},{\"itemid\":\"4667\"},{\"itemid\":\"4671\"},{\"itemid\":\"4873\"},{\"itemid\":\"4877\"},{\"itemid\":\"4881\"},{\"itemid\":\"4304\"},{\"itemid\":\"4308\"},{\"itemid\":\"4312\"},{\"itemid\":\"4496\"},{\"itemid\":\"4500\"},{\"itemid\":\"4504\"},{\"itemid\":\"4448\"},{\"itemid\":\"4452\"},{\"itemid\":\"4456\"},{\"itemid\":\"4948\"},{\"itemid\":\"4952\"},{\"itemid\":\"4956\"},{\"itemid\":\"4778\"},{\"itemid\":\"4782\"},{\"itemid\":\"4786\"},{\"itemid\":\"4996\"},{\"itemid\":\"5000\"},{\"itemid\":\"5004\"},{\"itemid\":\"4166\"},{\"itemid\":\"4171\"},{\"itemid\":\"4176\"},{\"itemid\":\"4751\"},{\"itemid\":\"4755\"},{\"itemid\":\"4759\"},{\"itemid\":\"4899\"},{\"itemid\":\"4903\"},{\"itemid\":\"4907\"},{\"itemid\":\"4664\"},{\"itemid\":\"4668\"},{\"itemid\":\"4672\"},{\"itemid\":\"4874\"},{\"itemid\":\"4878\"},{\"itemid\":\"4882\"},{\"itemid\":\"4301\"},{\"itemid\":\"4305\"},{\"itemid\":\"4309\"},{\"itemid\":\"4495\"},{\"itemid\":\"4499\"},{\"itemid\":\"4503\"},{\"itemid\":\"4447\"},{\"itemid\":\"4451\"},{\"itemid\":\"4455\"},{\"itemid\":\"4947\"},{\"itemid\":\"4951\"},{\"itemid\":\"4955\"},{\"itemid\":\"4777\"},{\"itemid\":\"4781\"},{\"itemid\":\"4785\"},{\"itemid\":\"4995\"},{\"itemid\":\"4999\"},{\"itemid\":\"5003\"},{\"itemid\":\"4170\"},{\"itemid\":\"4175\"},{\"itemid\":\"4180\"},{\"itemid\":\"4754\"},{\"itemid\":\"4758\"},{\"itemid\":\"4762\"},{\"itemid\":\"4897\"},{\"itemid\":\"4901\"},{\"itemid\":\"4905\"},{\"itemid\":\"4662\"},{\"itemid\":\"4666\"},{\"itemid\":\"4670\"},{\"itemid\":\"4876\"},{\"itemid\":\"4880\"},{\"itemid\":\"4884\"},{\"itemid\":\"4303\"},{\"itemid\":\"4307\"},{\"itemid\":\"4311\"},{\"itemid\":\"4494\"},{\"itemid\":\"4498\"},{\"itemid\":\"4502\"},{\"itemid\":\"4446\"},{\"itemid\":\"4450\"},{\"itemid\":\"4454\"},{\"itemid\":\"4946\"},{\"itemid\":\"4950\"},{\"itemid\":\"4954\"},{\"itemid\":\"4776\"},{\"itemid\":\"4780\"},{\"itemid\":\"4784\"},{\"itemid\":\"4168\"},{\"itemid\":\"4173\"},{\"itemid\":\"4178\"},{\"itemid\":\"461\"},{\"itemid\":\"357\"},{\"itemid\":\"509\"},{\"itemid\":\"462\"},{\"itemid\":\"565\"},{\"itemid\":\"567\"},{\"itemid\":\"356\"},{\"itemid\":\"412\"},{\"itemid\":\"498\"},{\"itemid\":\"499\"},{\"itemid\":\"429\"},{\"itemid\":\"463\"},{\"itemid\":\"464\"},{\"itemid\":\"465\"},{\"itemid\":\"466\"},{\"itemid\":\"467\"},{\"itemid\":\"468\"},{\"itemid\":\"469\"},{\"itemid\":\"470\"},{\"itemid\":\"471\"},{\"itemid\":\"472\"},{\"itemid\":\"473\"},{\"itemid\":\"474\"},{\"itemid\":\"475\"},{\"itemid\":\"476\"},{\"itemid\":\"477\"},{\"itemid\":\"478\"},{\"itemid\":\"479\"},{\"itemid\":\"480\"},{\"itemid\":\"481\"},{\"itemid\":\"482\"},{\"itemid\":\"483\"},{\"itemid\":\"484\"},{\"itemid\":\"485\"},{\"itemid\":\"486\"},{\"itemid\":\"487\"},{\"itemid\":\"488\"},{\"itemid\":\"489\"},{\"itemid\":\"3599\"},{\"itemid\":\"3600\"},{\"itemid\":\"3601\"},{\"itemid\":\"3602\"},{\"itemid\":\"490\"},{\"itemid\":\"3603\"},{\"itemid\":\"3604\"},{\"itemid\":\"3606\"},{\"itemid\":\"3605\"},{\"itemid\":\"3607\"},{\"itemid\":\"3611\"},{\"itemid\":\"3615\"},{\"itemid\":\"3619\"},{\"itemid\":\"3608\"},{\"itemid\":\"3612\"},{\"itemid\":\"3616\"},{\"itemid\":\"3620\"},{\"itemid\":\"3610\"},{\"itemid\":\"3614\"},{\"itemid\":\"3618\"},{\"itemid\":\"3622\"},{\"itemid\":\"3609\"},{\"itemid\":\"3613\"},{\"itemid\":\"3617\"},{\"itemid\":\"3621\"},{\"itemid\":\"491\"},{\"itemid\":\"5038\"},{\"itemid\":\"492\"},{\"itemid\":\"5031\"}],\"id\":0} or python 腳本：123456789101112131415161718192021222324252627282930313233343536373839404142$ vim get_items.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = {\"Content-Type\":\"application/json\"}# request jsondata = json.dumps({ \"jsonrpc\":\"2.0\", \"method\":\"item.get\", \"params\":{ \"output\":[\"itemids\",\"key_\"], \"hostids\":\"55\", }, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,})# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print host #print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本運行結果：12345678910111213141516171819202122$ python get_items.py Number Of Hosts: 435… …{u'itemid': u'469', u'key_': u'system.cpu.intr'}{u'itemid': u'470', u'key_': u'system.cpu.load[percpu,avg15]'}{u'itemid': u'471', u'key_': u'system.cpu.load[percpu,avg1]'}{u'itemid': u'472', u'key_': u'system.cpu.load[percpu,avg5]'}{u'itemid': u'473', u'key_': u'system.cpu.switches'}{u'itemid': u'474', u'key_': u'system.cpu.util[,idle]'}{u'itemid': u'475', u'key_': u'system.cpu.util[,interrupt]'}{u'itemid': u'476', u'key_': u'system.cpu.util[,iowait]'}{u'itemid': u'477', u'key_': u'system.cpu.util[,nice]'}{u'itemid': u'478', u'key_': u'system.cpu.util[,softirq]'}{u'itemid': u'479', u'key_': u'system.cpu.util[,steal]'}{u'itemid': u'480', u'key_': u'system.cpu.util[,system]'}{u'itemid': u'481', u'key_': u'system.cpu.util[,user]'}… …{u'itemid': u'491', u'key_': u'vm.memory.size[available]'}{u'itemid': u'5038', u'key_': u'vm.memory.size[cached]'}{u'itemid': u'492', u'key_': u'vm.memory.size[total]'}{u'itemid': u'5031', u'key_': u'vm.memory.size[used]'}… … 5. history.get方法獲取單個監控項的歷史數據curl 命令：1$ curl -i -X POST -H 'Content-Type:application/json' -d '{\"jsonrpc\":\"2.0\",\"method\":\"history.get\",\"params\":{\"history\":3,\"itemids\":\"480\",\"output\":\"extend\",\"limit\":10},\"auth\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0}' http://10.111.200.8/zabbix/api_jsonrpc.php 有些值結果會顯示為0, 暫時還要查看history.get獲得歷史數據的參數設定 curl 命令運行結果：1{\"jsonrpc\":\"2.0\",\"result\":[{\"itemid\":\"25154\",\"clock\":\"1410744134\",\"value\":\"4840\",\"ns\":\"375754276\"},{\"itemid\":\"25154\",\"clock\":\"1410744314\",\"value\":\"5408\",\"ns\":\"839852515\"},{\"itemid\":\"25154\",\"clock\":\"1410744374\",\"value\":\"7040\",\"ns\":\"964558609\"},{\"itemid\":\"25154\",\"clock\":\"1410744554\",\"value\":\"4072\",\"ns\":\"943177771\"},{\"itemid\":\"25154\",\"clock\":\"1410744614\",\"value\":\"8696\",\"ns\":\"995289716\"},{\"itemid\":\"25154\",\"clock\":\"1410744674\",\"value\":\"6144\",\"ns\":\"992462863\"},{\"itemid\":\"25154\",\"clock\":\"1410744734\",\"value\":\"6472\",\"ns\":\"152634327\"},{\"itemid\":\"25154\",\"clock\":\"1410744794\",\"value\":\"4312\",\"ns\":\"479599424\"},{\"itemid\":\"25154\",\"clock\":\"1410744854\",\"value\":\"4456\",\"ns\":\"263314898\"},{\"itemid\":\"25154\",\"clock\":\"1410744914\",\"value\":\"8656\",\"ns\":\"840460009\"}],\"id\":0} or python 腳本：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849$ get_items_history.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = {\"Content-Type\":\"application/json\"}# request jsonitemids_id = input('Pluse input itemids number: ')print('id ', itemids_id)data = json.dumps({ \"jsonrpc\":\"2.0\", \"method\":\"history.get\", \"params\":{ \"output\":\"extend\", \"history\":0, \"itemids\": itemids_id, \"sortfield\" : \"clock\", \"limit\": 10 }, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,})# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print host #print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本執行結果：這邊觀察{u'itemid': u'480', u'key_': u'system.cpu.util[,system]'}狀態 1234567891011121314$ python get_items_history.pyPluse input itemids number: 480('id ', 480)Number Of Hosts: 10{u'itemid': u'480', u'ns': u'366027868', u'value': u'2.3541', u'clock': u'1542254040'}{u'itemid': u'480', u'ns': u'409639017', u'value': u'2.3193', u'clock': u'1542254100'}{u'itemid': u'480', u'ns': u'476390125', u'value': u'2.2133', u'clock': u'1542254160'}{u'itemid': u'480', u'ns': u'609209011', u'value': u'2.1803', u'clock': u'1542254220'}{u'itemid': u'480', u'ns': u'8006955', u'value': u'3.6697', u'clock': u'1542254281'}{u'itemid': u'480', u'ns': u'676841696', u'value': u'5.0511', u'clock': u'1542254340'}{u'itemid': u'480', u'ns': u'652330397', u'value': u'3.6032', u'clock': u'1542254400'}{u'itemid': u'480', u'ns': u'567671572', u'value': u'3.1112', u'clock': u'1542254460'}{u'itemid': u'480', u'ns': u'553946907', u'value': u'2.5180', u'clock': u'1542254520'}{u'itemid': u'480', u'ns': u'496168653', u'value': u'2.3969', u'clock': u'1542254580'} 獲取下2018-11-21到2018-11-21期間的數據12$ curl -i -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"method\":\"history.get\",\"params\":{\"history\":0,\"itemids\":[\"480\"],\"time_from\":\"1542844800.0\",\"time_till\":\"1542758400.0\" ,\"output\":\"extend\"},\"auth\": \"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0}' http://10.111.200.8/zabbix/api_jsonrpc.php #備註參考1:獲取對應監控項一段時間內的歷史數據並格式化輸出 https://www.yangcs.net/posts/zabbix-api-introduce-and-use/ python 腳本:123456789101112131415161718192021222324252627282930313233$ vim history_data.py#!/usr/bin/env python# encoding: utf-8\"\"\"Retrieves history data for a given numeric (either int or float) item_id\"\"\"from zabbix_api import ZabbixAPIimport pprintfrom datetime import datetimeimport timeserver = \"http://172.16.241.130/zabbix\"username = \"Admin\"password = \"zabbix\"zapi = ZabbixAPI(server=server)zapi.login(username, password)item_id = \"23296\"# Create a time rangetime_till = time.mktime(datetime.now().timetuple())time_from = time_till - 60 * 60 * 24 * 10 # &lt;span id=\"inline-toc\"&gt;1.&lt;/span&gt; days# Query item's history (integer) datahistory = zapi.history.get({\"itemids\": item_id, \"time_from\": time_from, \"time_till\": time_till, \"output\": \"extend\", \"limit\": \"10\"})# If nothing was found, try getting it from history (float) dataif not len(history): history = zapi.history.get({\"itemids\": item_id, \"time_from\": time_from, \"time_till\": time_till, \"output\": \"extend\", \"limit\": \"10\", \"history\": 0})for point in history: print(\"{0}: {1}\".format(datetime.fromtimestamp(int(point['clock'])) .strftime(\"%x %X\"), point['value'])) python 運行結果:1234567891011$ python history_data.py12/29/16 09:40:16: 0.490012/29/16 09:41:16: 0.180012/29/16 09:42:16: 0.160012/29/16 09:43:16: 0.200012/29/16 09:44:16: 0.070012/29/16 09:45:16: 0.100012/29/16 09:46:16: 0.210012/29/16 09:47:16: 0.070012/29/16 09:48:16: 0.510012/29/16 09:49:16: 0.2200 #備註參考2:時間格式化輸出1234567891011121314151617181920212223242526#!/usr/bin/python# coding=utf-8import timeimport syswhile True: choose = input(\"請選擇轉換時間格式, 輸入1 : 時間轉秒, 輸入2: 秒轉時間: \") print ('Your input: ',choose ) if choose==1 : turn_sec = raw_input ('請輸入轉換時間: 2017-04-13 00:00:00: ') print ('input: ',turn_sec) a = turn_sec #輸出: 1492012800.0 print time.mktime(time.strptime(a,'%Y-%m-%d %H:%M:%S')) elif choose == 2: turn_date = float(raw_input('請輸入轉換時間: 1492012800.0: ')) #输出： 2017-04-13 00:00:00 print ('input: ',turn_date) x = time.localtime(turn_date) print time.strftime('%Y-%m-%d %H:%M:%S',x) else : print (\"Inpute Error\") 引用參考資料 [Zabbix API 使用] [python調用zabbix api接口實時展示數據] [zabbix API 獲取CPU 信息] [Zabbix Api 简介和使用]","link":"/2018/11/27/use-zabbix-api/"},{"title":"如何透過 Ansible Playbooks 部屬 Kubernetes+GPU叢集","text":"本篇記錄部署過程，主要參考「開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集」延伸[kube-ansible](https://github.com/kairen/kube-ansible)感謝KaiRen改版後增加Nvidia Docker為 ansible 部署過程中，協助增加NVIDAI Docker與k8s-device-plugin，完成Node節點環境的GPU資源使用（內文部署GPU過程記錄延伸）。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體主機： 本次 Kubernetes 安裝版本： Kubernetes v1.11.2 Etcd v3.2.9 containerd v1.1.2 節點資訊本次安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體主機： IP Address Hostname CPU Memory 192.168.0.98 VIP 192.168.0.81 k8s-m1 4 16G 192.168.0.82 k8s-m2 4 16G 192.168.0.83 k8s-m3 4 16G 192.168.0.84 k8s-g1 4 16G 192.168.0.85 k8s-g2 4 16G 192.168.0.86 k8s-g3 4 16G 192.168.0.87 k8s-g4 4 16G 所有節點事前準備安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點需要安裝 Ansible。 部署節點對其他節點不需要 SSH 密碼即可登入: 1$ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 確認環境網路DNS設定: 12345678$ echo \"nameserver 8.8.8.8\" &gt;&gt; /etc/resolvconf/resolv.conf.d/tail$ resolvconf -u$ cat /etc/resolv.conf# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)# DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTENnameserver 127.0.1.1nameserver 8.8.8.8 GPU節點事前準備 (Node)由於GPU使用需要事先安裝 CUDA &amp; NVIDIA Driver於環境部分： 透過 APT 安裝 NVIDIA Driver(v410.79) 與 CUDA 10 1234$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 部署節點（Master）Ubuntu 16.04 安裝 Ansible:123$ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成： 12345678910111213141516171819202122$ cat /usr/local/cuda/version.txtCUDA Version 10.0.130$ sudo nvidia-smiFri Dec 9 10:25:24 2018 +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 Off | N/A || 38% 28C P8 5W / 120W | 0MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 透過 Ansible 部署 Kubernetes這邊執行由kairen介紹的kube-ansible專案，並透過Ansible來部署 Kubernetes HA 叢集，透過Git取得專案: 12$ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible Kubernetes 叢集 + GPU修改inventory/hosts.ini來描述被部署的節點與群組關係： 這邊為設定節點/etc/host解析到所有主機，直接在主機IP後面直接ssh登入資訊 1234567891011121314151617$ vim inventory/hosts.ini[etcds]192.168.0.[81:83] ansible_user=ubuntu ansible_password=password[masters]192.168.0.[81:83] ansible_user=ubuntu ansible_password=password[nodes]192.168.0.84 ansible_user=ubuntu ansible_password=password192.168.0.85 ansible_user=ubuntu ansible_password=password192.168.0.86 ansible_user=ubuntu ansible_password=password192.168.0.87 ansible_user=ubuntu ansible_password=passowrd[kube-cluster:children]mastersnodes ansible_user 為節點系統 SSH 的使用者名稱。ansible_password 為節點系統 SSH 的使用者密碼。 接著編輯group_vars/all.yml來根據需求設定功能，如以下範例： 1234567891011121314151617181920212223242526272829303132$ vim group_vars/all.yml---kube_version: 1.11.2# Container runtime,# Supported: docker, nvidia-docker, containerd.container_runtime: nvidia-docker# Container network,# Supported: calico, flannel.cni_enable: truecontainer_network: calicocni_iface: \"enp0s25\" # CNI 網路綁定的網卡# Kubernetes HA extra variables.vip_interface: \"enp0s25\" # VIP 綁定的網卡vip_address: 192.168.0.98 # VIP 位址# etcd extra variables.etcd_iface: \"enp0s25\" # etcd 綁定的網卡# Kubernetes extra addonsenable_ingress: trueenable_dashboard: trueenable_logging: falseenable_monitoring: trueenable_metric_server: truegrafana_user: \"admin\"grafana_password: \"p@ssw0rd\" 上面綁定網卡若沒有輸入，通常會使用節點預設網卡(一般來說是第一張網卡)。 這邊測試發現，需要事先確認確認，所有節點中每個節點上的網卡名稱是否一致，\b實驗環境Ubuntu16.04網卡名稱都為enp0s25。完成設定group_vars/all.yml檔案後，就可以先透過 Ansible 來檢查叢集狀態： 123456789101112131415161718192021222324252627282930$ ansible -i inventory/hosts.ini all -m ping192.168.0.81 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.82 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.83 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.84 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.85 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.86 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"}192.168.0.87 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\"} 接續檢查GPU Driver是否成功運行狀態： 1234567891011121314151617181920212223242526272829303132333435363738394041$ ansible -i inventory/hosts.ini all -a \"nvidia-smi\" -b192.168.0.81 | SUCCESS | rc=0 &gt;&gt;Thu Dec 9 12:00:54 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 Off | N/A || 38% 29C P8 4W / 120W | 0MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+192.168.0.94 | SUCCESS | rc=0 &gt;&gt;Thu Dec 9 12:00:57 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 On | N/A || 40% 33C P8 7W / 120W | 323MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1202 G /usr/lib/xorg/Xorg 171MiB || 0 3191 G compiz 149MiB |+-----------------------------------------------------------------------------+ 當叢集確認沒有問題後，即可執行cluster.yml來部署 Kubernetes 叢集： 1$ ansible-playbook -i inventory/hosts.ini cluster.yml 查看元件狀態1234567$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy {\"health\": \"true\"}etcd-2 Healthy {\"health\": \"true\"}etcd-0 Healthy {\"health\": \"true\"} 123456789$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 2m v1.11.2k8s-m2 Ready master 2m v1.11.2k8s-m3 Ready master 2m v1.11.2k8s-n1 Ready &lt;none&gt; 2m v1.11.2k8s-n2 Ready &lt;none&gt; 2m v1.11.2k8s-n3 Ready &lt;none&gt; 2m v1.11.2k8s-n4 Ready &lt;none&gt; 2m v1.11.2 測試GPU節點是否可以正常運作這邊簡易部署gpu-pod測試節點divice pligin 可以正常使用 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: gpu-podspec: restartPolicy: Never containers: - image: nvidia/cuda name: cuda command: [\"nvidia-smi\"] resources: limits: nvidia.com/gpu: 1EOFpod \"gpu-pod\" created$ kubectl get po -a -o wideFlag --show-all has been deprecated, will be removed in an upcoming releaseNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEgpu-pod 0/1 Completed 0 1h 10.244.1.5 k8s-n1 &lt;none&gt;$ kubectl logs gpu-podSun Dec 9 10:26:43 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.79 Driver Version: 410.79 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 105... Off | 00000000:02:00.0 On | N/A || 40% 24C P8 N/A / 75W | 62MiB / 4032MiB | 1% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|+-----------------------------------------------------------------------------+ Addons 部署1$ ansible-playbook -i inventory/hosts.ini addons.yml 完成後即可透過 kubectl 來檢查服務，如 kubernetes-dashboard：123456$ kubectl get po,svc -n kube-system -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/kubernetes-dashboard-6948bdb78-7424h 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes-dashboard ClusterIP 10.108.226.213 &lt;none&gt; 443/TCP 1h 完成後，即可透過 API Server 的 Proxy 來存取 https://192.168.0.98:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login 登入查詢kubernetes-dashboard Token: 12345678910111213141516171819$ kubectl -n kube-system get secretNAME TYPE DATA AGEdeployment-controller-token-kmcmz kubernetes.io/service-account-token 3 1h$ kubectl -n kube-system describe secret deployment-controller-token-kmcmzName: deployment-controller-token-kmcmzNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=deployment-controller kubernetes.io/service-account.uid=e4e91ed4-fb9b-11e8-baef-d05099d079fbType: kubernetes.io/service-account-tokenData====ca.crt: 1428 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4ta21jbXoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTRlOTFlZDQtZmI5Yi0xMWU4LWJhZWYtZDA1MDk5ZDA3OWZiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.IRQUhsVU4AJ36-qNClW7htzFJis1Mf_YSySIBKYuZ7uuaCzGcXRZtJ-nPo0SFBq7XufBMydjKwKP6tmsG1NsjttC3ETX-OnCV7u9BW0DK4HX6YloS-6Ik2rN9nHOa5iRpSNwCB2l6axGofoLkIosRCYMhdUyI5E9ZIrNKV-AvKehZkFtxXQCE3DbWGiklj1QPVq2oypfkwBEZG4GSlFkxPoIkzQQTbmZDfH036hi9DpBcUJIU41IJb9npdx65NA39Oskjdwiym1z_JlAhlhnE-uCPc-IjHirw_bEcn7mhDBf-1O2kr0IVmAbczFi82aoCagTDtUjBLP7BJ3k0v0gxQ 顯示畫面： 重置叢集狀態最後若想要重新部署叢集的話，可以透過reset-cluster.yml來清除叢集： 1$ ansible-playbook -i inventory/hosts.ini reset-cluster.yml 部署補充info 網卡名稱修正參考[Ubuntu 16.04 把網卡名稱改為舊的命名方式(eth0)] 補充：若無需要HA部署(單m單n測試) IP Address Hostname CPU Memory 192.168.0.13 VIP 192.168.0.10 k8s-m1 4 16G 192.168.0.11 k8s-n1 4 16G 配置inventory/hosts.ini範例： 12345678910111213$ vim inventory/hosts.ini[etcds]192.168.0.10 ansible_user=ubuntu ansible_password=password[masters]192.168.0.10 ansible_user=ubuntu ansible_password=password[nodes]192.168.0.11 ansible_user=ubuntu ansible_password=password[kube-cluster:children]mastersnodes 修正inventory/group_vars/all.yml範例： 12345678910111213141516171819202122232425262728293031$ vim inventory/group_vars/all.yml---kube_version: 1.11.2# Container runtime,# Supported: docker, nvidia-docker, containerd.container_runtime: nvidia-docker# Container network,# Supported: calico, flannel.cni_enable: truecontainer_network: calicocni_iface: \"eth0\"# Kubernetes HA extra variables.vip_interface: \"eth0\"vip_address: 192.168.0.13# etcd extra variables.etcd_iface: \"eth0\"# Kubernetes extra addonsenable_ingress: trueenable_dashboard: trueenable_logging: falseenable_monitoring: trueenable_metric_server: truegrafana_user: \"admin\"grafana_password: \"p@ssw0rd\" 以上[補充範例]為一台Master＆一台Node節點透過部署，並修正確認網卡名稱為一致，而vip配置部分統一設值為master資訊，並且重新運行ansible HA腳本即可執行成功。","link":"/2018/12/09/ansible-deploy-kuberentes-with-gpu/"}],"tags":[{"name":"retrospect","slug":"retrospect","link":"/tags/retrospect/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"NVIDIA Driver","slug":"NVIDIA-Driver","link":"/tags/NVIDIA-Driver/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"kubeflow","slug":"kubeflow","link":"/tags/kubeflow/"},{"name":"kubecon","slug":"kubecon","link":"/tags/kubecon/"},{"name":"distributed-systems","slug":"distributed-systems","link":"/tags/distributed-systems/"},{"name":"zabbix","slug":"zabbix","link":"/tags/zabbix/"},{"name":"API","slug":"API","link":"/tags/API/"},{"name":"linpack","slug":"linpack","link":"/tags/linpack/"},{"name":"benchmark","slug":"benchmark","link":"/tags/benchmark/"},{"name":"microk8s","slug":"microk8s","link":"/tags/microk8s/"},{"name":"deploy","slug":"deploy","link":"/tags/deploy/"},{"name":"ansible","slug":"ansible","link":"/tags/ansible/"}],"categories":[{"name":"Driver","slug":"Driver","link":"/categories/Driver/"},{"name":"kubernetes","slug":"kubernetes","link":"/categories/kubernetes/"},{"name":"Zabbix","slug":"Zabbix","link":"/categories/Zabbix/"},{"name":"Linkpack","slug":"Linkpack","link":"/categories/Linkpack/"},{"name":"Kubeflow","slug":"Kubeflow","link":"/categories/Kubeflow/"},{"name":"Deploy","slug":"Deploy","link":"/categories/Deploy/"},{"name":"Ansible","slug":"Ansible","link":"/categories/Ansible/"}]}