{"meta":{"title":"Yi Yang's Blog","subtitle":"Learning Note","description":"Blog","author":"Yi Yang","url":"http://yylin1.github.io"},"pages":[{"title":"","date":"2018-10-23T13:43:25.936Z","updated":"2018-10-23T13:43:25.936Z","comments":true,"path":"google1926343a7854156d.html","permalink":"http://yylin1.github.io/google1926343a7854156d.html","excerpt":"","text":"google-site-verification: google1926343a7854156d.html"},{"title":"About Me","date":"2018-09-15T07:17:15.000Z","updated":"2018-09-17T09:37:15.800Z","comments":true,"path":"about/index.html","permalink":"http://yylin1.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-09-17T09:43:34.603Z","updated":"2018-09-17T09:43:34.603Z","comments":true,"path":"categories/index.html","permalink":"http://yylin1.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-09-17T09:43:56.985Z","updated":"2018-09-17T09:43:56.983Z","comments":true,"path":"tags/index.html","permalink":"http://yylin1.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"如何透過 Ansible Playbooks 部屬 Kubernetes+GPU叢集","slug":"ansible-deploy-kuberentes-with-gpu","date":"2018-12-09T15:20:49.000Z","updated":"2018-12-10T03:28:31.690Z","comments":true,"path":"2018/12/09/ansible-deploy-kuberentes-with-gpu/","link":"","permalink":"http://yylin1.github.io/2018/12/09/ansible-deploy-kuberentes-with-gpu/","excerpt":"本篇記錄部署過程，主要參考「開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集」延伸[kube-ansible](https://github.com/kairen/kube-ansible)感謝KaiRen改版後增加Nvidia Docker為 ansible 部署過程中，協助增加NVIDAI Docker與k8s-device-plugin，完成Node節點環境的GPU資源使用（內文部署GPU過程記錄延伸）。","text":"本篇記錄部署過程，主要參考「開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集」延伸[kube-ansible](https://github.com/kairen/kube-ansible)感謝KaiRen改版後增加Nvidia Docker為 ansible 部署過程中，協助增加NVIDAI Docker與k8s-device-plugin，完成Node節點環境的GPU資源使用（內文部署GPU過程記錄延伸）。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體主機： 本次 Kubernetes 安裝版本： Kubernetes v1.11.2 Etcd v3.2.9 containerd v1.1.2 節點資訊本次安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體主機： IP Address Hostname CPU Memory 192.168.0.98 VIP 192.168.0.81 k8s-m1 4 16G 192.168.0.82 k8s-m2 4 16G 192.168.0.83 k8s-m3 4 16G 192.168.0.84 k8s-g1 4 16G 192.168.0.85 k8s-g2 4 16G 192.168.0.86 k8s-g3 4 16G 192.168.0.87 k8s-g4 4 16G 所有節點事前準備安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點需要安裝 Ansible。 部署節點對其他節點不需要 SSH 密碼即可登入: 1$ echo \"ubuntu ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 確認環境網路DNS設定: 12345678$ echo \"nameserver 8.8.8.8\" &gt;&gt; /etc/resolvconf/resolv.conf.d/tail$ resolvconf -u$ cat /etc/resolv.conf# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)# DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTENnameserver 127.0.1.1nameserver 8.8.8.8 GPU節點事前準備 (Node)由於GPU使用需要事先安裝 CUDA &amp; NVIDIA Driver於環境部分： 透過 APT 安裝 NVIDIA Driver(v410.79) 與 CUDA 10 1234$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 部署節點（Master）Ubuntu 16.04 安裝 Ansible:123$ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成： 12345678910111213141516171819202122$ cat /usr/local/cuda/version.txtCUDA Version 10.0.130$ sudo nvidia-smiFri Dec 9 10:25:24 2018 +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 Off | N/A || 38% 28C P8 5W / 120W | 0MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 透過 Ansible 部署 Kubernetes這邊執行由kairen介紹的kube-ansible專案，並透過Ansible來部署 Kubernetes HA 叢集，透過Git取得專案: 12$ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible Kubernetes 叢集 + GPU修改inventory/hosts.ini來描述被部署的節點與群組關係： 這邊為設定節點/etc/host解析到所有主機，直接在主機IP後面直接ssh登入資訊 1234567891011121314151617$ vim inventory/hosts.ini[etcds]192.168.0.[81:83] ansible_user=ubuntu ansible_password=password[masters]192.168.0.[81:83] ansible_user=ubuntu ansible_password=password[nodes]192.168.0.84 ansible_user=ubuntu ansible_password=password192.168.0.85 ansible_user=ubuntu ansible_password=password192.168.0.86 ansible_user=ubuntu ansible_password=password192.168.0.87 ansible_user=ubuntu ansible_password=passowrd[kube-cluster:children]mastersnodes ansible_user 為節點系統 SSH 的使用者名稱。ansible_password 為節點系統 SSH 的使用者密碼。 接著編輯group_vars/all.yml來根據需求設定功能，如以下範例： 1234567891011121314151617181920212223242526272829303132$ vim group_vars/all.yml---kube_version: 1.11.2# Container runtime,# Supported: docker, nvidia-docker, containerd.container_runtime: nvidia-docker# Container network,# Supported: calico, flannel.cni_enable: truecontainer_network: calicocni_iface: \"enp0s25\" # CNI 網路綁定的網卡# Kubernetes HA extra variables.vip_interface: \"enp0s25\" # VIP 綁定的網卡vip_address: 192.168.0.98 # VIP 位址# etcd extra variables.etcd_iface: \"enp0s25\" # etcd 綁定的網卡# Kubernetes extra addonsenable_ingress: trueenable_dashboard: trueenable_logging: falseenable_monitoring: trueenable_metric_server: truegrafana_user: \"admin\"grafana_password: \"p@ssw0rd\" 上面綁定網卡若沒有輸入，通常會使用節點預設網卡(一般來說是第一張網卡)。 這邊測試發現，需要事先確認確認，所有節點中每個節點上的網卡名稱是否一致，\b實驗環境Ubuntu16.04網卡名稱都為enp0s25。完成設定group_vars/all.yml檔案後，就可以先透過 Ansible 來檢查叢集狀態： 123456789101112131415161718192021222324252627282930$ ansible -i inventory/hosts.ini all -m ping192.168.0.81 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.82 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.83 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.84 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.85 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.86 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.0.87 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125; 接續檢查GPU Driver是否成功運行狀態： 1234567891011121314151617181920212223242526272829303132333435363738394041$ ansible -i inventory/hosts.ini all -a \"nvidia-smi\" -b192.168.0.81 | SUCCESS | rc=0 &gt;&gt;Thu Dec 9 12:00:54 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 Off | N/A || 38% 29C P8 4W / 120W | 0MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+192.168.0.94 | SUCCESS | rc=0 &gt;&gt;Thu Dec 9 12:00:57 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48 Driver Version: 410.48 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 On | N/A || 40% 33C P8 7W / 120W | 323MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1202 G /usr/lib/xorg/Xorg 171MiB || 0 3191 G compiz 149MiB |+-----------------------------------------------------------------------------+ 當叢集確認沒有問題後，即可執行cluster.yml來部署 Kubernetes 叢集： 1$ ansible-playbook -i inventory/hosts.ini cluster.yml 查看元件狀態1234567$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-0 Healthy &#123;\"health\": \"true\"&#125; 123456789$ kubectl get noNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 2m v1.11.2k8s-m2 Ready master 2m v1.11.2k8s-m3 Ready master 2m v1.11.2k8s-n1 Ready &lt;none&gt; 2m v1.11.2k8s-n2 Ready &lt;none&gt; 2m v1.11.2k8s-n3 Ready &lt;none&gt; 2m v1.11.2k8s-n4 Ready &lt;none&gt; 2m v1.11.2 測試GPU節點是否可以正常運作這邊簡易部署gpu-pod測試節點divice pligin 可以正常使用 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata: name: gpu-podspec: restartPolicy: Never containers: - image: nvidia/cuda name: cuda command: [\"nvidia-smi\"] resources: limits: nvidia.com/gpu: 1EOFpod \"gpu-pod\" createdkubectl get po -a -o wideFlag --show-all has been deprecated, will be removed in an upcoming releaseNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEgpu-pod 0/1 Completed 0 1h 10.244.1.5 k8s-n1 &lt;none&gt;kubectl logs gpu-podSun Dec 9 10:26:43 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.79 Driver Version: 410.79 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 105... Off | 00000000:02:00.0 On | N/A || 40% 24C P8 N/A / 75W | 62MiB / 4032MiB | 1% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|+-----------------------------------------------------------------------------+ Addons 部署1$ ansible-playbook -i inventory/hosts.ini addons.yml 完成後即可透過 kubectl 來檢查服務，如 kubernetes-dashboard：123456$ kubectl get po,svc -n kube-system -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/kubernetes-dashboard-6948bdb78-7424h 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes-dashboard ClusterIP 10.108.226.213 &lt;none&gt; 443/TCP 1h 完成後，即可透過 API Server 的 Proxy 來存取 https://192.168.0.98:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login 登入查詢kubernetes-dashboard Token: 12345678910111213141516171819$ kubectl -n kube-system get secretNAME TYPE DATA AGEdeployment-controller-token-kmcmz kubernetes.io/service-account-token 3 1h$ kubectl -n kube-system describe secret deployment-controller-token-kmcmzName: deployment-controller-token-kmcmzNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=deployment-controller kubernetes.io/service-account.uid=e4e91ed4-fb9b-11e8-baef-d05099d079fbType: kubernetes.io/service-account-tokenData====ca.crt: 1428 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4ta21jbXoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTRlOTFlZDQtZmI5Yi0xMWU4LWJhZWYtZDA1MDk5ZDA3OWZiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.IRQUhsVU4AJ36-qNClW7htzFJis1Mf_YSySIBKYuZ7uuaCzGcXRZtJ-nPo0SFBq7XufBMydjKwKP6tmsG1NsjttC3ETX-OnCV7u9BW0DK4HX6YloS-6Ik2rN9nHOa5iRpSNwCB2l6axGofoLkIosRCYMhdUyI5E9ZIrNKV-AvKehZkFtxXQCE3DbWGiklj1QPVq2oypfkwBEZG4GSlFkxPoIkzQQTbmZDfH036hi9DpBcUJIU41IJb9npdx65NA39Oskjdwiym1z_JlAhlhnE-uCPc-IjHirw_bEcn7mhDBf-1O2kr0IVmAbczFi82aoCagTDtUjBLP7BJ3k0v0gxQ 顯示畫面： 重置叢集狀態最後若想要重新部署叢集的話，可以透過reset-cluster.yml來清除叢集： 1$ ansible-playbook -i inventory/hosts.ini reset-cluster.yml 部署補充info 網卡名稱修正參考[Ubuntu 16.04 把網卡名稱改為舊的命名方式(eth0)] 補充：若無需要HA部署(單m單n測試) IP Address Hostname CPU Memory 192.168.0.13 VIP 192.168.0.10 k8s-m1 4 16G 192.168.0.11 k8s-n1 4 16G 配置inventory/hosts.ini範例： 12345678910111213$ vim inventory/hosts.ini[etcds]192.168.0.10 ansible_user=ubuntu ansible_password=password[masters]192.168.0.10 ansible_user=ubuntu ansible_password=password[nodes]192.168.0.11 ansible_user=ubuntu ansible_password=password[kube-cluster:children]mastersnodes 修正inventory/group_vars/all.yml範例： 12345678910111213141516171819202122232425262728293031$ vim inventory/group_vars/all.yml---kube_version: 1.11.2# Container runtime,# Supported: docker, nvidia-docker, containerd.container_runtime: nvidia-docker# Container network,# Supported: calico, flannel.cni_enable: truecontainer_network: calicocni_iface: \"eth0\"# Kubernetes HA extra variables.vip_interface: \"eth0\"vip_address: 192.168.0.13# etcd extra variables.etcd_iface: \"eth0\"# Kubernetes extra addonsenable_ingress: trueenable_dashboard: trueenable_logging: falseenable_monitoring: trueenable_metric_server: truegrafana_user: \"admin\"grafana_password: \"p@ssw0rd\" 以上[補充範例]為一台Master＆一台Node節點透過部署，並修正確認網卡名稱為一致，而vip配置部分統一設值為master資訊，並且重新運行ansible HA腳本即可執行成功。","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://yylin1.github.io/tags/ubuntu/"},{"name":"GPU","slug":"GPU","permalink":"http://yylin1.github.io/tags/GPU/"},{"name":"NVIDIA Driver","slug":"NVIDIA-Driver","permalink":"http://yylin1.github.io/tags/NVIDIA-Driver/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"ansible","slug":"ansible","permalink":"http://yylin1.github.io/tags/ansible/"}]},{"title":"安裝NVIDIA顯卡驅動時可能遇到的問題","slug":"install-nvidia-driver-problem","date":"2018-12-01T15:21:20.000Z","updated":"2018-12-09T14:45:45.498Z","comments":true,"path":"2018/12/01/install-nvidia-driver-problem/","link":"","permalink":"http://yylin1.github.io/2018/12/01/install-nvidia-driver-problem/","excerpt":"本篇主要介紹如何解決Ubuntu環境安裝NVIDIA GPU Driver後，會出現「循環登入的問題」，當裝完驅動重啟後，輸入登錄密碼之後，桌面一閃就退回到登錄界面了，然後就陷入到了輸入密碼登錄、彈出的循環，這時就需要進行顯卡驅動程序的卸載重裝。","text":"本篇主要介紹如何解決Ubuntu環境安裝NVIDIA GPU Driver後，會出現「循環登入的問題」，當裝完驅動重啟後，輸入登錄密碼之後，桌面一閃就退回到登錄界面了，然後就陷入到了輸入密碼登錄、彈出的循環，這時就需要進行顯卡驅動程序的卸載重裝。 卸載方法如下:首先在登錄介面進入到Linux的shell ie tty model，同時按下Ctrl+Alt+F1 （F1~F6其中一個就可以），然後輸入使用者輸入使用者帳號密碼，成功進入到shell，開始卸載NVIDIA驅動： 卸載乾淨所有安裝過的nvidia驅動12$ sudo apt-get remove --purge nvidia-*$ sudo apt-get autoremove 並透過直接下驅動解除安裝 1$ sudo nvidia-uninstall 再次檢查，透過 dpkg 檢查是否有非透過 apt-get 安裝的需要移除12$ sudo dpkg -l 'nvidia'$ sudo dpkg --remove nvidia-&#123;name&#125; 完成步驟後重啟系統 1$ reboot 重新安裝 NVIDIA Driver官網連結：NVIDIA Driver 下載連結 這邊測試環境GPU 為 GTX 1080 再次透過登入介面Ctrl+Alt+F1切換到tty1執行 1234567# 關閉X server$ sudo service lightdm stop $ sudo ./NVIDIA-Linux-x86_64-410.78.run -no-x-check -no-nouveau-check -no-opengl-files# -no-x-check 安裝時關閉X Server ;# -no-nouveau-check 安装驅動時禁用Nouveau# -no-opengl-files 安装時只裝驅動檔案，不安裝Opengl 安裝好後即，再次重啟電腦reboot 1sudo service lightdm restart 重啟後即可透過UI介面登入環境，就可以解決循環登入的問題。 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：123456789101112131415161718$ sudo nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.87 Driver Version: 390.87 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 108... Off | 00000000:02:00.0 On | N/A || 26% 40C P8 16W / 250W | 328MiB / 11173MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 837 C python 157MiB || 0 1250 G /usr/lib/xorg/Xorg 159MiB |+-----------------------------------------------------------------------------+ 補充：安裝NVIDIA顯卡驅動是可能遇到的問題：出現An error occurred while p erforming the step : &quot; Building kernel modules &quot;這個問題: 問題原因 Linix系統的內核是在不斷更新的，而安裝的NVIDIA驅動是之前下載好的，沒有更新，因此安裝過程中無法創建內核。 解決方法 這時候從NVIDIA下載驅動對應新版本的驅動，並安裝執行以上步驟即可。 相關狀況參考資料 |ubuntu 16.04 循环登录Ubuntu 16.04 安裝 CUDA + cuDNN + nvidia driver 的踩雷心得 (非安裝步驟詳解)安裝 NVIDIA Docker 2 來讓容器使用 GPU","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://yylin1.github.io/tags/ubuntu/"},{"name":"GPU","slug":"GPU","permalink":"http://yylin1.github.io/tags/GPU/"},{"name":"NVIDIA Driver","slug":"NVIDIA-Driver","permalink":"http://yylin1.github.io/tags/NVIDIA-Driver/"}]},{"title":"透過 Zabbix API 監測Trigger狀態","slug":"zabbix-api-trigger","date":"2018-11-29T15:55:00.000Z","updated":"2018-11-29T16:21:07.399Z","comments":true,"path":"2018/11/29/zabbix-api-trigger/","link":"","permalink":"http://yylin1.github.io/2018/11/29/zabbix-api-trigger/","excerpt":"此篇主要說明如何透過，Zabbix提供的觸發器(Trigger)來監測CPU超標狀態，並透過python直接呼叫Zabbix API獲取所需要的狀態資訊，並協助監測遷移。","text":"此篇主要說明如何透過，Zabbix提供的觸發器(Trigger)來監測CPU超標狀態，並透過python直接呼叫Zabbix API獲取所需要的狀態資訊，並協助監測遷移。 1. 新建Zabbix Trigger觸發器本節你會學習如何配置一個觸發器（trigger） 首先登入你Zabbix網頁並輸入帳號密碼 範例：1http://localhost/zabbix/ 進入Zabbix網站後，開始配置觸發器（trigger） 前往配置（Configuration）→ 主機（Hosts）→ 選擇你要進行配置觸發器（trigger）的節點 右側可以選擇要設置的環境，這邊Group我們選擇Compute節點群 接續會顯示Compute節點中可以觀察的三台Node主機 → 這邊我們直接點選node-1.domain.tld預備測試的節點進入設置 進入節點點擊上方的觸發器（Triggers），然後即可點擊創建觸發器（Create trigger），這將會向我們顯示一個觸發器定義表單 找到’新增主機（New host）’，點擊旁邊的觸發器（Triggers），然後點擊創建觸發器（Create trigger）。這將會向我們展現一個觸發器定義表單。 2. 設置Zabbix Trigger表達式由於我們需要持續觀察Computer節點上面CPU狀態，Trigger設定這邊可以直接透過語法進行觀察如下: 需要連續三分鐘CPU使用率平均值超過80%觸發報警 名稱（Name） 可以輸入: “CPU load too high on ‘node-1.domain.tld’ for 3 minutes”作為值。這個值會作為觸發器的名稱被現實在列表和其他地方。 表達式（Expression）1&#123;node-1.domain.tld:system.cpu.util[,idle].max(3m)&#125;&lt;20 Trigger 觀察主要透過system.cpu.util[,idle]狀態顯示，反向思考如果CPU能使用空閒小於20%，即CPU佔用超過80%立即觸發報警 把Trigger表達式打在Expression中 可以限制Trigger Severity警告提式: 這邊直接設定為High警告 完成後，點擊添加（Add）。新的觸發器將會顯示在觸發器列表中。 3. 顯示觸發器狀態 創建好的Trigger即可馬上列表出目前狀態 如果要查看Trigger目前監控狀態可以透過，前往監控（Monitoring） → 觸發器（Triggers），3分鐘後（我們需要等待3分鐘以評估這個觸發器的3分鐘平均值），觸發器會在這裡顯示。應該會有一個綠色的’OK’在’狀態（Status）’列中閃爍。 接下來可以透過存放於Compute node-1.domain.tld相關VM，來提升CPU使用率，達到Compute超標，這邊範例測試為超標CPU 50%表示為警告。 一般情況，綠色為system.cpu.util[,idle]狀態幾乎是很空閒的 超標狀態，可以明顯看到右側綠色能使用的system.cpu.util[,idle]明顯剩餘50%以下 從Zabbix 觀察Trigger狀態後，此處出現一個閃爍的紅色’PROBLEM’,顯然，這說明了CPU負載已經超過了你在觸發器裡定義的閾值。 4. 透過Python獲得Zabbix API 獲取Trigger超標狀態 參考 GitHub: 連結 此部分主要透過API方式取得Trigger.get欄位超標狀態，如果出現Trigger顯示為紅色閃爍的紅色PROBLEM，即可抓取到status:0的狀態，如果沒有超標則無法獲得資訊。 執行狀態可以直接獲得超標狀態123$ python zabbix-get-trigger.py Trigger message list: [ CPU load too high on 'node-1.domain.tld' for 3 minutes limit 50% ] view :status 0 若無超標及無法抓到Trigger列表的狀態123$ python zabbix-get-trigger.py No Trigger load high problem in list. 參考資料: [原创]Python利用Zabbix API定时报告存在报警的机器（更新：针对zabbix3.x） Zabbix Trigger表達式實例","categories":[],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"http://yylin1.github.io/tags/zabbix/"},{"name":"API","slug":"API","permalink":"http://yylin1.github.io/tags/API/"}]},{"title":"監控工具 Zabbix API 使用記錄","slug":"use-zabbix-api","date":"2018-11-26T16:36:51.000Z","updated":"2018-11-29T16:29:58.696Z","comments":true,"path":"2018/11/27/use-zabbix-api/","link":"","permalink":"http://yylin1.github.io/2018/11/27/use-zabbix-api/","excerpt":"此篇主要記錄為，在Openstack環境下觀察第三方監控工具Zabbix使用API獲取歷史數據。","text":"此篇主要記錄為，在Openstack環境下觀察第三方監控工具Zabbix使用API獲取歷史數據。 Zabbix API 監控架構 透過 Python zabbix api &amp; CURL1. user.login方法獲取 zabbix server 的認證結果curl 命令：123curl -i -X POST -H 'Content-Type:application/json' -d '&#123;\"jsonrpc\":\"2.0\",\"method\":\"user.login\",\"params\":&#123;\"user\":\"admin\",\"password\":\"zabbix\"&#125;,\"auth\":null,\"id\":0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令運行結果：1&#123;\"jsonrpc\":\"2.0\",\"result\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\":0&#125; or Python 腳本：123456789101112131415161718192021222324252627282930313233$ vim auth.py #/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = &#123;\"Content-Type\":\"application/json\"&#125;# auth user and passworddata = json.dumps(&#123; \"jsonrpc\": \"2.0\", \"method\": \"user.login\", \"params\": &#123; \"user\": \"admin\", \"password\": \"zabbix\"&#125;,\"id\": 0&#125;)# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# auth and get authidtry: result = urllib2.urlopen(request)except URLError as e: print \"Auth Failed, Please Check Your Name AndPassword:\",e.codeelse: response = json.loads(result.read()) result.close()print\"Auth Successful. The Auth ID Is:\",response['result'] Python 腳本運行結果：12$ python auth.pyAuth Successful. The Auth ID Is: df432b915671b77035d5e26f85c8bf5c 2. hostgroup.get方法獲取所有主機組 IDcurl 命令：1$ curl -i -X POST -H 'Content-Type:application/json' -d '&#123;\"jsonrpc\": \"2.0\",\"method\":\"hostgroup.get\",\"params\":&#123;\"output\":[\"groupid\",\"name\"]&#125;,\"auth\":\"1d152c36140c245665ec8dc717675fb1\",\"id\": 0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php curl 執行結果：1&#123;\"jsonrpc\":\"2.0\",\"result\":[&#123;\"groupid\":\"7\",\"name\":\"Ceph Cluster\"&#125;,&#123;\"groupid\":\"6\",\"name\":\"Ceph MONs\"&#125;,&#123;\"groupid\":\"10\",\"name\":\"Ceph OSDs\"&#125;,&#123;\"groupid\":\"11\",\"name\":\"Computes\"&#125;,&#123;\"groupid\":\"9\",\"name\":\"Controllers\"&#125;,&#123;\"groupid\":\"5\",\"name\":\"Discovered hosts\"&#125;,&#123;\"groupid\":\"2\",\"name\":\"Linux servers\"&#125;,&#123;\"groupid\":\"12\",\"name\":\"Load Balancers\"&#125;,&#123;\"groupid\":\"8\",\"name\":\"ManagedByPuppet\"&#125;,&#123;\"groupid\":\"1\",\"name\":\"Templates\"&#125;],\"id\":0&#125; or python 腳本：1234567891011121314151617181920212223242526272829303132333435363738394041$ vim get_hostgroup_list.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = &#123;\"Content-Type\":\"application/json\"&#125;# request jsondata = json.dumps(&#123; \"jsonrpc\":\"2.0\", \"method\":\"hostgroup.get\", \"params\":&#123; \"output\":[\"groupid\",\"name\"], &#125;, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,&#125;)# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) #print response for group in response['result']: print \"Group ID:\",group['groupid'],\"\\tGroupName:\",group['name'] python 腳本執行結果：123456789101112$ python get_hostgroup_list.py Number Of Hosts: 10Group ID: 7 GroupName: Ceph ClusterGroup ID: 6 GroupName: Ceph MONsGroup ID: 10 GroupName: Ceph OSDsGroup ID: 11 GroupName: ComputesGroup ID: 9 GroupName: ControllersGroup ID: 5 GroupName: Discovered hostsGroup ID: 2 GroupName: Linux serversGroup ID: 12 GroupName: Load BalancersGroup ID: 8 GroupName: ManagedByPuppetGroup ID: 1 GroupName: Templates 3. host.get方法獲取單個主機組下所有的主機 IDcurl 命令：1$ curl -i -X POST -H'Content-Type: application/json' -d '&#123;\"jsonrpc\":\"2.0\",\"method\":\"host.get\",\"params\":&#123;\"output\":[\"hostid\",\"name\"],\"groupids\":\"11\"&#125;,\"auth\":\"1d152c36140c245665ec8dc717675fb1\",\"id\": 0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令執行結果：1234&#123;\"jsonrpc\":\"2.0\",\"result\":[&#123;\"hostid\":\"54\",\"name\":\"node-1.domain.tld\"&#125;,&#123;\"hostid\":\"55\",\"name\":\"node-3.domain.tld\"&#125;,&#123;\"hostid\":\"56\",\"name\":\"node-2.domain.tld\"&#125;],\"id\":0&#125; or python 腳本：123456789101112131415161718192021222324252627282930313233343536373839#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = &#123;\"Content-Type\":\"application/json\"&#125;# request jsondata = json.dumps(&#123; \"jsonrpc\":\"2.0\", \"method\":\"host.get\", \"params\":&#123; \"output\":[\"hostid\",\"name\"], \"groupids\":\"11\", &#125;, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,&#125;)# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本執行結果：12345$ python get_group_one.pyNumber Of Hosts: 3Host ID: 54 HostName: node-1.domain.tldHost ID: 55 HostName: node-3.domain.tldHost ID: 56 HostName: node-2.domain.tld 4. itemsid.get方法獲取單個主機下所有的監控項 ID根據標題 3 中獲取到的所有主機 id 與名稱，找到你想要獲取的主機 id ，獲取它下面的所有 items curl 命令：這邊選擇觀察 Host ID: 54 HostName: node-1.domain.tld1$ curl -i -X POST -H 'Content-Type:application/json' -d '&#123;\"jsonrpc\":\"2.0\",\"method\":\"item.get\",\"params\":&#123;\"output\":\"itemids\",\"hostids\":\"54\"&#125;,\"auth\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php curl 命令執行結果：1&#123;\"jsonrpc\":\"2.0\",\"result\":[&#123;\"itemid\":\"572\"&#125;,&#123;\"itemid\":\"573\"&#125;,&#123;\"itemid\":\"574\"&#125;,&#123;\"itemid\":\"416\"&#125;,&#123;\"itemid\":\"417\"&#125;,&#123;\"itemid\":\"418\"&#125;,&#123;\"itemid\":\"419\"&#125;,&#123;\"itemid\":\"420\"&#125;,&#123;\"itemid\":\"421\"&#125;,&#123;\"itemid\":\"422\"&#125;,&#123;\"itemid\":\"423\"&#125;,&#123;\"itemid\":\"458\"&#125;,&#123;\"itemid\":\"459\"&#125;,&#123;\"itemid\":\"460\"&#125;,&#123;\"itemid\":\"3527\"&#125;,&#123;\"itemid\":\"3541\"&#125;,&#123;\"itemid\":\"3555\"&#125;,&#123;\"itemid\":\"3528\"&#125;,&#123;\"itemid\":\"3542\"&#125;,&#123;\"itemid\":\"3556\"&#125;,&#123;\"itemid\":\"3519\"&#125;,&#123;\"itemid\":\"3533\"&#125;,&#123;\"itemid\":\"3547\"&#125;,&#123;\"itemid\":\"3517\"&#125;,&#123;\"itemid\":\"3531\"&#125;,&#123;\"itemid\":\"3545\"&#125;,&#123;\"itemid\":\"3522\"&#125;,&#123;\"itemid\":\"3536\"&#125;,&#123;\"itemid\":\"3550\"&#125;,&#123;\"itemid\":\"3518\"&#125;,&#123;\"itemid\":\"3532\"&#125;,&#123;\"itemid\":\"3546\"&#125;,&#123;\"itemid\":\"3516\"&#125;,&#123;\"itemid\":\"3530\"&#125;,&#123;\"itemid\":\"3544\"&#125;,&#123;\"itemid\":\"3524\"&#125;,&#123;\"itemid\":\"3538\"&#125;,&#123;\"itemid\":\"3552\"&#125;,&#123;\"itemid\":\"3515\"&#125;,&#123;\"itemid\":\"3529\"&#125;,&#123;\"itemid\":\"3543\"&#125;,&#123;\"itemid\":\"3520\"&#125;,&#123;\"itemid\":\"3534\"&#125;,&#123;\"itemid\":\"3548\"&#125;,&#123;\"itemid\":\"3523\"&#125;,&#123;\"itemid\":\"3537\"&#125;,&#123;\"itemid\":\"3551\"&#125;,&#123;\"itemid\":\"3525\"&#125;,&#123;\"itemid\":\"3539\"&#125;,&#123;\"itemid\":\"3553\"&#125;,&#123;\"itemid\":\"3521\"&#125;,&#123;\"itemid\":\"3535\"&#125;,&#123;\"itemid\":\"3549\"&#125;,&#123;\"itemid\":\"3526\"&#125;,&#123;\"itemid\":\"3540\"&#125;,&#123;\"itemid\":\"3554\"&#125;,&#123;\"itemid\":\"4982\"&#125;,&#123;\"itemid\":\"4986\"&#125;,&#123;\"itemid\":\"4990\"&#125;,&#123;\"itemid\":\"4154\"&#125;,&#123;\"itemid\":\"4159\"&#125;,&#123;\"itemid\":\"4164\"&#125;,&#123;\"itemid\":\"4741\"&#125;,&#123;\"itemid\":\"4745\"&#125;,&#123;\"itemid\":\"4749\"&#125;,&#123;\"itemid\":\"4888\"&#125;,&#123;\"itemid\":\"4892\"&#125;,&#123;\"itemid\":\"4896\"&#125;,&#123;\"itemid\":\"4649\"&#125;,&#123;\"itemid\":\"4653\"&#125;,&#123;\"itemid\":\"4657\"&#125;,&#123;\"itemid\":\"4863\"&#125;,&#123;\"itemid\":\"4867\"&#125;,&#123;\"itemid\":\"4871\"&#125;,&#123;\"itemid\":\"4290\"&#125;,&#123;\"itemid\":\"4294\"&#125;,&#123;\"itemid\":\"4298\"&#125;,&#123;\"itemid\":\"4481\"&#125;,&#123;\"itemid\":\"4485\"&#125;,&#123;\"itemid\":\"4489\"&#125;,&#123;\"itemid\":\"4433\"&#125;,&#123;\"itemid\":\"4437\"&#125;,&#123;\"itemid\":\"4441\"&#125;,&#123;\"itemid\":\"4933\"&#125;,&#123;\"itemid\":\"4937\"&#125;,&#123;\"itemid\":\"4941\"&#125;,&#123;\"itemid\":\"4763\"&#125;,&#123;\"itemid\":\"4767\"&#125;,&#123;\"itemid\":\"4771\"&#125;,&#123;\"itemid\":\"4981\"&#125;,&#123;\"itemid\":\"4985\"&#125;,&#123;\"itemid\":\"4989\"&#125;,&#123;\"itemid\":\"4152\"&#125;,&#123;\"itemid\":\"4157\"&#125;,&#123;\"itemid\":\"4162\"&#125;,&#123;\"itemid\":\"4740\"&#125;,&#123;\"itemid\":\"4744\"&#125;,&#123;\"itemid\":\"4748\"&#125;,&#123;\"itemid\":\"4886\"&#125;,&#123;\"itemid\":\"4890\"&#125;,&#123;\"itemid\":\"4894\"&#125;,&#123;\"itemid\":\"4651\"&#125;,&#123;\"itemid\":\"4655\"&#125;,&#123;\"itemid\":\"4659\"&#125;,&#123;\"itemid\":\"4861\"&#125;,&#123;\"itemid\":\"4865\"&#125;,&#123;\"itemid\":\"4869\"&#125;,&#123;\"itemid\":\"4292\"&#125;,&#123;\"itemid\":\"4296\"&#125;,&#123;\"itemid\":\"4300\"&#125;,&#123;\"itemid\":\"4484\"&#125;,&#123;\"itemid\":\"4488\"&#125;,&#123;\"itemid\":\"4492\"&#125;,&#123;\"itemid\":\"4436\"&#125;,&#123;\"itemid\":\"4440\"&#125;,&#123;\"itemid\":\"4444\"&#125;,&#123;\"itemid\":\"4936\"&#125;,&#123;\"itemid\":\"4940\"&#125;,&#123;\"itemid\":\"4944\"&#125;,&#123;\"itemid\":\"4766\"&#125;,&#123;\"itemid\":\"4770\"&#125;,&#123;\"itemid\":\"4774\"&#125;,&#123;\"itemid\":\"4984\"&#125;,&#123;\"itemid\":\"4988\"&#125;,&#123;\"itemid\":\"4992\"&#125;,&#123;\"itemid\":\"4151\"&#125;,&#123;\"itemid\":\"4156\"&#125;,&#123;\"itemid\":\"4161\"&#125;,&#123;\"itemid\":\"4739\"&#125;,&#123;\"itemid\":\"4743\"&#125;,&#123;\"itemid\":\"4747\"&#125;,&#123;\"itemid\":\"4887\"&#125;,&#123;\"itemid\":\"4891\"&#125;,&#123;\"itemid\":\"4895\"&#125;,&#123;\"itemid\":\"4652\"&#125;,&#123;\"itemid\":\"4656\"&#125;,&#123;\"itemid\":\"4660\"&#125;,&#123;\"itemid\":\"4862\"&#125;,&#123;\"itemid\":\"4866\"&#125;,&#123;\"itemid\":\"4870\"&#125;,&#123;\"itemid\":\"4289\"&#125;,&#123;\"itemid\":\"4293\"&#125;,&#123;\"itemid\":\"4297\"&#125;,&#123;\"itemid\":\"4483\"&#125;,&#123;\"itemid\":\"4487\"&#125;,&#123;\"itemid\":\"4491\"&#125;,&#123;\"itemid\":\"4435\"&#125;,&#123;\"itemid\":\"4439\"&#125;,&#123;\"itemid\":\"4443\"&#125;,&#123;\"itemid\":\"4935\"&#125;,&#123;\"itemid\":\"4939\"&#125;,&#123;\"itemid\":\"4943\"&#125;,&#123;\"itemid\":\"4765\"&#125;,&#123;\"itemid\":\"4769\"&#125;,&#123;\"itemid\":\"4773\"&#125;,&#123;\"itemid\":\"4983\"&#125;,&#123;\"itemid\":\"4987\"&#125;,&#123;\"itemid\":\"4991\"&#125;,&#123;\"itemid\":\"4155\"&#125;,&#123;\"itemid\":\"4160\"&#125;,&#123;\"itemid\":\"4165\"&#125;,&#123;\"itemid\":\"4742\"&#125;,&#123;\"itemid\":\"4746\"&#125;,&#123;\"itemid\":\"4750\"&#125;,&#123;\"itemid\":\"4885\"&#125;,&#123;\"itemid\":\"4889\"&#125;,&#123;\"itemid\":\"4893\"&#125;,&#123;\"itemid\":\"4650\"&#125;,&#123;\"itemid\":\"4654\"&#125;,&#123;\"itemid\":\"4658\"&#125;,&#123;\"itemid\":\"4864\"&#125;,&#123;\"itemid\":\"4868\"&#125;,&#123;\"itemid\":\"4872\"&#125;,&#123;\"itemid\":\"4291\"&#125;,&#123;\"itemid\":\"4295\"&#125;,&#123;\"itemid\":\"4299\"&#125;,&#123;\"itemid\":\"4482\"&#125;,&#123;\"itemid\":\"4486\"&#125;,&#123;\"itemid\":\"4490\"&#125;,&#123;\"itemid\":\"4434\"&#125;,&#123;\"itemid\":\"4438\"&#125;,&#123;\"itemid\":\"4442\"&#125;,&#123;\"itemid\":\"4934\"&#125;,&#123;\"itemid\":\"4938\"&#125;,&#123;\"itemid\":\"4942\"&#125;,&#123;\"itemid\":\"4764\"&#125;,&#123;\"itemid\":\"4768\"&#125;,&#123;\"itemid\":\"4772\"&#125;,&#123;\"itemid\":\"4153\"&#125;,&#123;\"itemid\":\"4158\"&#125;,&#123;\"itemid\":\"4163\"&#125;,&#123;\"itemid\":\"3569\"&#125;,&#123;\"itemid\":\"3583\"&#125;,&#123;\"itemid\":\"3597\"&#125;,&#123;\"itemid\":\"3570\"&#125;,&#123;\"itemid\":\"3584\"&#125;,&#123;\"itemid\":\"3598\"&#125;,&#123;\"itemid\":\"3561\"&#125;,&#123;\"itemid\":\"3575\"&#125;,&#123;\"itemid\":\"3589\"&#125;,&#123;\"itemid\":\"3559\"&#125;,&#123;\"itemid\":\"3573\"&#125;,&#123;\"itemid\":\"3587\"&#125;,&#123;\"itemid\":\"3564\"&#125;,&#123;\"itemid\":\"3578\"&#125;,&#123;\"itemid\":\"3592\"&#125;,&#123;\"itemid\":\"3560\"&#125;,&#123;\"itemid\":\"3574\"&#125;,&#123;\"itemid\":\"3588\"&#125;,&#123;\"itemid\":\"3558\"&#125;,&#123;\"itemid\":\"3572\"&#125;,&#123;\"itemid\":\"3586\"&#125;,&#123;\"itemid\":\"3566\"&#125;,&#123;\"itemid\":\"3580\"&#125;,&#123;\"itemid\":\"3594\"&#125;,&#123;\"itemid\":\"3557\"&#125;,&#123;\"itemid\":\"3571\"&#125;,&#123;\"itemid\":\"3585\"&#125;,&#123;\"itemid\":\"3562\"&#125;,&#123;\"itemid\":\"3576\"&#125;,&#123;\"itemid\":\"3590\"&#125;,&#123;\"itemid\":\"3565\"&#125;,&#123;\"itemid\":\"3579\"&#125;,&#123;\"itemid\":\"3593\"&#125;,&#123;\"itemid\":\"3567\"&#125;,&#123;\"itemid\":\"3581\"&#125;,&#123;\"itemid\":\"3595\"&#125;,&#123;\"itemid\":\"3563\"&#125;,&#123;\"itemid\":\"3577\"&#125;,&#123;\"itemid\":\"3591\"&#125;,&#123;\"itemid\":\"3568\"&#125;,&#123;\"itemid\":\"3582\"&#125;,&#123;\"itemid\":\"3596\"&#125;,&#123;\"itemid\":\"4994\"&#125;,&#123;\"itemid\":\"4998\"&#125;,&#123;\"itemid\":\"5002\"&#125;,&#123;\"itemid\":\"4169\"&#125;,&#123;\"itemid\":\"4174\"&#125;,&#123;\"itemid\":\"4179\"&#125;,&#123;\"itemid\":\"4753\"&#125;,&#123;\"itemid\":\"4757\"&#125;,&#123;\"itemid\":\"4761\"&#125;,&#123;\"itemid\":\"4900\"&#125;,&#123;\"itemid\":\"4904\"&#125;,&#123;\"itemid\":\"4908\"&#125;,&#123;\"itemid\":\"4661\"&#125;,&#123;\"itemid\":\"4665\"&#125;,&#123;\"itemid\":\"4669\"&#125;,&#123;\"itemid\":\"4875\"&#125;,&#123;\"itemid\":\"4879\"&#125;,&#123;\"itemid\":\"4883\"&#125;,&#123;\"itemid\":\"4302\"&#125;,&#123;\"itemid\":\"4306\"&#125;,&#123;\"itemid\":\"4310\"&#125;,&#123;\"itemid\":\"4493\"&#125;,&#123;\"itemid\":\"4497\"&#125;,&#123;\"itemid\":\"4501\"&#125;,&#123;\"itemid\":\"4445\"&#125;,&#123;\"itemid\":\"4449\"&#125;,&#123;\"itemid\":\"4453\"&#125;,&#123;\"itemid\":\"4945\"&#125;,&#123;\"itemid\":\"4949\"&#125;,&#123;\"itemid\":\"4953\"&#125;,&#123;\"itemid\":\"4775\"&#125;,&#123;\"itemid\":\"4779\"&#125;,&#123;\"itemid\":\"4783\"&#125;,&#123;\"itemid\":\"4993\"&#125;,&#123;\"itemid\":\"4997\"&#125;,&#123;\"itemid\":\"5001\"&#125;,&#123;\"itemid\":\"4167\"&#125;,&#123;\"itemid\":\"4172\"&#125;,&#123;\"itemid\":\"4177\"&#125;,&#123;\"itemid\":\"4752\"&#125;,&#123;\"itemid\":\"4756\"&#125;,&#123;\"itemid\":\"4760\"&#125;,&#123;\"itemid\":\"4898\"&#125;,&#123;\"itemid\":\"4902\"&#125;,&#123;\"itemid\":\"4906\"&#125;,&#123;\"itemid\":\"4663\"&#125;,&#123;\"itemid\":\"4667\"&#125;,&#123;\"itemid\":\"4671\"&#125;,&#123;\"itemid\":\"4873\"&#125;,&#123;\"itemid\":\"4877\"&#125;,&#123;\"itemid\":\"4881\"&#125;,&#123;\"itemid\":\"4304\"&#125;,&#123;\"itemid\":\"4308\"&#125;,&#123;\"itemid\":\"4312\"&#125;,&#123;\"itemid\":\"4496\"&#125;,&#123;\"itemid\":\"4500\"&#125;,&#123;\"itemid\":\"4504\"&#125;,&#123;\"itemid\":\"4448\"&#125;,&#123;\"itemid\":\"4452\"&#125;,&#123;\"itemid\":\"4456\"&#125;,&#123;\"itemid\":\"4948\"&#125;,&#123;\"itemid\":\"4952\"&#125;,&#123;\"itemid\":\"4956\"&#125;,&#123;\"itemid\":\"4778\"&#125;,&#123;\"itemid\":\"4782\"&#125;,&#123;\"itemid\":\"4786\"&#125;,&#123;\"itemid\":\"4996\"&#125;,&#123;\"itemid\":\"5000\"&#125;,&#123;\"itemid\":\"5004\"&#125;,&#123;\"itemid\":\"4166\"&#125;,&#123;\"itemid\":\"4171\"&#125;,&#123;\"itemid\":\"4176\"&#125;,&#123;\"itemid\":\"4751\"&#125;,&#123;\"itemid\":\"4755\"&#125;,&#123;\"itemid\":\"4759\"&#125;,&#123;\"itemid\":\"4899\"&#125;,&#123;\"itemid\":\"4903\"&#125;,&#123;\"itemid\":\"4907\"&#125;,&#123;\"itemid\":\"4664\"&#125;,&#123;\"itemid\":\"4668\"&#125;,&#123;\"itemid\":\"4672\"&#125;,&#123;\"itemid\":\"4874\"&#125;,&#123;\"itemid\":\"4878\"&#125;,&#123;\"itemid\":\"4882\"&#125;,&#123;\"itemid\":\"4301\"&#125;,&#123;\"itemid\":\"4305\"&#125;,&#123;\"itemid\":\"4309\"&#125;,&#123;\"itemid\":\"4495\"&#125;,&#123;\"itemid\":\"4499\"&#125;,&#123;\"itemid\":\"4503\"&#125;,&#123;\"itemid\":\"4447\"&#125;,&#123;\"itemid\":\"4451\"&#125;,&#123;\"itemid\":\"4455\"&#125;,&#123;\"itemid\":\"4947\"&#125;,&#123;\"itemid\":\"4951\"&#125;,&#123;\"itemid\":\"4955\"&#125;,&#123;\"itemid\":\"4777\"&#125;,&#123;\"itemid\":\"4781\"&#125;,&#123;\"itemid\":\"4785\"&#125;,&#123;\"itemid\":\"4995\"&#125;,&#123;\"itemid\":\"4999\"&#125;,&#123;\"itemid\":\"5003\"&#125;,&#123;\"itemid\":\"4170\"&#125;,&#123;\"itemid\":\"4175\"&#125;,&#123;\"itemid\":\"4180\"&#125;,&#123;\"itemid\":\"4754\"&#125;,&#123;\"itemid\":\"4758\"&#125;,&#123;\"itemid\":\"4762\"&#125;,&#123;\"itemid\":\"4897\"&#125;,&#123;\"itemid\":\"4901\"&#125;,&#123;\"itemid\":\"4905\"&#125;,&#123;\"itemid\":\"4662\"&#125;,&#123;\"itemid\":\"4666\"&#125;,&#123;\"itemid\":\"4670\"&#125;,&#123;\"itemid\":\"4876\"&#125;,&#123;\"itemid\":\"4880\"&#125;,&#123;\"itemid\":\"4884\"&#125;,&#123;\"itemid\":\"4303\"&#125;,&#123;\"itemid\":\"4307\"&#125;,&#123;\"itemid\":\"4311\"&#125;,&#123;\"itemid\":\"4494\"&#125;,&#123;\"itemid\":\"4498\"&#125;,&#123;\"itemid\":\"4502\"&#125;,&#123;\"itemid\":\"4446\"&#125;,&#123;\"itemid\":\"4450\"&#125;,&#123;\"itemid\":\"4454\"&#125;,&#123;\"itemid\":\"4946\"&#125;,&#123;\"itemid\":\"4950\"&#125;,&#123;\"itemid\":\"4954\"&#125;,&#123;\"itemid\":\"4776\"&#125;,&#123;\"itemid\":\"4780\"&#125;,&#123;\"itemid\":\"4784\"&#125;,&#123;\"itemid\":\"4168\"&#125;,&#123;\"itemid\":\"4173\"&#125;,&#123;\"itemid\":\"4178\"&#125;,&#123;\"itemid\":\"461\"&#125;,&#123;\"itemid\":\"357\"&#125;,&#123;\"itemid\":\"509\"&#125;,&#123;\"itemid\":\"462\"&#125;,&#123;\"itemid\":\"565\"&#125;,&#123;\"itemid\":\"567\"&#125;,&#123;\"itemid\":\"356\"&#125;,&#123;\"itemid\":\"412\"&#125;,&#123;\"itemid\":\"498\"&#125;,&#123;\"itemid\":\"499\"&#125;,&#123;\"itemid\":\"429\"&#125;,&#123;\"itemid\":\"463\"&#125;,&#123;\"itemid\":\"464\"&#125;,&#123;\"itemid\":\"465\"&#125;,&#123;\"itemid\":\"466\"&#125;,&#123;\"itemid\":\"467\"&#125;,&#123;\"itemid\":\"468\"&#125;,&#123;\"itemid\":\"469\"&#125;,&#123;\"itemid\":\"470\"&#125;,&#123;\"itemid\":\"471\"&#125;,&#123;\"itemid\":\"472\"&#125;,&#123;\"itemid\":\"473\"&#125;,&#123;\"itemid\":\"474\"&#125;,&#123;\"itemid\":\"475\"&#125;,&#123;\"itemid\":\"476\"&#125;,&#123;\"itemid\":\"477\"&#125;,&#123;\"itemid\":\"478\"&#125;,&#123;\"itemid\":\"479\"&#125;,&#123;\"itemid\":\"480\"&#125;,&#123;\"itemid\":\"481\"&#125;,&#123;\"itemid\":\"482\"&#125;,&#123;\"itemid\":\"483\"&#125;,&#123;\"itemid\":\"484\"&#125;,&#123;\"itemid\":\"485\"&#125;,&#123;\"itemid\":\"486\"&#125;,&#123;\"itemid\":\"487\"&#125;,&#123;\"itemid\":\"488\"&#125;,&#123;\"itemid\":\"489\"&#125;,&#123;\"itemid\":\"3599\"&#125;,&#123;\"itemid\":\"3600\"&#125;,&#123;\"itemid\":\"3601\"&#125;,&#123;\"itemid\":\"3602\"&#125;,&#123;\"itemid\":\"490\"&#125;,&#123;\"itemid\":\"3603\"&#125;,&#123;\"itemid\":\"3604\"&#125;,&#123;\"itemid\":\"3606\"&#125;,&#123;\"itemid\":\"3605\"&#125;,&#123;\"itemid\":\"3607\"&#125;,&#123;\"itemid\":\"3611\"&#125;,&#123;\"itemid\":\"3615\"&#125;,&#123;\"itemid\":\"3619\"&#125;,&#123;\"itemid\":\"3608\"&#125;,&#123;\"itemid\":\"3612\"&#125;,&#123;\"itemid\":\"3616\"&#125;,&#123;\"itemid\":\"3620\"&#125;,&#123;\"itemid\":\"3610\"&#125;,&#123;\"itemid\":\"3614\"&#125;,&#123;\"itemid\":\"3618\"&#125;,&#123;\"itemid\":\"3622\"&#125;,&#123;\"itemid\":\"3609\"&#125;,&#123;\"itemid\":\"3613\"&#125;,&#123;\"itemid\":\"3617\"&#125;,&#123;\"itemid\":\"3621\"&#125;,&#123;\"itemid\":\"491\"&#125;,&#123;\"itemid\":\"5038\"&#125;,&#123;\"itemid\":\"492\"&#125;,&#123;\"itemid\":\"5031\"&#125;],\"id\":0&#125; or python 腳本：123456789101112131415161718192021222324252627282930313233343536373839404142$ vim get_items.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = &#123;\"Content-Type\":\"application/json\"&#125;# request jsondata = json.dumps(&#123; \"jsonrpc\":\"2.0\", \"method\":\"item.get\", \"params\":&#123; \"output\":[\"itemids\",\"key_\"], \"hostids\":\"55\", &#125;, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,&#125;)# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print host #print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本運行結果：12345678910111213141516171819202122$ python get_items.py Number Of Hosts: 435… …&#123;u'itemid': u'469', u'key_': u'system.cpu.intr'&#125;&#123;u'itemid': u'470', u'key_': u'system.cpu.load[percpu,avg15]'&#125;&#123;u'itemid': u'471', u'key_': u'system.cpu.load[percpu,avg1]'&#125;&#123;u'itemid': u'472', u'key_': u'system.cpu.load[percpu,avg5]'&#125;&#123;u'itemid': u'473', u'key_': u'system.cpu.switches'&#125;&#123;u'itemid': u'474', u'key_': u'system.cpu.util[,idle]'&#125;&#123;u'itemid': u'475', u'key_': u'system.cpu.util[,interrupt]'&#125;&#123;u'itemid': u'476', u'key_': u'system.cpu.util[,iowait]'&#125;&#123;u'itemid': u'477', u'key_': u'system.cpu.util[,nice]'&#125;&#123;u'itemid': u'478', u'key_': u'system.cpu.util[,softirq]'&#125;&#123;u'itemid': u'479', u'key_': u'system.cpu.util[,steal]'&#125;&#123;u'itemid': u'480', u'key_': u'system.cpu.util[,system]'&#125;&#123;u'itemid': u'481', u'key_': u'system.cpu.util[,user]'&#125;… …&#123;u'itemid': u'491', u'key_': u'vm.memory.size[available]'&#125;&#123;u'itemid': u'5038', u'key_': u'vm.memory.size[cached]'&#125;&#123;u'itemid': u'492', u'key_': u'vm.memory.size[total]'&#125;&#123;u'itemid': u'5031', u'key_': u'vm.memory.size[used]'&#125;… … 5. history.get方法獲取單個監控項的歷史數據curl 命令：1$ curl -i -X POST -H 'Content-Type:application/json' -d '&#123;\"jsonrpc\":\"2.0\",\"method\":\"history.get\",\"params\":&#123;\"history\":3,\"itemids\":\"480\",\"output\":\"extend\",\"limit\":10&#125;,\"auth\":\"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php 有些值結果會顯示為0, 暫時還要查看history.get獲得歷史數據的參數設定 curl 命令運行結果：1&#123;\"jsonrpc\":\"2.0\",\"result\":[&#123;\"itemid\":\"25154\",\"clock\":\"1410744134\",\"value\":\"4840\",\"ns\":\"375754276\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744314\",\"value\":\"5408\",\"ns\":\"839852515\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744374\",\"value\":\"7040\",\"ns\":\"964558609\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744554\",\"value\":\"4072\",\"ns\":\"943177771\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744614\",\"value\":\"8696\",\"ns\":\"995289716\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744674\",\"value\":\"6144\",\"ns\":\"992462863\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744734\",\"value\":\"6472\",\"ns\":\"152634327\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744794\",\"value\":\"4312\",\"ns\":\"479599424\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744854\",\"value\":\"4456\",\"ns\":\"263314898\"&#125;,&#123;\"itemid\":\"25154\",\"clock\":\"1410744914\",\"value\":\"8656\",\"ns\":\"840460009\"&#125;],\"id\":0&#125; or python 腳本：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849$ get_items_history.py#!/usr/bin/env python2.7#coding=utf-8import jsonimport urllib2# based url and required headerurl = \"http://10.111.200.8/zabbix/api_jsonrpc.php\"header = &#123;\"Content-Type\":\"application/json\"&#125;# request jsonitemids_id = input('Pluse input itemids number: ')print('id ', itemids_id)data = json.dumps(&#123; \"jsonrpc\":\"2.0\", \"method\":\"history.get\", \"params\":&#123; \"output\":\"extend\", \"history\":0, \"itemids\": itemids_id, \"sortfield\" : \"clock\", \"limit\": 10 &#125;, \"auth\":\"df432b915671b77035d5e26f85c8bf5c\", # theauth id is what auth script returns, remeber it is string \"id\":1,&#125;)# create request objectrequest = urllib2.Request(url,data)for key in header: request.add_header(key,header[key])# get host listtry: result = urllib2.urlopen(request)except URLError as e: if hasattr(e, 'reason'): print 'We failed to reach a server.' print 'Reason: ', e.reason elif hasattr(e, 'code'): print 'The server could not fulfill the request.' print 'Error code: ', e.codeelse: response = json.loads(result.read()) result.close() print \"Number Of Hosts: \", len(response['result']) for host in response['result']: print host #print \"Host ID:\",host['hostid'],\"HostName:\",host['name'] python 腳本執行結果：這邊觀察{u&#39;itemid&#39;: u&#39;480&#39;, u&#39;key_&#39;: u&#39;system.cpu.util[,system]&#39;}狀態 1234567891011121314$ python get_items_history.pyPluse input itemids number: 480('id ', 480)Number Of Hosts: 10&#123;u'itemid': u'480', u'ns': u'366027868', u'value': u'2.3541', u'clock': u'1542254040'&#125;&#123;u'itemid': u'480', u'ns': u'409639017', u'value': u'2.3193', u'clock': u'1542254100'&#125;&#123;u'itemid': u'480', u'ns': u'476390125', u'value': u'2.2133', u'clock': u'1542254160'&#125;&#123;u'itemid': u'480', u'ns': u'609209011', u'value': u'2.1803', u'clock': u'1542254220'&#125;&#123;u'itemid': u'480', u'ns': u'8006955', u'value': u'3.6697', u'clock': u'1542254281'&#125;&#123;u'itemid': u'480', u'ns': u'676841696', u'value': u'5.0511', u'clock': u'1542254340'&#125;&#123;u'itemid': u'480', u'ns': u'652330397', u'value': u'3.6032', u'clock': u'1542254400'&#125;&#123;u'itemid': u'480', u'ns': u'567671572', u'value': u'3.1112', u'clock': u'1542254460'&#125;&#123;u'itemid': u'480', u'ns': u'553946907', u'value': u'2.5180', u'clock': u'1542254520'&#125;&#123;u'itemid': u'480', u'ns': u'496168653', u'value': u'2.3969', u'clock': u'1542254580'&#125; 獲取下2018-11-21到2018-11-21期間的數據12$ curl -i -X POST -H 'Content-Type: application/json' -d '&#123;\"jsonrpc\":\"2.0\",\"method\":\"history.get\",\"params\":&#123;\"history\":0,\"itemids\":[\"480\"],\"time_from\":\"1542844800.0\",\"time_till\":\"1542758400.0\" ,\"output\":\"extend\"&#125;,\"auth\": \"df432b915671b77035d5e26f85c8bf5c\",\"id\": 0&#125;' http://10.111.200.8/zabbix/api_jsonrpc.php #備註參考1:獲取對應監控項一段時間內的歷史數據並格式化輸出 https://www.yangcs.net/posts/zabbix-api-introduce-and-use/ python 腳本:123456789101112131415161718192021222324252627282930313233$ vim history_data.py#!/usr/bin/env python# encoding: utf-8\"\"\"Retrieves history data for a given numeric (either int or float) item_id\"\"\"from zabbix_api import ZabbixAPIimport pprintfrom datetime import datetimeimport timeserver = \"http://172.16.241.130/zabbix\"username = \"Admin\"password = \"zabbix\"zapi = ZabbixAPI(server=server)zapi.login(username, password)item_id = \"23296\"# Create a time rangetime_till = time.mktime(datetime.now().timetuple())time_from = time_till - 60 * 60 * 24 * 10 # &lt;span id=\"inline-toc\"&gt;1.&lt;/span&gt; days# Query item's history (integer) datahistory = zapi.history.get(&#123;\"itemids\": item_id, \"time_from\": time_from, \"time_till\": time_till, \"output\": \"extend\", \"limit\": \"10\"&#125;)# If nothing was found, try getting it from history (float) dataif not len(history): history = zapi.history.get(&#123;\"itemids\": item_id, \"time_from\": time_from, \"time_till\": time_till, \"output\": \"extend\", \"limit\": \"10\", \"history\": 0&#125;)for point in history: print(\"&#123;0&#125;: &#123;1&#125;\".format(datetime.fromtimestamp(int(point['clock'])) .strftime(\"%x %X\"), point['value'])) python 運行結果:1234567891011$ python history_data.py12/29/16 09:40:16: 0.490012/29/16 09:41:16: 0.180012/29/16 09:42:16: 0.160012/29/16 09:43:16: 0.200012/29/16 09:44:16: 0.070012/29/16 09:45:16: 0.100012/29/16 09:46:16: 0.210012/29/16 09:47:16: 0.070012/29/16 09:48:16: 0.510012/29/16 09:49:16: 0.2200 #備註參考2:時間格式化輸出1234567891011121314151617181920212223242526#!/usr/bin/python# coding=utf-8import timeimport syswhile True: choose = input(\"請選擇轉換時間格式, 輸入1 : 時間轉秒, 輸入2: 秒轉時間: \") print ('Your input: ',choose ) if choose==1 : turn_sec = raw_input ('請輸入轉換時間: 2017-04-13 00:00:00: ') print ('input: ',turn_sec) a = turn_sec #輸出: 1492012800.0 print time.mktime(time.strptime(a,'%Y-%m-%d %H:%M:%S')) elif choose == 2: turn_date = float(raw_input('請輸入轉換時間: 1492012800.0: ')) #输出： 2017-04-13 00:00:00 print ('input: ',turn_date) x = time.localtime(turn_date) print time.strftime('%Y-%m-%d %H:%M:%S',x) else : print (\"Inpute Error\") 引用參考資料 [Zabbix API 使用] [python調用zabbix api接口實時展示數據] [zabbix API 獲取CPU 信息] [Zabbix Api 简介和使用]","categories":[],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"http://yylin1.github.io/tags/zabbix/"},{"name":"API","slug":"API","permalink":"http://yylin1.github.io/tags/API/"}]},{"title":"如何自行編譯 HPL-GPU 來測試 Benchmarks","slug":"build-hpl-gpu","date":"2018-10-23T13:11:49.000Z","updated":"2018-11-29T16:30:31.590Z","comments":true,"path":"2018/10/23/build-hpl-gpu/","link":"","permalink":"http://yylin1.github.io/2018/10/23/build-hpl-gpu/","excerpt":"構建 NVIDIA CUDA Linpack 環境執行環境非常困難，網路上訊息非常少，然後linkpack測試更新版本已經有一段時間，記錄實作主要參考「Hybrid HPL(GPU版HPL)安装教程」與「AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる」兩個文章教學，並嘗試運行現在環境支援的版本，部署過程記錄。","text":"構建 NVIDIA CUDA Linpack 環境執行環境非常困難，網路上訊息非常少，然後linkpack測試更新版本已經有一段時間，記錄實作主要參考「Hybrid HPL(GPU版HPL)安装教程」與「AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる」兩個文章教學，並嘗試運行現在環境支援的版本，部署過程記錄。 環境部署資訊Linpack 部署的版本資訊： Mpich: v3.2.1 Openmpi: v1.10.3 Intel MKL: l_mkl_2019.0.117 Linpack: hpl-2.0_FERMI_v15 實驗環境安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體機器： Role vCPU RAM Extra Device ubuntu 8 16G GTX 1060 6G 事前準備:::danger測試 Linkpack 之前，需要確保以下條件達成：確認環境是否安裝以下NVIDIA driver、CUDA、Intel MKL`Openmpi、mpich2`，並設定好環境變數。::: 安裝 NVIDIA驅動和CUDA tookit由於CUDA toolkit中，安裝時已包含了NVIDIA Driver，可一併安裝 1234$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：123456789101112131415161718192021222324252627282930$ lsmod | grep nvidianvidia_uvm 790528 0nvidia_drm 40960 2nvidia_modeset 1089536 3 nvidia_drmdrm_kms_helper 167936 1 nvidia_drmdrm 360448 5 nvidia_drm,drm_kms_helpernvidia 14032896 96 nvidia_modeset,nvidia_uvmipmi_msghandler 45056 2 nvidia,ipmi_devintf$ cat /usr/local/cuda/version.txtCUDA Version 9.2.148$ nvidia-smiTue Oct 2 18:15:47 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 396.44 Driver Version: 396.44 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 On | N/A || 39% 31C P8 7W / 120W | 52MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1603 G /usr/lib/xorg/Xorg 49MiB |+-----------------------------------------------------------------------------+ 準備LinpackLink : https://developer.nvidia.com/rdp/assets/cuda-accelerated-linpack-linux64從上面的連結，登入CUDA註冊開發者會員，下載linpack for Linux64版本，這裡下載到的版本為hpl-2.0_FERMI_v15.tgz。 參考連結Hybrid HPL(GPU版HPL)安装教程AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる 安裝INTEL MKL透過連結註冊帳號https://software.intel.com/en-us/qualify-for-free-software 註冊後，它會向您發送序列號於信箱，以便進行安裝準備。 這邊是下載最新l_mkl_2019.0.117.tgz版本 下載取得l_mkl_2019.0.117.tgz後，即可透過install.sh運行安裝。 12$ tar zxvf l_mkl_2019.0.117.tgz$ cd l_mkl_2019.0.117 Intel mkl的安裝很簡單的，每一步也都有說明，按Enter繼續下一步預設設定安裝即可，安裝到某一步會要求輸入序列號，申請30天試用版所給的那個序列號。123456789101112131415161718192021222324$ sh ./install.sh--------------------------------------------------------------------------------Initializing, please wait...--------------------------------------------------------------------------------Welcome--------------------------------------------------------------------------------Welcome to the Intel(R) Math Kernel Library 2019 for Linux*--------------------------------------------------------------------------------You will complete the following steps: 1. Welcome 2. License Agreement 3. Options 4. Installation 5. Complete----------------------------------------------------------------------------------------------------------------------------------------------------------------Press \"Enter\" key to continue or \"q\" to quit:License Agreement-------------------------------------------------------------------------------- 確認後會安裝一些套件，這裡就可以看到MKL預設情況下，會安裝在/opt/intel下面。123456789101112131415161718------------------------Options &gt; Pre-install Summary--------------------------------------------------------------------------------Install location: /opt/intelComponent(s) selected: Intel Math Kernel Library 2019 for C/C++ 2.6GB Intel MKL core libraries for C/C++ Intel TBB threading support GNU* C/C++ compiler support Intel Math Kernel Library 2019 for Fortran 2.6GB Intel MKL core libraries for Fortran GNU* Fortran compiler support Fortran 95 interfaces for BLAS and LAPACK Install space required: 2.8GB 編譯完成後，即會顯示安裝資訊。 123456789101112------------------------Complete--------------------------------------------------------------------------------Thank you for installing Intel(R) Math Kernel Library 2019 for Linux*.If you have not done so already, please register your product with IntelRegistration Center to create your support account and take full advantage ofyour product purchase.Your support account gives you access to free product updates and upgradesas well as Priority Customer support at the Online Service Centerhttps://supporttickets.intel.com. 完整安裝過程於Gist。 安装mpich2123456$ wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gztar zxvf mpich-3.2.1.tar.gz$ cd mpich-3.2.1./configure -prefix=/home/username/mpich$ make$ make install 配置環境打開/etc/environment1$ vim /etc/environment 將自己的路徑添加到PATH最後，注意別忘了冒號“：”，添加後的PATH如下1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存退出，在終端輸入source /etc/environment再輸入echo $PATH測試發現已經更新，環境變量配置成功。 本文来自 ForTheDreamSMS 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/baidu_34045013/article/details/78237842?utm_source=copy 安裝參考:ubuntu16.04安裝配置mpich2 安裝openmpi1234567$ wget -c https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.3.tar.gz$ tar zxvf openmpi-1.10.3.tar.gz$ cd openmpi-1.10.3$ ./configure --prefix=/opt/openmpi$ make$ sudo make install 安裝make和make instal需要一段時間，等待完成即可，openmpi環境配置會在後面統一設定。 參考 : OpenMPI設定叢集環境 配置環境變量首先更改環境變量PATH： 1sudo vim /etc/environment 在PATH變量加上/usr/local/cuda-9.2/bin,前面要有分號，後面沒有，修改後例如下面這樣：1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存文件，然後再執行：source /etc/environment完成後，可以執行echo $PATH,查看是否修改成功 接著還需更改ldconfig12cd /etc/ld.so.conf.d/sudo vim hpl.conf 輸入如下內容12345/usr/local/cuda-9.2/lib64/lib/opt/intel/mkl/lib/intel64/opt/intel/lib/intel64/home/ubuntu/hpl/src/cuda 最後一行/home/使用者/hpl/src/cuda是編譯HPL時才需要改的，在這裡一併修改。這個目錄就是編譯hpl時，hpl的路徑。 添加上述內容並保存後，執行1sudu ldconfig 可以輸入下面命令進行檢驗，有輸出內容就對了1sudo ldconfig -v | grep cuda 接著還要執行Intel MKL的環境變量設置腳本1234export LD_LIBRARY_PATH=/opt/intel/mkl/lib/intel64:/opt/intel/compilers_and_libraries/linux/lib/intel64:/home/ubuntu/hpl/src/cuda:/opt/openmpi/libexport PATH=/opt/openmpi/bin:$PATHsource /opt/intel/compilers_and_libraries_2019.0.117/linux/mkl/bin/mklvars.sh intel64 請確認以上路徑與當前環境上所有套件的路徑是否對應存在，再執行1source ~/.bashrc 這樣，環境變量就設置好了。最好echo $PATH查看下是否多了一行intel的信息，如果沒有配置成功的話，在編譯HPL時會提示/usr/bin/ld: cannot find -liomp5的錯誤。 開始編譯Linpack benchmark for CUDA這邊將hpl-2.0_FERMI_v15.tgz解壓縮放置主目錄下hpl文件夾，可以依照自己設定的路徑對應編譯。 12345$ tar -xvf hpl-2.0_FERMI_v15.tgz –C ~/hpl$ cd ~/hpl$ lsbin BUGS COPYRIGHT CUDA_LINPACK_README.txt HISTORY include INSTALL lib Make.CUDA Makefile makes Make.top man README setup src testing TODO TUNING www 編譯Make.CUDA編輯配置這時還需要編輯Make.CUDA測試環境參考連結，需更改Make.CUDA中的TOPdir為hpl的目錄。 123456103 TOPdir = /home/ubuntu/hpl132 LAdir = /opt/intel/mkl/lib/intel64133 LAMP5dir = /opt/intel/compilers_and_libraries/linux/lib/intel64134 LAinc = -I/opt/intel/mkl/include 接著可以開始編譯了12cd ~/hplmake arch=CUDA 如果沒有提示錯誤，就是編譯成功了。 編譯完成後，還需要修改~/hpl/bin/CUDA/run_linpack中的HPL_DIR為你hpl的路徑 1HPL_DIR=/home/ubuntu/hpl 修改完成後就可以開始測試了。 測試之前建議把HPL.dat的參數改小一點，N改成8000，這樣所需的測試時間少。也先把P，Q，PxQ都改成1，保證可以執行測試: 1$ mpirun -n 1 ./run_linpack 輸出結果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ mpirun -n 1 ./run_linpack================================================================================HPLinpack 2.0 -- High-Performance Linpack benchmark -- September 10, 2008Written by A. Petitet and R. Clint Whaley, Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V : Wall time / encoded variant.N : The order of the coefficient matrix A.NB : The partitioning blocking factor.P : The number of process rows.Q : The number of process columns.Time : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N : 25000 30000NB : 768 1024 1280 1536PMAP : Row-major process mappingP : 1Q : 1PFACT : LeftNBMIN : 2NDIV : 2RFACT : LeftBCAST : 1ringDEPTH : 1SWAP : Spread-roll (long)L1 : no-transposed formU : no-transposed formEQUIL : yesALIGN : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed: ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be 1.110223e-16- Computational tests pass if scaled residuals are less than 16.0================================================================================T/V N NB P Q Time Gflops--------------------------------------------------------------------------------WR10L2L2 25000 768 1 1 43.07 2.419e+02--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)= 0.0040802 ...... PASSED================================================================================ 完整環境配置與設置有放到Gits，可以提供參考。","categories":[],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://yylin1.github.io/tags/GPU/"},{"name":"linpack","slug":"linpack","permalink":"http://yylin1.github.io/tags/linpack/"}]},{"title":"快速部署 Kubeflow v0.3 容器機器學習平台","slug":"kubeflow-deploy-v0.3.0","date":"2018-10-23T12:53:45.000Z","updated":"2018-10-30T15:42:56.172Z","comments":true,"path":"2018/10/23/kubeflow-deploy-v0.3.0/","link":"","permalink":"http://yylin1.github.io/2018/10/23/kubeflow-deploy-v0.3.0/","excerpt":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。","text":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。 Kubeflow目標不是在於重建其他服務，而是提供一個最佳開發系統，來部署到任何集群中，有效確保ML在集群之間移動性，並輕鬆將任務擴展任何集群。 由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠運行部署 Kubeflow。 在不同基礎設施上協助實現Machine learning 工作流程「更加簡單」與「便於攜帶」、「可擴展性」的部署，在不同基礎設施上(Local Laptop、GPU Server cluster、Cloud Production Cluster) 簡述Kubelfow工作流程： 下載Kubeflow shell script 和 配置文件 自定義 ksonnet部署所需的JSON組件 執行shell script將容器部署到所選擇的Kubernetes Enveroment 開始執行機器學習訓練 我們透過Kubeflow工具管理，可以解決ML環境配置困難的問題，其中在Kubeflow提供Operator，對不同的機器學習框架，擴展的 Kubernetes API 進行自動建立、管理與配置應用程式容器實例，並提供訓練在不同環境和服務，包含數據準備、模型培訓、預測服務和服務管理。 Kubernetes 有提供不同集群部署支援，包括單節點Minikube部署、 透過Hypervisor部署的Multipass &amp; Microk8s、Cloud。無論你在哪裡運行Kubernetes，你都能夠運行Kubeflow。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Destop，測試環境為實體機器部署HA Cluster： IP Address Role vCPU RAM Extra Device 172.22.132.98 VIP 172.22.132.81 k8s-m1 8 16G GTX 1060 6G 172.22.132.82 k8s-m2 8 16G GTX 1060 6G 172.22.132.83 k8s-m3 8 16G GTX 1060 6G 172.22.132.84 k8s-g1 8 16G GTX 1060 6G 172.22.132.97 k8s-g2 8 16G GTX 1060 6G 172.22.132.86 k8s-g3 8 16G GTX 1060 6G 172.22.132.94 k8s-g11 8 16G GTX 1060 6G kubernetes部署可以參考 開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集教學 本次部署使用版本 kubernetes v1.11.2 Master x 3, Worker x 4 ksonnet 0.13.0 kubeflow v0.3.0 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點確認已經安裝 NVIDIA driver、CUDA、Docker、NVIDIA Docker，以下為實驗環境版本資訊： CUDA Version: 10.0.130 Driver Version: 410.48 Docker Version: 18.03.0-ce NVIDIA Docker: 2.0.3 安裝 NFS設置Kubeflow需要Volumes來存儲數據，建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： 1234567891011# 在 master 執行，這邊nfs-server建立在k8s-m1節點$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data# 可以在 `nfs-data`資料夾建立多個pv資料夾，提供kubeflow儲存使用$ cd /nfs-data$ mkdir user1 user2 ... $ echo \"/nfs-data *(rw,sync,no_root_squash,no_subtree_check)\" | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 這邊也可以使用其他存儲數據方式: Kubernetes: Local Volume 安裝 KsonnetKsonnet 是一個命令工具，可以更輕鬆地管理由多個組件組成的複雜部署，簡化編寫和部署 Kubernetes 配置。 123456789$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.0/ks_0.13.0_linux_amd64.tar.gz$ tar xvf ks_0.13.0_linux_amd64.tar.gz$ sudo cp ks_0.13.0_linux_amd64 /usr/local/bin/$ chmod +x ks$ ks versionksonnet version: 0.13.0jsonnet version: v0.11.2client-go version: kubernetes-1.10.4 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。部署版本選擇為kubeflow v0.3.0。 下載 Kubeflow 腳本下載GitHub中Kubeflow設置所需配置： 12345678910# 建立配置指定資料夾路徑，可自行更換名稱$ KUBEFLOW_SRC=mykfsrc $ mkdir $&#123;KUBEFLOW_SRC&#125;$ cd $&#123;KUBEFLOW_SRC&#125;# 部署kubeflow版本標籤，可自行選擇其他分支$ export KUBEFLOW_TAG=v0.3.0 # 執行shell script$ curl https://raw.githubusercontent.com/kubeflow/kubeflow/$&#123;KUBEFLOW_TAG&#125;/scripts/download.sh | bash 下載後目錄下會多kubeflow、scripts資料夾: 123$ tree -L 1 ├── kubeflow└── scripts 執行腳本配置和部署Kubeflow123456789101112131415161718# 退回上一層建立對應路徑$ cd ../$ KUBEFLOW_REPO=$(pwd)/mykfsrc# $&#123;KFAPP&#125; 為配置 kubeflow 儲存目錄的名稱，執行init時將創建此目錄# ksonnet應用程式將在 $&#123;KFAPP&#125;/ks_app目錄中創建$ KFAPP=mykfapp$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh init $&#123;KFAPP&#125; --platform none# 安裝 Kubeflow 套件至應用程式目錄$ cd $&#123;KFAPP&#125;$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh generate k8s# 定義 Namespace 為kubeflow來管理相關資源$ kubectl create ns kubeflow# 部署 Kubeflow$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh apply k8s ！部署預設為default，這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 完成後檢查 Kubeflow 元件部署結果：12345678910111213141516171819$ kubectl -n kubeflow get po -o wide ambassador-868d5dbdc4-5fgdw 3/3 Running 0 7m 10.244.2.90 k8s-g3 &lt;none&gt;ambassador-868d5dbdc4-qcp5z 3/3 Running 1 7m 10.244.5.78 k8s-g11 &lt;none&gt;ambassador-868d5dbdc4-x9bfc 3/3 Running 0 7m 10.244.3.96 k8s-g2 &lt;none&gt;argo-ui-84464bd59c-hmhmd 1/1 Running 0 7m 10.244.4.89 k8s-g1 &lt;none&gt;centraldashboard-c76877875-nqbhg 1/1 Running 0 7m 10.244.3.97 k8s-g2 &lt;none&gt;modeldb-backend-58969447f6-z2xmf 1/1 Running 1 11s 10.244.3.102 k8s-g2 &lt;none&gt;modeldb-db-57b855f5b7-dw5s6 1/1 Running 0 7m 10.244.5.79 k8s-g11 &lt;none&gt;modeldb-frontend-769d5bdd66-njkqs 1/1 Running 0 7m 10.244.3.99 k8s-g2 &lt;none&gt;spartakus-volunteer-5bfd5876fb-b86h5 1/1 Running 0 7m 10.244.2.91 k8s-g3 &lt;none&gt;studyjob-controller-56588dc6f9-shg72 1/1 Running 0 7m 10.244.3.101 k8s-g2 &lt;none&gt;tf-hub-0 1/1 Running 0 7m 10.244.4.86 k8s-g1 &lt;none&gt;tf-job-dashboard-7777b6bf-qltfc 1/1 Running 0 7m 10.244.3.98 k8s-g2 &lt;none&gt;tf-job-operator-v1alpha2-5c5b4dcfdf-x5rkv 1/1 Running 0 7m 10.244.4.87 k8s-g1 &lt;none&gt;vizier-core-5584ccbd8-l2bxr 0/1 CrashLoopBackOff 4 7m 10.244.5.80 k8s-g11 &lt;none&gt;vizier-db-547967c899-mf988 0/1 Pending 0 7m &lt;none&gt; &lt;none&gt; &lt;none&gt;vizier-suggestion-grid-8547dbb55b-k8tw6 1/1 Running 0 7m 10.244.4.90 k8s-g1 &lt;none&gt;vizier-suggestion-random-d7c5cd68b-7p9nz 1/1 Running 0 7m 10.244.3.100 k8s-g2 &lt;none&gt;workflow-controller-5c95f95f58-d9plp 1/1 Running 0 7m 10.244.4.88 k8s-g1 &lt;none&gt; 可以看到vizier狀態為Pending，失敗的原因是PersistentVolume未分配給vizer-db（實際是MySQL）。所以vizier-db正在等待並且未能啟動vizier-core。 這邊透過使用kubectl describe命令檢查原因: 123456789$ kubectl -n kubeflow describe pod vizier-db-547967c899-mf988Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m (x417 over 12m) default-scheduler pod has unbound PersistentVolumeClaims (repeated 4 times)$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Pending 15m 這邊我們建立一個 NFS PV 來提供給 vizier-db 使用：1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: vizier-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv1EOF 請更換自己的 nfs-server IP 再次查看 PV &amp; PVC 狀態，就可以看到vizier-db獲得 20Gi 的 ersistentVolume 空間 1234567$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEvizier-pv 20Gi RWO Retain Bound kubeflow/vizier-db 16s$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Bound vizier-pv 20Gi RWO 20m 其他儲存方式參考：Kubernetes: Local Volumeの検証 暫時問題：modeldb-backend 會不定時 Crash Kubeflow UI查看Service中，可以看到kubeflow v0.3.0版本中，附帶了許多Web UI: Argo UI Central UI for navigation JupyterHub Katib TFJobs Dashboard 這邊我們透過Service查看一下目前pod狀態： 123456789101112131415161718$ kubectl -n kubeflow get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador ClusterIP 10.98.216.151 &lt;none&gt; 80/TCP 7m service=ambassadorambassador-admin ClusterIP 10.102.20.172 &lt;none&gt; 8877/TCP 6m service=ambassadorargo-ui NodePort 10.102.36.121 &lt;none&gt; 80:32530/TCP 6m app=argo-uicentraldashboard ClusterIP 10.99.255.3 &lt;none&gt; 80/TCP 6m app=centraldashboardk8s-dashboard ClusterIP 10.110.209.78 &lt;none&gt; 443/TCP 6m k8s-app=kubernetes-dashboardmodeldb-backend ClusterIP 10.102.179.187 &lt;none&gt; 6543/TCP 6m app=modeldb,component=backendmodeldb-db ClusterIP 10.102.143.161 &lt;none&gt; 27017/TCP 6m app=modeldb,component=dbmodeldb-frontend ClusterIP 10.100.76.92 &lt;none&gt; 3000/TCP 6m app=modeldb,component=frontendstatsd-sink ClusterIP 10.98.167.5 &lt;none&gt; 9102/TCP 7m service=ambassadortf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 6m app=tf-hubtf-hub-lb ClusterIP 10.106.99.192 &lt;none&gt; 80/TCP 6m app=tf-hubtf-job-dashboard ClusterIP 10.100.66.145 &lt;none&gt; 80/TCP 6m name=tf-job-dashboardvizier-core NodePort 10.105.154.126 &lt;none&gt; 6789:30678/TCP 6m app=vizier,component=corevizier-db ClusterIP 10.100.32.162 &lt;none&gt; 3306/TCP 6m app=vizier,component=dbvizier-suggestion-grid ClusterIP 10.105.68.247 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-gridvizier-suggestion-random ClusterIP 10.110.164.76 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-random 這時候就可以透過kubeflow UI，能引導到 登入JupyterHub，但這邊需要修改 Kubernetes Service，透過以下指令進行： 123456789101112# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc ambassador...sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125;...#修改後就可以透過NodePort接上Port進入顯示Jupyter NotebookNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador NodePort 10.98.216.151 &lt;none&gt; 80:32752/TCP 15m service=ambassador 完成後連接 http://Master_IP:Port即可看到kubeflow UI。 可以透過上方連結查看Jupyter Hub、TFjob Dashboard、Kubernetes Dashboard 等。 測試 Jupyter Notebook這邊需要再次先建立一個 NFS PV 來提供給 Kubeflow Jupyter使用： 1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: notebook-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv2EOF NFS PV 建立好後這邊UI點選Jupyter Notebook登入測試，並輸入任意帳號密碼進行登入： Spawner Optinos 這邊可以輸入Notebook Pod節點資源，包含Image、CPU、Memory、GPU數量，最後點選Spawn來完成建立 Server，如下圖所示： kubeflow v0.3版本開始加入，在部署Jupyter Hub時的Event log方便了解部署狀態 部署完後即可直接操作擁有GPU的Jupyter Notebook 查看GPU資源狀態可以透過執行以下指令確認是否GPU可被分配資源：123456789$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUk8s-g1 1k8s-g11 1k8s-g2 1k8s-g3 1k8s-m1 &lt;none&gt;k8s-m2 &lt;none&gt;k8s-m3 &lt;none&gt; 後續有陸續關注，關於 kubeflow CLI的專案kubeflow/Arena，主要供資料科學家運行和監控機器學習培訓工作並以簡單的方式檢查其結果。目前它支持單獨/分佈式TensorFlow訓練。部署基於Kubernetes，helm和Kubeflow。 同時，最終用戶需要GPU資源和節點管理。Arena還提供top檢查Kubernetes集群中可用GPU資源的命令。 123456789101112$ arena top node NAME IPADDRESS ROLE GPU(Total) GPU(Allocated)k8s-g1 172.22.132.84 &lt;none&gt; 1 0k8s-g11 172.22.132.94 &lt;none&gt; 1 0k8s-g2 172.22.132.97 &lt;none&gt; 1 0k8s-g3 172.22.132.86 &lt;none&gt; 1 1k8s-m1 172.22.132.81 master 0 0k8s-m2 172.22.132.82 master 0 0k8s-m3 172.22.132.83 master 0 0-----------------------------------------------------------------------------------------Allocated/Total GPUs In Cluster:1/5 (20%) 透過CLI方式來 watch -n1 arena top node就能很清楚知道節點資源使用狀況。 移除 kubeflow這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 1kubectl -n kubeflow delete namespace kubeflow 相關文章:[kubeflow] Local Volumeを使ったKubeflowの動作検証[kubeflow] Microk8s for Kubeflow #! 更多相關kubeflow Components實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]},{"title":"透過 Microk8s 快速部署 Kubeflow","slug":"microk8s-deploy-kubeflow","date":"2018-09-17T04:12:02.000Z","updated":"2018-11-29T16:30:13.648Z","comments":true,"path":"2018/09/17/microk8s-deploy-kubeflow/","link":"","permalink":"http://yylin1.github.io/2018/09/17/microk8s-deploy-kubeflow/","excerpt":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。","text":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。 什麼是microk8s 簡單來說，microk8s設計為快速輕巧Kubernetes最新版本安裝與主機隔離但不通過虛擬機。通過在單個快照包snap中打包 Kubernetes、Docker.io、iptables和CNI的所有上游二進製文件來實現此隔離。snap是一個應用程序容器，您可以將其想像為Docker容器的輕量級版本。它使用了許多相同的底層技術進行隔離，而不會有網絡隔離的所帶來的開銷。而minikube需使用虛擬化工具來環境隔離創建K8S集群，想必會快照更加耗時。 安裝MultipassMac OS X. (本篇文章以Mac環境進行)) 本機Mac OS 安裝程序安裝Multipass Linux OS 透過snap使用指令安裝 1$ sudo snap install multipass --beta --classic 啟動Ubuntu虛擬機部署前要先下載cloud-init文件1wget https://bit.ly/2tOfMUA -O kubeflow.init 這邊查看一下文件內容，主要是配置kubeflow環境並去執行kubeflow 已經寫好script能更快速部署，有需要修正自己的環境可以修改後執行123456789101112# cloud-init for kubeflowpackage_update: truepackage_upgrade: trueruncmd: - mkdir /kubeflow - wget https://bit.ly/2tp2aOo -O /kubeflow/install-kubeflow-pre-micro.sh - chmod a+x /kubeflow/install-kubeflow-pre-micro.sh - wget https://bit.ly/2tndL0g -O /kubeflow/install-kubeflow.sh - chmod a+x /kubeflow/install-kubeflow.sh - printf \"\\n\\nexport KUBECONFIG=/snap/microk8s/current/client.config\\n\\n\" &gt;&gt; /home/multipass/.bashrc 啟動Multipass VM123456$ multipass launch bionic -n kubeflow -m 8G -d 40G -c 4 --cloud-init kubeflow.initRetrieving image: 66%Retrieving image: 87%Verifying image: -Launched: kubeflow 預設備至Multipass為Kubeflow部署創建的VM上的最低建議設置，這邊是可以自由地調整根據主機的能力和工作負載需求。 依據環境狀況應該很快速就建立好單節點集群，這邊可以查看一下123$ multipass list Name State IPv4 Releasekubeflow RUNNING 192.168.64.2 Ubuntu 18.04 LTS 如果部署有問題隨時都可以透過multipass -h查看 接下來就mount本地文件與multipass配置1$ multipass mount . kubeflow:/multipass 安裝kubernetes接下來我們就可以進入vm，安裝由microk8s驅動的kubernetes，以及部署Kubeflow所需的其他工具。 透過multipass進入vm12345678910111213141516171819202122$ multipass shell kubeflowWelcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.15.0-34-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Mon Sep 17 09:03:39 CST 2018 System load: 0.63 Processes: 153 Usage of /: 2.9% of 38.60GB Users logged in: 0 Memory usage: 2% IP address for enp0s2: 192.168.64.2 Swap usage: 0% Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.multipass@kubeflow:~$ 安裝microk8s、etc1234567891011121314151617$ sudo /kubeflow/install-kubeflow-pre-micro.shSaving to: ‘ksonnet.tar.gz’ksonnet.tar.gz 100%[============================================================================================&gt;] 14.77M 934KB/s in 44s2018-09-17 09:22:01 (342 KB/s) - ‘ksonnet.tar.gz’ saved [15491217/15491217]ks_0.11.0_linux_amd64/CHANGELOG.mdks_0.11.0_linux_amd64/CODE-OF-CONDUCT.mdks_0.11.0_linux_amd64/CONTRIBUTING.mdks_0.11.0_linux_amd64/LICENSEks_0.11.0_linux_amd64/README.mdks_0.11.0_linux_amd64/ksChecking kube-system status until all pods are running (2 not running)Checking kube-system status until all pods are running (3 not running)Before running install-kubeflow.sh, please 'export GITHUB_TOKEN=&lt;your token&gt;' 這邊提供GitHub Token是為了避免ksonnet部署反覆部署可能會造成GitHub限速問題，可以參考建立Token。 更換自己的Token1$ export GITHUB_TOKEN=972702e4fb348d3dba52f5dd3b99d7ae83e857ab kubeflow 部署在vm環境中執行kubeflow script，就可以快速完成kubeflow部署，而這邊會有Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.狀況就依照環境是否把所有應該生成的Pod建立完成，就執行結束。 1234567891011121314$ multipass@kubeflow:~$ /kubeflow/install-kubeflow.shnamespace/kubeflow createdINFO Using context \"microk8s\" from kubeconfig file \"/snap/microk8s/current/client.config\"INFO Creating environment \"default\" with namespace \"default\", pointing to cluster at address \"http://127.0.0.1:8080\"INFO Generating ksonnet-lib data at path '/home/multipass/my-kubeflow/lib/v1.11.1'INFO Retrieved 22 filesINFO Retrieved 5 filesINFO Retrieved 5 files···Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.JupyterHub Port: 31808 以上就完成microk8s+kubeflow部署 部署完成後環境完成後檢查 Kubeflow 元件部署結果： 12345678910multipass@kubeflow:~$ kubectl -n kubeflow get po -o wideNAME READY STATUS RESTARTS AGE IP NODEambassador-68954d75f4-2n6lv 2/2 Running 4 4h 10.1.1.55 kubeflowambassador-68954d75f4-7dgvl 2/2 Running 4 4h 10.1.1.56 kubeflowambassador-68954d75f4-bgs8h 2/2 Running 5 4h 10.1.1.44 kubeflowmxnet-operator-f46557c4f-wmntn 1/1 Running 2 3h 10.1.1.52 kubeflowspartakus-volunteer-d54b65666-klrfk 1/1 Running 2 4h 10.1.1.45 kubeflowtf-hub-0 1/1 Running 2 4h 10.1.1.47 kubeflowtf-job-dashboard-784cdcbb4f-j2cjx 1/1 Running 2 4h 10.1.1.51 kubeflowtf-job-operator-85b46d47b7-xfwmp 1/1 Running 2 4h 10.1.1.48 kubeflow 運行JupyterHub測試透過查看vm狀態中，完成後連接 http://IP:Port，並輸入任意帳號密碼進行登入。 Spawner options可以自行配置Jupyter Notebook 資源狀態 Image: 預設會有多種映像檔可以使用其他參數可調整 CPU、Memory、GPU資源限制 按下Spawn kubeflow就會啟動一個pod配置資源給Jupyter notebook，部署過程時Image下載需要花一點時間。 配置過程中有時會後有狀況，可能需要多嘗試 完成後即可在透過Jupyter Notebook 編寫Model進行DL訓練 Kubernetes Addonsmicrok8s在Kubernetes安裝了一個準系統。這代表只要安裝和運行api-server、controller-manager、scheduler、kubelet，cni、kube-proxy。可以使用該microk8s.enable命令運行kube-dns和Dashboard等附加服務。 1microk8s.enable dns dashboard 使用該disable命令隨時禁用這些插件1microk8s.disable dashboard dns 12345678$ microk8s.kubectl cluster-infoKubernetes master is running at http://127.0.0.1:8080Heapster is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/heapster/proxyKubeDNS is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyGrafana is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxyInfluxDB is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:http/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 停止並重新啟動microk8 (測試環境為Mac) 在 Linx 環境指令都是偷過snap enable/disable 來管理microk8s Mac\b\b環境可以透過multipass -h查詢可執行指令 暫時關閉vm運行1multipass stop kubeflow 啟動vm運行1multipass stop kubeflow 查看multipass目前vm狀態12Name State IPv4 Releasekubeflow STOPPED -- Ubuntu 18.04 LTS 刪除microk8s透過multipass list 檢查instances狀態 如果只下delete只有instances狀態刪除，是可以透過recover恢復已刪除的instances123multipass delete kubeflowName State IPv4 Releasekubeflow DELETED -- Not Available 所以我們要透過purge來清除永久清除所有已刪除的instances1multipass purge 相關文章:[Kubernetes] A local Kubernetes with microk8s[kubeflow] Microk8s for Kubeflow 本篇文章主介紹microk8s部署kubeflow，更多相關kubeflow 實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"microk8s","slug":"microk8s","permalink":"http://yylin1.github.io/tags/microk8s/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]}]}