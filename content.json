{"meta":{"title":"Yi Yang's Blog","subtitle":"Learning Note","description":"test test","author":"Yi Yang","url":"http://yylin1.github.io"},"pages":[{"title":"About me","date":"2018-09-15T07:17:15.000Z","updated":"2018-09-17T09:37:15.800Z","comments":true,"path":"about/index.html","permalink":"http://yylin1.github.io/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-09-17T09:43:56.985Z","updated":"2018-09-17T09:43:56.983Z","comments":true,"path":"tags/index.html","permalink":"http://yylin1.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-09-17T09:43:34.603Z","updated":"2018-09-17T09:43:34.603Z","comments":true,"path":"categories/index.html","permalink":"http://yylin1.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"快速部署 kubeflow v0.3 容器機器學習平台","slug":"kubeflow-deploy-v0.3.0","date":"2018-10-23T12:53:45.000Z","updated":"2018-10-23T13:06:21.232Z","comments":true,"path":"2018/10/23/kubeflow-deploy-v0.3.0/","link":"","permalink":"http://yylin1.github.io/2018/10/23/kubeflow-deploy-v0.3.0/","excerpt":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。","text":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。 Kubeflow目標不是在於重建其他服務，而是提供一個最佳開發系統，來部署到任何集群中，有效確保ML在集群之間移動性，並輕鬆將任務擴展任何集群。 由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠運行部署 Kubeflow。 在不同基礎設施上協助實現Machine learning 工作流程「更加簡單」與「便於攜帶」、「可擴展性」的部署，在不同基礎設施上(Local Laptop、GPU Server cluster、Cloud Production Cluster) 簡述Kubelfow工作流程： 下載Kubeflow shell script 和 配置文件 自定義 ksonnet部署所需的JSON組件 執行shell script將容器部署到所選擇的Kubernetes Enveroment 開始執行機器學習訓練 我們透過Kubeflow工具管理，可以解決ML環境配置困難的問題，其中在Kubeflow提供Operator，對不同的機器學習框架，擴展的 Kubernetes API 進行自動建立、管理與配置應用程式容器實例，並提供訓練在不同環境和服務，包含數據準備、模型培訓、預測服務和服務管理。 Kubernetes 有提供不同集群部署支援，包括單節點Minikube部署、 透過Hypervisor部署的Multipass &amp; Microk8s、Cloud。無論你在哪裡運行Kubernetes，你都能夠運行Kubeflow。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Destop，測試環境為實體機器部署HA Cluster： IP Address Role vCPU RAM Extra Device 140.128.18.98 VIP 140.128.18.81 k8s-m1 8 16G GTX 1060 6G 140.128.18.82 k8s-m2 8 16G GTX 1060 6G 140.128.18.83 k8s-m3 8 16G GTX 1060 6G 140.128.18.84 k8s-g1 8 16G GTX 1060 6G 140.128.18.97 k8s-g2 8 16G GTX 1060 6G 140.128.18.86 k8s-g3 8 16G GTX 1060 6G 140.128.18.94 k8s-g11 8 16G GTX 1060 6G kubernetes部署可以參考 開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集教學 本次部署使用版本 kubernetes v1.11.2 Master x 3, Worker x 4 ksonnet 0.13.0 kubeflow v0.3.0 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點確認已經安裝 NVIDIA driver、CUDA、Docker、NVIDIA Docker，以下為實驗環境版本資訊： CUDA Version: 10.0.130 Driver Version: 410.48 Docker Version: 18.03.0-ce NVIDIA Docker: 2.0.3 安裝 NFS設置Kubeflow需要Volumes來存儲數據，建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： 12345678910# 在 master 執行，這邊nfs-server建立在k8s-m1節點$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data# 可以在 `nfs-data`資料夾建立多個pv資料夾，提供kubeflow儲存使用$ cd nds-data$ echo \"/nfs-data *(rw,sync,no_root_squash,no_subtree_check)\" | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 這邊也可以使用其他存儲數據方式: Kubernetes: Local Volume 安裝 KsonnetKsonnet 是一個命令工具，可以更輕鬆地管理由多個組件組成的複雜部署，簡化編寫和部署 Kubernetes 配置。 123456789$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.0/ks_0.13.0_linux_amd64.tar.gz$ tar xvf ks_0.13.0_linux_amd64.tar.gz$ sudo cp ks_0.13.0_linux_amd64 /usr/local/bin/$ chmod +x ks$ ks versionksonnet version: 0.13.0jsonnet version: v0.11.2client-go version: kubernetes-1.10.4 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。部署版本選擇為kubeflow v0.3.0。 下載 Kubeflow 腳本下載GitHub中Kubeflow設置所需配置： 12345678910# 建立配置指定資料夾路徑，可自行更換名稱$ KUBEFLOW_SRC=mykfsrc $ mkdir $&#123;KUBEFLOW_SRC&#125;$ cd $&#123;KUBEFLOW_SRC&#125;# 部署kubeflow版本標籤，可自行選擇其他分支$ export KUBEFLOW_TAG=v0.3.0 # 執行shell script$ curl https://raw.githubusercontent.com/kubeflow/kubeflow/$&#123;KUBEFLOW_TAG&#125;/scripts/download.sh | bash 下載後目錄下會多kubeflow、scripts資料夾: 123$ tree -L 1 ├── kubeflow└── scripts 執行腳本配置和部署Kubeflow123456789101112131415161718# 退回上一層建立對應路徑$ cd ../$ KUBEFLOW_REPO=$(pwd)/mykfsrc# $&#123;KFAPP&#125; 為配置 kubeflow 儲存目錄的名稱，執行init時將創建此目錄# ksonnet應用程式將在 $&#123;KFAPP&#125;/ks_app目錄中創建$ KFAPP=mykfapp$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh init $&#123;KFAPP&#125; --platform none# 安裝 Kubeflow 套件至應用程式目錄$ cd $&#123;KFAPP&#125;$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh generate k8s# 定義 Namespace 為kubeflow來管理相關資源$ kubectl create ns kubeflow# 部署 Kubeflow$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh apply k8s ！部署預設為default，這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 完成後檢查 Kubeflow 元件部署結果：12345678910111213141516171819$ kubectl -n kubeflow get po -o wide ambassador-868d5dbdc4-5fgdw 3/3 Running 0 7m 10.244.2.90 k8s-g3 &lt;none&gt;ambassador-868d5dbdc4-qcp5z 3/3 Running 1 7m 10.244.5.78 k8s-g11 &lt;none&gt;ambassador-868d5dbdc4-x9bfc 3/3 Running 0 7m 10.244.3.96 k8s-g2 &lt;none&gt;argo-ui-84464bd59c-hmhmd 1/1 Running 0 7m 10.244.4.89 k8s-g1 &lt;none&gt;centraldashboard-c76877875-nqbhg 1/1 Running 0 7m 10.244.3.97 k8s-g2 &lt;none&gt;modeldb-backend-58969447f6-z2xmf 1/1 Running 1 11s 10.244.3.102 k8s-g2 &lt;none&gt;modeldb-db-57b855f5b7-dw5s6 1/1 Running 0 7m 10.244.5.79 k8s-g11 &lt;none&gt;modeldb-frontend-769d5bdd66-njkqs 1/1 Running 0 7m 10.244.3.99 k8s-g2 &lt;none&gt;spartakus-volunteer-5bfd5876fb-b86h5 1/1 Running 0 7m 10.244.2.91 k8s-g3 &lt;none&gt;studyjob-controller-56588dc6f9-shg72 1/1 Running 0 7m 10.244.3.101 k8s-g2 &lt;none&gt;tf-hub-0 1/1 Running 0 7m 10.244.4.86 k8s-g1 &lt;none&gt;tf-job-dashboard-7777b6bf-qltfc 1/1 Running 0 7m 10.244.3.98 k8s-g2 &lt;none&gt;tf-job-operator-v1alpha2-5c5b4dcfdf-x5rkv 1/1 Running 0 7m 10.244.4.87 k8s-g1 &lt;none&gt;vizier-core-5584ccbd8-l2bxr 0/1 CrashLoopBackOff 4 7m 10.244.5.80 k8s-g11 &lt;none&gt;vizier-db-547967c899-mf988 0/1 Pending 0 7m &lt;none&gt; &lt;none&gt; &lt;none&gt;vizier-suggestion-grid-8547dbb55b-k8tw6 1/1 Running 0 7m 10.244.4.90 k8s-g1 &lt;none&gt;vizier-suggestion-random-d7c5cd68b-7p9nz 1/1 Running 0 7m 10.244.3.100 k8s-g2 &lt;none&gt;workflow-controller-5c95f95f58-d9plp 1/1 Running 0 7m 10.244.4.88 k8s-g1 &lt;none&gt; 可以看到vizier狀態為Pending，失敗的原因是PersistentVolume未分配給vizer-db（實際是MySQL）。所以vizier-db正在等待並且未能啟動vizier-core。 這邊透過使用kubectl describe命令檢查原因: 123456789$ kubectl -n kubeflow describe pod vizier-db-547967c899-mf988Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m (x417 over 12m) default-scheduler pod has unbound PersistentVolumeClaims (repeated 4 times)$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Pending 15m 這邊我們建立一個 NFS PV 來提供給 vizier-db 使用：1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: vizier-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 140.128.18.81 path: /nfs-data/kubeflow-pv1EOF 請更換自己的 nfs-server IP 再次查看 PV &amp; PVC 狀態，就可以看到vizier-db獲得 20Gi 的 ersistentVolume 空間 1234567$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEvizier-pv 20Gi RWO Retain Bound kubeflow/vizier-db 16s$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Bound vizier-pv 20Gi RWO 20m 其他儲存方式參考：Kubernetes: Local Volumeの検証 暫時問題：modeldb-backend 會不定時 Crash Kubeflow UI查看Service中，可以看到kubeflow v0.3.0版本中，附帶了許多Web UI: Argo UI Central UI for navigation JupyterHub Katib TFJobs Dashboard 這邊我們透過Service查看一下目前pod狀態： 123456789101112131415161718$ kubectl -n kubeflow get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador ClusterIP 10.98.216.151 &lt;none&gt; 80/TCP 7m service=ambassadorambassador-admin ClusterIP 10.102.20.172 &lt;none&gt; 8877/TCP 6m service=ambassadorargo-ui NodePort 10.102.36.121 &lt;none&gt; 80:32530/TCP 6m app=argo-uicentraldashboard ClusterIP 10.99.255.3 &lt;none&gt; 80/TCP 6m app=centraldashboardk8s-dashboard ClusterIP 10.110.209.78 &lt;none&gt; 443/TCP 6m k8s-app=kubernetes-dashboardmodeldb-backend ClusterIP 10.102.179.187 &lt;none&gt; 6543/TCP 6m app=modeldb,component=backendmodeldb-db ClusterIP 10.102.143.161 &lt;none&gt; 27017/TCP 6m app=modeldb,component=dbmodeldb-frontend ClusterIP 10.100.76.92 &lt;none&gt; 3000/TCP 6m app=modeldb,component=frontendstatsd-sink ClusterIP 10.98.167.5 &lt;none&gt; 9102/TCP 7m service=ambassadortf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 6m app=tf-hubtf-hub-lb ClusterIP 10.106.99.192 &lt;none&gt; 80/TCP 6m app=tf-hubtf-job-dashboard ClusterIP 10.100.66.145 &lt;none&gt; 80/TCP 6m name=tf-job-dashboardvizier-core NodePort 10.105.154.126 &lt;none&gt; 6789:30678/TCP 6m app=vizier,component=corevizier-db ClusterIP 10.100.32.162 &lt;none&gt; 3306/TCP 6m app=vizier,component=dbvizier-suggestion-grid ClusterIP 10.105.68.247 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-gridvizier-suggestion-random ClusterIP 10.110.164.76 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-random 這時候就可以透過kubeflow UI，能引導到 登入JupyterHub，但這邊需要修改 Kubernetes Service，透過以下指令進行： 123456789101112# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc tf-hub-lb...sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125;...#修改後就可以透過NodePort接上Port進入顯示Jupyter NotebookNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador NodePort 10.98.216.151 &lt;none&gt; 80:32752/TCP 15m service=ambassador 完成後連接 http://Master_IP:Port即可看到kubeflow UI。 可以透過上方連結查看Jupyter Hub、TFjob Dashboard、Kubernetes Dashboard 等。 測試 Jupyter Notebook這邊需要再次先建立一個 NFS PV 來提供給 Kubeflow Jupyter使用： 1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: notebook-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 140.128.18.81 path: /nfs-data/kubeflow-pv2EOF NFS PV 建立好後這邊UI點選Jupyter Notebook登入測試，並輸入任意帳號密碼進行登入： Spawner Optinos 這邊可以輸入Notebook Pod節點資源，包含Image、CPU、Memory、GPU數量，最後點選Spawn來完成建立 Server，如下圖所示： kubeflow v0.3版本開始加入，在部署Jupyter Hub時的Event log方便了解部署狀態 部署完後即可直接操作擁有GPU的Jupyter Notebook 查看GPU資源狀態可以透過執行以下指令確認是否GPU可被分配資源：123456789$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUk8s-g1 1k8s-g11 1k8s-g2 1k8s-g3 1k8s-m1 &lt;none&gt;k8s-m2 &lt;none&gt;k8s-m3 &lt;none&gt; 後續有陸續關注，關於 kubeflow CLI的專案kubeflow/Arena，主要供資料科學家運行和監控機器學習培訓工作並以簡單的方式檢查其結果。目前它支持單獨/分佈式TensorFlow訓練。部署基於Kubernetes，helm和Kubeflow。 同時，最終用戶需要GPU資源和節點管理。Arena還提供top檢查Kubernetes集群中可用GPU資源的命令。 123456789101112$ arena top node NAME IPADDRESS ROLE GPU(Total) GPU(Allocated)k8s-g1 140.128.18.84 &lt;none&gt; 1 0k8s-g11 140.128.18.94 &lt;none&gt; 1 0k8s-g2 140.128.18.97 &lt;none&gt; 1 0k8s-g3 140.128.18.86 &lt;none&gt; 1 1k8s-m1 140.128.18.81 master 0 0k8s-m2 140.128.18.82 master 0 0k8s-m3 140.128.18.83 master 0 0-----------------------------------------------------------------------------------------Allocated/Total GPUs In Cluster:1/5 (20%) 透過CLI方式來 watch -n1 arena top node就能很清楚知道節點資源使用狀況。 移除 kubeflow這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 1kubectl -n kubeflow delete namespace kubeflow 相關文章:[kubeflow] Local Volumeを使ったKubeflowの動作検証[kubeflow] Microk8s for Kubeflow #! 更多相關kubeflow Components實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]},{"title":"透過Microk8s快速部署kubeflow","slug":"microk8s-deploy-kubeflow","date":"2018-09-17T04:12:02.000Z","updated":"2018-10-23T13:07:05.923Z","comments":true,"path":"2018/09/17/microk8s-deploy-kubeflow/","link":"","permalink":"http://yylin1.github.io/2018/09/17/microk8s-deploy-kubeflow/","excerpt":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。","text":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。 什麼是microk8s 簡單來說，microk8s設計為快速輕巧Kubernetes最新版本安裝與主機隔離但不通過虛擬機。通過在單個快照包snap中打包 Kubernetes、Docker.io、iptables和CNI的所有上游二進製文件來實現此隔離。snap是一個應用程序容器，您可以將其想像為Docker容器的輕量級版本。它使用了許多相同的底層技術進行隔離，而不會有網絡隔離的所帶來的開銷。而minikube需使用虛擬化工具來環境隔離創建K8S集群，想必會快照更加耗時。 安裝MultipassMac OS X. (本篇文章以Mac環境進行)) 本機Mac OS 安裝程序安裝Multipass Linux OS 透過snap使用指令安裝 1$ sudo snap install multipass --beta --classic 啟動Ubuntu虛擬機部署前要先下載cloud-init文件1wget https://bit.ly/2tOfMUA -O kubeflow.init 這邊查看一下文件內容，主要是配置kubeflow環境並去執行kubeflow 已經寫好script能更快速部署，有需要修正自己的環境可以修改後執行123456789101112# cloud-init for kubeflowpackage_update: truepackage_upgrade: trueruncmd: - mkdir /kubeflow - wget https://bit.ly/2tp2aOo -O /kubeflow/install-kubeflow-pre-micro.sh - chmod a+x /kubeflow/install-kubeflow-pre-micro.sh - wget https://bit.ly/2tndL0g -O /kubeflow/install-kubeflow.sh - chmod a+x /kubeflow/install-kubeflow.sh - printf \"\\n\\nexport KUBECONFIG=/snap/microk8s/current/client.config\\n\\n\" &gt;&gt; /home/multipass/.bashrc 啟動Multipass VM123456$ multipass launch bionic -n kubeflow -m 8G -d 40G -c 4 --cloud-init kubeflow.initRetrieving image: 66%Retrieving image: 87%Verifying image: -Launched: kubeflow 預設備至Multipass為Kubeflow部署創建的VM上的最低建議設置，這邊是可以自由地調整根據主機的能力和工作負載需求。 依據環境狀況應該很快速就建立好單節點集群，這邊可以查看一下123$ multipass list Name State IPv4 Releasekubeflow RUNNING 192.168.64.2 Ubuntu 18.04 LTS 如果部署有問題隨時都可以透過multipass -h查看 接下來就mount本地文件與multipass配置1$ multipass mount . kubeflow:/multipass 安裝kubernetes接下來我們就可以進入vm，安裝由microk8s驅動的kubernetes，以及部署Kubeflow所需的其他工具。 透過multipass進入vm12345678910111213141516171819202122$ multipass shell kubeflowWelcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.15.0-34-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Mon Sep 17 09:03:39 CST 2018 System load: 0.63 Processes: 153 Usage of /: 2.9% of 38.60GB Users logged in: 0 Memory usage: 2% IP address for enp0s2: 192.168.64.2 Swap usage: 0% Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.multipass@kubeflow:~$ 安裝microk8s、etc1234567891011121314151617$ sudo /kubeflow/install-kubeflow-pre-micro.shSaving to: ‘ksonnet.tar.gz’ksonnet.tar.gz 100%[============================================================================================&gt;] 14.77M 934KB/s in 44s2018-09-17 09:22:01 (342 KB/s) - ‘ksonnet.tar.gz’ saved [15491217/15491217]ks_0.11.0_linux_amd64/CHANGELOG.mdks_0.11.0_linux_amd64/CODE-OF-CONDUCT.mdks_0.11.0_linux_amd64/CONTRIBUTING.mdks_0.11.0_linux_amd64/LICENSEks_0.11.0_linux_amd64/README.mdks_0.11.0_linux_amd64/ksChecking kube-system status until all pods are running (2 not running)Checking kube-system status until all pods are running (3 not running)Before running install-kubeflow.sh, please 'export GITHUB_TOKEN=&lt;your token&gt;' 這邊提供GitHub Token是為了避免ksonnet部署反覆部署可能會造成GitHub限速問題，可以參考建立Token。 更換自己的Token1$ export GITHUB_TOKEN=972702e4fb348d3dba52f5dd3b99d7ae83e857ab kubeflow 部署在vm環境中執行kubeflow script，就可以快速完成kubeflow部署，而這邊會有Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.狀況就依照環境是否把所有應該生成的Pod建立完成，就執行結束。 1234567891011121314$ multipass@kubeflow:~$ /kubeflow/install-kubeflow.shnamespace/kubeflow createdINFO Using context \"microk8s\" from kubeconfig file \"/snap/microk8s/current/client.config\"INFO Creating environment \"default\" with namespace \"default\", pointing to cluster at address \"http://127.0.0.1:8080\"INFO Generating ksonnet-lib data at path '/home/multipass/my-kubeflow/lib/v1.11.1'INFO Retrieved 22 filesINFO Retrieved 5 filesINFO Retrieved 5 files···Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.JupyterHub Port: 31808 以上就完成microk8s+kubeflow部署 部署完成後環境完成後檢查 Kubeflow 元件部署結果： 12345678910multipass@kubeflow:~$ kubectl -n kubeflow get po -o wideNAME READY STATUS RESTARTS AGE IP NODEambassador-68954d75f4-2n6lv 2/2 Running 4 4h 10.1.1.55 kubeflowambassador-68954d75f4-7dgvl 2/2 Running 4 4h 10.1.1.56 kubeflowambassador-68954d75f4-bgs8h 2/2 Running 5 4h 10.1.1.44 kubeflowmxnet-operator-f46557c4f-wmntn 1/1 Running 2 3h 10.1.1.52 kubeflowspartakus-volunteer-d54b65666-klrfk 1/1 Running 2 4h 10.1.1.45 kubeflowtf-hub-0 1/1 Running 2 4h 10.1.1.47 kubeflowtf-job-dashboard-784cdcbb4f-j2cjx 1/1 Running 2 4h 10.1.1.51 kubeflowtf-job-operator-85b46d47b7-xfwmp 1/1 Running 2 4h 10.1.1.48 kubeflow 運行JupyterHub測試透過查看vm狀態中，完成後連接 http://IP:Port，並輸入任意帳號密碼進行登入。 Spawner options可以自行配置Jupyter Notebook 資源狀態 Image: 預設會有多種映像檔可以使用其他參數可調整 CPU、Memory、GPU資源限制 按下Spawn kubeflow就會啟動一個pod配置資源給Jupyter notebook，部署過程時Image下載需要花一點時間。 配置過程中有時會後有狀況，可能需要多嘗試 完成後即可在透過Jupyter Notebook 編寫Model進行DL訓練 Kubernetes Addonsmicrok8s在Kubernetes安裝了一個準系統。這代表只要安裝和運行api-server、controller-manager、scheduler、kubelet，cni、kube-proxy。可以使用該microk8s.enable命令運行kube-dns和Dashboard等附加服務。 1microk8s.enable dns dashboard 使用該disable命令隨時禁用這些插件1microk8s.disable dashboard dns 12345678$ microk8s.kubectl cluster-infoKubernetes master is running at http://127.0.0.1:8080Heapster is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/heapster/proxyKubeDNS is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyGrafana is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxyInfluxDB is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:http/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 停止並重新啟動microk8 (測試環境為Mac) 在 Linx 環境指令都是偷過snap enable/disable 來管理microk8s Mac\b\b環境可以透過multipass -h查詢可執行指令 暫時關閉vm運行1multipass stop kubeflow 啟動vm運行1multipass stop kubeflow 查看multipass目前vm狀態12Name State IPv4 Releasekubeflow STOPPED -- Ubuntu 18.04 LTS 刪除microk8s透過multipass list 檢查instances狀態 如果只下delete只有instances狀態刪除，是可以透過recover恢復已刪除的instances123multipass delete kubeflowName State IPv4 Releasekubeflow DELETED -- Not Available 所以我們要透過purge來清除永久清除所有已刪除的instances1multipass purge 相關文章:[Kubernetes] A local Kubernetes with microk8s[kubeflow] Microk8s for Kubeflow 本篇文章主介紹microk8s部署kubeflow，更多相關kubeflow 實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"microk8s","slug":"microk8s","permalink":"http://yylin1.github.io/tags/microk8s/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]}]}