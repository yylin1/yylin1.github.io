{"meta":{"title":"Yi Yang's Blog","subtitle":"Learning Note","description":"Blog","author":"Yi Yang","url":"http://yylin1.github.io"},"pages":[{"title":"","date":"2018-10-23T13:43:25.936Z","updated":"2018-10-23T13:43:25.936Z","comments":true,"path":"google1926343a7854156d.html","permalink":"http://yylin1.github.io/google1926343a7854156d.html","excerpt":"","text":"google-site-verification: google1926343a7854156d.html"},{"title":"About me","date":"2018-09-15T07:17:15.000Z","updated":"2018-09-17T09:37:15.800Z","comments":true,"path":"about/index.html","permalink":"http://yylin1.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-09-17T09:43:34.603Z","updated":"2018-09-17T09:43:34.603Z","comments":true,"path":"categories/index.html","permalink":"http://yylin1.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-09-17T09:43:56.985Z","updated":"2018-09-17T09:43:56.983Z","comments":true,"path":"tags/index.html","permalink":"http://yylin1.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"如何自行編譯HPL-GPU來測試Benchmarks","slug":"build-hpl-gpu","date":"2018-10-23T13:11:49.000Z","updated":"2018-10-23T13:25:40.462Z","comments":true,"path":"2018/10/23/build-hpl-gpu/","link":"","permalink":"http://yylin1.github.io/2018/10/23/build-hpl-gpu/","excerpt":"構建 NVIDIA CUDA Linpack 環境執行環境非常困難，網路上訊息非常少，然後linkpack測試更新版本已經有一段時間，記錄實作主要參考「Hybrid HPL(GPU版HPL)安装教程」與「AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる」兩個文章教學，並嘗試運行現在環境支援的版本，部署過程記錄。","text":"構建 NVIDIA CUDA Linpack 環境執行環境非常困難，網路上訊息非常少，然後linkpack測試更新版本已經有一段時間，記錄實作主要參考「Hybrid HPL(GPU版HPL)安装教程」與「AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる」兩個文章教學，並嘗試運行現在環境支援的版本，部署過程記錄。 環境部署資訊Linpack 部署的版本資訊： Mpich: v3.2.1 Openmpi: v1.10.3 Intel MKL: l_mkl_2019.0.117 Linpack: hpl-2.0_FERMI_v15 實驗環境安裝作業系統採用Ubuntu 16.04 Desktop，測試環境為實體機器： Role vCPU RAM Extra Device ubuntu 8 16G GTX 1060 6G 事前準備:::danger測試 Linkpack 之前，需要確保以下條件達成：確認環境是否安裝以下NVIDIA driver、CUDA、Intel MKL`Openmpi、mpich2`，並設定好環境變數。::: 安裝 NVIDIA驅動和CUDA tookit由於CUDA toolkit中，安裝時已包含了NVIDIA Driver，可一併安裝 1234$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：123456789101112131415161718192021222324252627282930$ lsmod | grep nvidianvidia_uvm 790528 0nvidia_drm 40960 2nvidia_modeset 1089536 3 nvidia_drmdrm_kms_helper 167936 1 nvidia_drmdrm 360448 5 nvidia_drm,drm_kms_helpernvidia 14032896 96 nvidia_modeset,nvidia_uvmipmi_msghandler 45056 2 nvidia,ipmi_devintf$ cat /usr/local/cuda/version.txtCUDA Version 9.2.148$ nvidia-smiTue Oct 2 18:15:47 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 396.44 Driver Version: 396.44 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:03:00.0 On | N/A || 39% 31C P8 7W / 120W | 52MiB / 6077MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1603 G /usr/lib/xorg/Xorg 49MiB |+-----------------------------------------------------------------------------+ 準備LinpackLink : https://developer.nvidia.com/rdp/assets/cuda-accelerated-linpack-linux64從上面的連結，登入CUDA註冊開發者會員，下載linpack for Linux64版本，這裡下載到的版本為hpl-2.0_FERMI_v15.tgz。 參考連結Hybrid HPL(GPU版HPL)安装教程AWS-GPUとスパコンを比較する方法-スパコン用ベンチマークソフトを動かしてみる 安裝INTEL MKL透過連結註冊帳號https://software.intel.com/en-us/qualify-for-free-software 註冊後，它會向您發送序列號於信箱，以便進行安裝準備。 這邊是下載最新l_mkl_2019.0.117.tgz版本 下載取得l_mkl_2019.0.117.tgz後，即可透過install.sh運行安裝。 12$ tar zxvf l_mkl_2019.0.117.tgz$ cd l_mkl_2019.0.117 Intel mkl的安裝很簡單的，每一步也都有說明，按Enter繼續下一步預設設定安裝即可，安裝到某一步會要求輸入序列號，申請30天試用版所給的那個序列號。123456789101112131415161718192021222324$ sh ./install.sh--------------------------------------------------------------------------------Initializing, please wait...--------------------------------------------------------------------------------Welcome--------------------------------------------------------------------------------Welcome to the Intel(R) Math Kernel Library 2019 for Linux*--------------------------------------------------------------------------------You will complete the following steps: 1. Welcome 2. License Agreement 3. Options 4. Installation 5. Complete----------------------------------------------------------------------------------------------------------------------------------------------------------------Press \"Enter\" key to continue or \"q\" to quit:License Agreement-------------------------------------------------------------------------------- 確認後會安裝一些套件，這裡就可以看到MKL預設情況下，會安裝在/opt/intel下面。123456789101112131415161718------------------------Options &gt; Pre-install Summary--------------------------------------------------------------------------------Install location: /opt/intelComponent(s) selected: Intel Math Kernel Library 2019 for C/C++ 2.6GB Intel MKL core libraries for C/C++ Intel TBB threading support GNU* C/C++ compiler support Intel Math Kernel Library 2019 for Fortran 2.6GB Intel MKL core libraries for Fortran GNU* Fortran compiler support Fortran 95 interfaces for BLAS and LAPACK Install space required: 2.8GB 編譯完成後，即會顯示安裝資訊。 123456789101112------------------------Complete--------------------------------------------------------------------------------Thank you for installing Intel(R) Math Kernel Library 2019 for Linux*.If you have not done so already, please register your product with IntelRegistration Center to create your support account and take full advantage ofyour product purchase.Your support account gives you access to free product updates and upgradesas well as Priority Customer support at the Online Service Centerhttps://supporttickets.intel.com. 完整安裝過程於Gist。 安装mpich2123456$ wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gztar zxvf mpich-3.2.1.tar.gz$ cd mpich-3.2.1./configure -prefix=/home/username/mpich$ make$ make install 配置環境打開/etc/environment1$ vim /etc/environment 將自己的路徑添加到PATH最後，注意別忘了冒號“：”，添加後的PATH如下1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存退出，在終端輸入source /etc/environment再輸入echo $PATH測試發現已經更新，環境變量配置成功。 本文来自 ForTheDreamSMS 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/baidu_34045013/article/details/78237842?utm_source=copy 安裝參考:ubuntu16.04安裝配置mpich2 安裝openmpi1234567$ wget -c https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.3.tar.gz$ tar zxvf openmpi-1.10.3.tar.gz$ cd openmpi-1.10.3$ ./configure --prefix=/opt/openmpi$ make$ sudo make install 安裝make和make instal需要一段時間，等待完成即可，openmpi環境配置會在後面統一設定。 參考 : OpenMPI設定叢集環境 配置環境變量首先更改環境變量PATH： 1sudo vim /etc/environment 在PATH變量加上/usr/local/cuda-9.2/bin,前面要有分號，後面沒有，修改後例如下面這樣：1PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-9.2/bin:/home/username/mpich/bin\" 保存文件，然後再執行：source /etc/environment完成後，可以執行echo $PATH,查看是否修改成功 接著還需更改ldconfig12cd /etc/ld.so.conf.d/sudo vim hpl.conf 輸入如下內容12345/usr/local/cuda-9.2/lib64/lib/opt/intel/mkl/lib/intel64/opt/intel/lib/intel64/home/ubuntu/hpl/src/cuda 最後一行/home/使用者/hpl/src/cuda是編譯HPL時才需要改的，在這裡一併修改。這個目錄就是編譯hpl時，hpl的路徑。 添加上述內容並保存後，執行1sudu ldconfig 可以輸入下面命令進行檢驗，有輸出內容就對了1sudo ldconfig -v | grep cuda 接著還要執行Intel MKL的環境變量設置腳本1234export LD_LIBRARY_PATH=/opt/intel/mkl/lib/intel64:/opt/intel/compilers_and_libraries/linux/lib/intel64:/home/ubuntu/hpl/src/cuda:/opt/openmpi/libexport PATH=/opt/openmpi/bin:$PATHsource /opt/intel/compilers_and_libraries_2019.0.117/linux/mkl/bin/mklvars.sh intel64 請確認以上路徑與當前環境上所有套件的路徑是否對應存在，再執行1source ~/.bashrc 這樣，環境變量就設置好了。最好echo $PATH查看下是否多了一行intel的信息，如果沒有配置成功的話，在編譯HPL時會提示/usr/bin/ld: cannot find -liomp5的錯誤。 開始編譯Linpack benchmark for CUDA這邊將hpl-2.0_FERMI_v15.tgz解壓縮放置主目錄下hpl文件夾，可以依照自己設定的路徑對應編譯。 12345$ tar -xvf hpl-2.0_FERMI_v15.tgz –C ~/hpl$ cd ~/hpl$ lsbin BUGS COPYRIGHT CUDA_LINPACK_README.txt HISTORY include INSTALL lib Make.CUDA Makefile makes Make.top man README setup src testing TODO TUNING www 編譯Make.CUDA編輯配置這時還需要編輯Make.CUDA測試環境參考連結，需更改Make.CUDA中的TOPdir為hpl的目錄。 123456103 TOPdir = /home/ubuntu/hpl132 LAdir = /opt/intel/mkl/lib/intel64133 LAMP5dir = /opt/intel/compilers_and_libraries/linux/lib/intel64134 LAinc = -I/opt/intel/mkl/include 接著可以開始編譯了12cd ~/hplmake arch=CUDA 如果沒有提示錯誤，就是編譯成功了。 編譯完成後，還需要修改~/hpl/bin/CUDA/run_linpack中的HPL_DIR為你hpl的路徑 1HPL_DIR=/home/ubuntu/hpl 修改完成後就可以開始測試了。 測試之前建議把HPL.dat的參數改小一點，N改成8000，這樣所需的測試時間少。也先把P，Q，PxQ都改成1，保證可以執行測試: 1$ mpirun -n 1 ./run_linpack 輸出結果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ mpirun -n 1 ./run_linpack================================================================================HPLinpack 2.0 -- High-Performance Linpack benchmark -- September 10, 2008Written by A. Petitet and R. Clint Whaley, Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V : Wall time / encoded variant.N : The order of the coefficient matrix A.NB : The partitioning blocking factor.P : The number of process rows.Q : The number of process columns.Time : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N : 25000 30000NB : 768 1024 1280 1536PMAP : Row-major process mappingP : 1Q : 1PFACT : LeftNBMIN : 2NDIV : 2RFACT : LeftBCAST : 1ringDEPTH : 1SWAP : Spread-roll (long)L1 : no-transposed formU : no-transposed formEQUIL : yesALIGN : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed: ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be 1.110223e-16- Computational tests pass if scaled residuals are less than 16.0================================================================================T/V N NB P Q Time Gflops--------------------------------------------------------------------------------WR10L2L2 25000 768 1 1 43.07 2.419e+02--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)= 0.0040802 ...... PASSED================================================================================ 完整環境配置與設置有放到Gits，可以提供參考。","categories":[],"tags":[{"name":"linpack","slug":"linpack","permalink":"http://yylin1.github.io/tags/linpack/"},{"name":"GPU","slug":"GPU","permalink":"http://yylin1.github.io/tags/GPU/"}]},{"title":"快速部署 Kubeflow v0.3 容器機器學習平台","slug":"kubeflow-deploy-v0.3.0","date":"2018-10-23T12:53:45.000Z","updated":"2018-10-30T15:42:56.172Z","comments":true,"path":"2018/10/23/kubeflow-deploy-v0.3.0/","link":"","permalink":"http://yylin1.github.io/2018/10/23/kubeflow-deploy-v0.3.0/","excerpt":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。","text":"Kubeflow 是由 Google 與相關公司共同發起的開源專案，其目標是利用 Kubernetes 容器平台來簡化機器學習的建置與執行過程，使之更簡單、可重複的攜帶性與可擴展，並提供一套標準的雲原生(Cloud Native)機器學習解決方案，以幫助演算法科學家專注在模型開發本身，而由於 Kubeflow 以 Kubernetes 做基礎，因此只要有 Kubernetes 地方就能夠快速部署並執行。 Kubeflow目標不是在於重建其他服務，而是提供一個最佳開發系統，來部署到任何集群中，有效確保ML在集群之間移動性，並輕鬆將任務擴展任何集群。 由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠運行部署 Kubeflow。 在不同基礎設施上協助實現Machine learning 工作流程「更加簡單」與「便於攜帶」、「可擴展性」的部署，在不同基礎設施上(Local Laptop、GPU Server cluster、Cloud Production Cluster) 簡述Kubelfow工作流程： 下載Kubeflow shell script 和 配置文件 自定義 ksonnet部署所需的JSON組件 執行shell script將容器部署到所選擇的Kubernetes Enveroment 開始執行機器學習訓練 我們透過Kubeflow工具管理，可以解決ML環境配置困難的問題，其中在Kubeflow提供Operator，對不同的機器學習框架，擴展的 Kubernetes API 進行自動建立、管理與配置應用程式容器實例，並提供訓練在不同環境和服務，包含數據準備、模型培訓、預測服務和服務管理。 Kubernetes 有提供不同集群部署支援，包括單節點Minikube部署、 透過Hypervisor部署的Multipass &amp; Microk8s、Cloud。無論你在哪裡運行Kubernetes，你都能夠運行Kubeflow。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Destop，測試環境為實體機器部署HA Cluster： IP Address Role vCPU RAM Extra Device 172.22.132.98 VIP 172.22.132.81 k8s-m1 8 16G GTX 1060 6G 172.22.132.82 k8s-m2 8 16G GTX 1060 6G 172.22.132.83 k8s-m3 8 16G GTX 1060 6G 172.22.132.84 k8s-g1 8 16G GTX 1060 6G 172.22.132.97 k8s-g2 8 16G GTX 1060 6G 172.22.132.86 k8s-g3 8 16G GTX 1060 6G 172.22.132.94 k8s-g11 8 16G GTX 1060 6G kubernetes部署可以參考 開發 Ansible Playbooks 部署 Kubernetes v1.11.x HA 叢集教學 本次部署使用版本 kubernetes v1.11.2 Master x 3, Worker x 4 ksonnet 0.13.0 kubeflow v0.3.0 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點確認已經安裝 NVIDIA driver、CUDA、Docker、NVIDIA Docker，以下為實驗環境版本資訊： CUDA Version: 10.0.130 Driver Version: 410.48 Docker Version: 18.03.0-ce NVIDIA Docker: 2.0.3 安裝 NFS設置Kubeflow需要Volumes來存儲數據，建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： 1234567891011# 在 master 執行，這邊nfs-server建立在k8s-m1節點$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server$ sudo mkdir /nfs-data# 可以在 `nfs-data`資料夾建立多個pv資料夾，提供kubeflow儲存使用$ cd /nfs-data$ mkdir user1 user2 ... $ echo \"/nfs-data *(rw,sync,no_root_squash,no_subtree_check)\" | sudo tee -a /etc/exports$ sudo /etc/init.d/nfs-kernel-server restart# 在 node 執行$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 這邊也可以使用其他存儲數據方式: Kubernetes: Local Volume 安裝 KsonnetKsonnet 是一個命令工具，可以更輕鬆地管理由多個組件組成的複雜部署，簡化編寫和部署 Kubernetes 配置。 123456789$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.0/ks_0.13.0_linux_amd64.tar.gz$ tar xvf ks_0.13.0_linux_amd64.tar.gz$ sudo cp ks_0.13.0_linux_amd64 /usr/local/bin/$ chmod +x ks$ ks versionksonnet version: 0.13.0jsonnet version: v0.11.2client-go version: kubernetes-1.10.4 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。部署版本選擇為kubeflow v0.3.0。 下載 Kubeflow 腳本下載GitHub中Kubeflow設置所需配置： 12345678910# 建立配置指定資料夾路徑，可自行更換名稱$ KUBEFLOW_SRC=mykfsrc $ mkdir $&#123;KUBEFLOW_SRC&#125;$ cd $&#123;KUBEFLOW_SRC&#125;# 部署kubeflow版本標籤，可自行選擇其他分支$ export KUBEFLOW_TAG=v0.3.0 # 執行shell script$ curl https://raw.githubusercontent.com/kubeflow/kubeflow/$&#123;KUBEFLOW_TAG&#125;/scripts/download.sh | bash 下載後目錄下會多kubeflow、scripts資料夾: 123$ tree -L 1 ├── kubeflow└── scripts 執行腳本配置和部署Kubeflow123456789101112131415161718# 退回上一層建立對應路徑$ cd ../$ KUBEFLOW_REPO=$(pwd)/mykfsrc# $&#123;KFAPP&#125; 為配置 kubeflow 儲存目錄的名稱，執行init時將創建此目錄# ksonnet應用程式將在 $&#123;KFAPP&#125;/ks_app目錄中創建$ KFAPP=mykfapp$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh init $&#123;KFAPP&#125; --platform none# 安裝 Kubeflow 套件至應用程式目錄$ cd $&#123;KFAPP&#125;$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh generate k8s# 定義 Namespace 為kubeflow來管理相關資源$ kubectl create ns kubeflow# 部署 Kubeflow$ $&#123;KUBEFLOW_REPO&#125;/scripts/kfctl.sh apply k8s ！部署預設為default，這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 完成後檢查 Kubeflow 元件部署結果：12345678910111213141516171819$ kubectl -n kubeflow get po -o wide ambassador-868d5dbdc4-5fgdw 3/3 Running 0 7m 10.244.2.90 k8s-g3 &lt;none&gt;ambassador-868d5dbdc4-qcp5z 3/3 Running 1 7m 10.244.5.78 k8s-g11 &lt;none&gt;ambassador-868d5dbdc4-x9bfc 3/3 Running 0 7m 10.244.3.96 k8s-g2 &lt;none&gt;argo-ui-84464bd59c-hmhmd 1/1 Running 0 7m 10.244.4.89 k8s-g1 &lt;none&gt;centraldashboard-c76877875-nqbhg 1/1 Running 0 7m 10.244.3.97 k8s-g2 &lt;none&gt;modeldb-backend-58969447f6-z2xmf 1/1 Running 1 11s 10.244.3.102 k8s-g2 &lt;none&gt;modeldb-db-57b855f5b7-dw5s6 1/1 Running 0 7m 10.244.5.79 k8s-g11 &lt;none&gt;modeldb-frontend-769d5bdd66-njkqs 1/1 Running 0 7m 10.244.3.99 k8s-g2 &lt;none&gt;spartakus-volunteer-5bfd5876fb-b86h5 1/1 Running 0 7m 10.244.2.91 k8s-g3 &lt;none&gt;studyjob-controller-56588dc6f9-shg72 1/1 Running 0 7m 10.244.3.101 k8s-g2 &lt;none&gt;tf-hub-0 1/1 Running 0 7m 10.244.4.86 k8s-g1 &lt;none&gt;tf-job-dashboard-7777b6bf-qltfc 1/1 Running 0 7m 10.244.3.98 k8s-g2 &lt;none&gt;tf-job-operator-v1alpha2-5c5b4dcfdf-x5rkv 1/1 Running 0 7m 10.244.4.87 k8s-g1 &lt;none&gt;vizier-core-5584ccbd8-l2bxr 0/1 CrashLoopBackOff 4 7m 10.244.5.80 k8s-g11 &lt;none&gt;vizier-db-547967c899-mf988 0/1 Pending 0 7m &lt;none&gt; &lt;none&gt; &lt;none&gt;vizier-suggestion-grid-8547dbb55b-k8tw6 1/1 Running 0 7m 10.244.4.90 k8s-g1 &lt;none&gt;vizier-suggestion-random-d7c5cd68b-7p9nz 1/1 Running 0 7m 10.244.3.100 k8s-g2 &lt;none&gt;workflow-controller-5c95f95f58-d9plp 1/1 Running 0 7m 10.244.4.88 k8s-g1 &lt;none&gt; 可以看到vizier狀態為Pending，失敗的原因是PersistentVolume未分配給vizer-db（實際是MySQL）。所以vizier-db正在等待並且未能啟動vizier-core。 這邊透過使用kubectl describe命令檢查原因: 123456789$ kubectl -n kubeflow describe pod vizier-db-547967c899-mf988Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m (x417 over 12m) default-scheduler pod has unbound PersistentVolumeClaims (repeated 4 times)$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Pending 15m 這邊我們建立一個 NFS PV 來提供給 vizier-db 使用：1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: vizier-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv1EOF 請更換自己的 nfs-server IP 再次查看 PV &amp; PVC 狀態，就可以看到vizier-db獲得 20Gi 的 ersistentVolume 空間 1234567$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEvizier-pv 20Gi RWO Retain Bound kubeflow/vizier-db 16s$ kubectl -n kubeflow get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvizier-db Bound vizier-pv 20Gi RWO 20m 其他儲存方式參考：Kubernetes: Local Volumeの検証 暫時問題：modeldb-backend 會不定時 Crash Kubeflow UI查看Service中，可以看到kubeflow v0.3.0版本中，附帶了許多Web UI: Argo UI Central UI for navigation JupyterHub Katib TFJobs Dashboard 這邊我們透過Service查看一下目前pod狀態： 123456789101112131415161718$ kubectl -n kubeflow get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador ClusterIP 10.98.216.151 &lt;none&gt; 80/TCP 7m service=ambassadorambassador-admin ClusterIP 10.102.20.172 &lt;none&gt; 8877/TCP 6m service=ambassadorargo-ui NodePort 10.102.36.121 &lt;none&gt; 80:32530/TCP 6m app=argo-uicentraldashboard ClusterIP 10.99.255.3 &lt;none&gt; 80/TCP 6m app=centraldashboardk8s-dashboard ClusterIP 10.110.209.78 &lt;none&gt; 443/TCP 6m k8s-app=kubernetes-dashboardmodeldb-backend ClusterIP 10.102.179.187 &lt;none&gt; 6543/TCP 6m app=modeldb,component=backendmodeldb-db ClusterIP 10.102.143.161 &lt;none&gt; 27017/TCP 6m app=modeldb,component=dbmodeldb-frontend ClusterIP 10.100.76.92 &lt;none&gt; 3000/TCP 6m app=modeldb,component=frontendstatsd-sink ClusterIP 10.98.167.5 &lt;none&gt; 9102/TCP 7m service=ambassadortf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 6m app=tf-hubtf-hub-lb ClusterIP 10.106.99.192 &lt;none&gt; 80/TCP 6m app=tf-hubtf-job-dashboard ClusterIP 10.100.66.145 &lt;none&gt; 80/TCP 6m name=tf-job-dashboardvizier-core NodePort 10.105.154.126 &lt;none&gt; 6789:30678/TCP 6m app=vizier,component=corevizier-db ClusterIP 10.100.32.162 &lt;none&gt; 3306/TCP 6m app=vizier,component=dbvizier-suggestion-grid ClusterIP 10.105.68.247 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-gridvizier-suggestion-random ClusterIP 10.110.164.76 &lt;none&gt; 6789/TCP 6m app=vizier,component=suggestion-random 這時候就可以透過kubeflow UI，能引導到 登入JupyterHub，但這邊需要修改 Kubernetes Service，透過以下指令進行： 123456789101112# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。$ kubectl -n kubeflow edit svc ambassador...sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125;...#修改後就可以透過NodePort接上Port進入顯示Jupyter NotebookNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORambassador NodePort 10.98.216.151 &lt;none&gt; 80:32752/TCP 15m service=ambassador 完成後連接 http://Master_IP:Port即可看到kubeflow UI。 可以透過上方連結查看Jupyter Hub、TFjob Dashboard、Kubernetes Dashboard 等。 測試 Jupyter Notebook這邊需要再次先建立一個 NFS PV 來提供給 Kubeflow Jupyter使用： 1234567891011121314$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: notebook-pvspec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.81 path: /nfs-data/kubeflow-pv2EOF NFS PV 建立好後這邊UI點選Jupyter Notebook登入測試，並輸入任意帳號密碼進行登入： Spawner Optinos 這邊可以輸入Notebook Pod節點資源，包含Image、CPU、Memory、GPU數量，最後點選Spawn來完成建立 Server，如下圖所示： kubeflow v0.3版本開始加入，在部署Jupyter Hub時的Event log方便了解部署狀態 部署完後即可直接操作擁有GPU的Jupyter Notebook 查看GPU資源狀態可以透過執行以下指令確認是否GPU可被分配資源：123456789$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"NAME GPUk8s-g1 1k8s-g11 1k8s-g2 1k8s-g3 1k8s-m1 &lt;none&gt;k8s-m2 &lt;none&gt;k8s-m3 &lt;none&gt; 後續有陸續關注，關於 kubeflow CLI的專案kubeflow/Arena，主要供資料科學家運行和監控機器學習培訓工作並以簡單的方式檢查其結果。目前它支持單獨/分佈式TensorFlow訓練。部署基於Kubernetes，helm和Kubeflow。 同時，最終用戶需要GPU資源和節點管理。Arena還提供top檢查Kubernetes集群中可用GPU資源的命令。 123456789101112$ arena top node NAME IPADDRESS ROLE GPU(Total) GPU(Allocated)k8s-g1 172.22.132.84 &lt;none&gt; 1 0k8s-g11 172.22.132.94 &lt;none&gt; 1 0k8s-g2 172.22.132.97 &lt;none&gt; 1 0k8s-g3 172.22.132.86 &lt;none&gt; 1 1k8s-m1 172.22.132.81 master 0 0k8s-m2 172.22.132.82 master 0 0k8s-m3 172.22.132.83 master 0 0-----------------------------------------------------------------------------------------Allocated/Total GPUs In Cluster:1/5 (20%) 透過CLI方式來 watch -n1 arena top node就能很清楚知道節點資源使用狀況。 移除 kubeflow這邊部署有特別給kubeflow 命名空間，如果要移除kubeflow，直接刪除kubeflow Namespaces 時，所有物件也會被刪除 1kubectl -n kubeflow delete namespace kubeflow 相關文章:[kubeflow] Local Volumeを使ったKubeflowの動作検証[kubeflow] Microk8s for Kubeflow #! 更多相關kubeflow Components實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]},{"title":"透過Microk8s快速部署kubeflow","slug":"microk8s-deploy-kubeflow","date":"2018-09-17T04:12:02.000Z","updated":"2018-10-23T13:07:05.923Z","comments":true,"path":"2018/09/17/microk8s-deploy-kubeflow/","link":"","permalink":"http://yylin1.github.io/2018/09/17/microk8s-deploy-kubeflow/","excerpt":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。","text":"此文章將記錄透過安裝Microk8s，在本地機器上輕鬆部署kubernetes 集群，並且透過運行腳本讓kubeflow環境一起部署完成。最後您將擁有簡易的單節點K8S集群以及在Pod中部署為服務的Kubeflow的所有默認核心組件。並且能訪問JupyterHub筆記本和Kubeflow Dashboard，進行kubeflow相關測試。 什麼是microk8s 簡單來說，microk8s設計為快速輕巧Kubernetes最新版本安裝與主機隔離但不通過虛擬機。通過在單個快照包snap中打包 Kubernetes、Docker.io、iptables和CNI的所有上游二進製文件來實現此隔離。snap是一個應用程序容器，您可以將其想像為Docker容器的輕量級版本。它使用了許多相同的底層技術進行隔離，而不會有網絡隔離的所帶來的開銷。而minikube需使用虛擬化工具來環境隔離創建K8S集群，想必會快照更加耗時。 安裝MultipassMac OS X. (本篇文章以Mac環境進行)) 本機Mac OS 安裝程序安裝Multipass Linux OS 透過snap使用指令安裝 1$ sudo snap install multipass --beta --classic 啟動Ubuntu虛擬機部署前要先下載cloud-init文件1wget https://bit.ly/2tOfMUA -O kubeflow.init 這邊查看一下文件內容，主要是配置kubeflow環境並去執行kubeflow 已經寫好script能更快速部署，有需要修正自己的環境可以修改後執行123456789101112# cloud-init for kubeflowpackage_update: truepackage_upgrade: trueruncmd: - mkdir /kubeflow - wget https://bit.ly/2tp2aOo -O /kubeflow/install-kubeflow-pre-micro.sh - chmod a+x /kubeflow/install-kubeflow-pre-micro.sh - wget https://bit.ly/2tndL0g -O /kubeflow/install-kubeflow.sh - chmod a+x /kubeflow/install-kubeflow.sh - printf \"\\n\\nexport KUBECONFIG=/snap/microk8s/current/client.config\\n\\n\" &gt;&gt; /home/multipass/.bashrc 啟動Multipass VM123456$ multipass launch bionic -n kubeflow -m 8G -d 40G -c 4 --cloud-init kubeflow.initRetrieving image: 66%Retrieving image: 87%Verifying image: -Launched: kubeflow 預設備至Multipass為Kubeflow部署創建的VM上的最低建議設置，這邊是可以自由地調整根據主機的能力和工作負載需求。 依據環境狀況應該很快速就建立好單節點集群，這邊可以查看一下123$ multipass list Name State IPv4 Releasekubeflow RUNNING 192.168.64.2 Ubuntu 18.04 LTS 如果部署有問題隨時都可以透過multipass -h查看 接下來就mount本地文件與multipass配置1$ multipass mount . kubeflow:/multipass 安裝kubernetes接下來我們就可以進入vm，安裝由microk8s驅動的kubernetes，以及部署Kubeflow所需的其他工具。 透過multipass進入vm12345678910111213141516171819202122$ multipass shell kubeflowWelcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.15.0-34-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Mon Sep 17 09:03:39 CST 2018 System load: 0.63 Processes: 153 Usage of /: 2.9% of 38.60GB Users logged in: 0 Memory usage: 2% IP address for enp0s2: 192.168.64.2 Swap usage: 0% Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.multipass@kubeflow:~$ 安裝microk8s、etc1234567891011121314151617$ sudo /kubeflow/install-kubeflow-pre-micro.shSaving to: ‘ksonnet.tar.gz’ksonnet.tar.gz 100%[============================================================================================&gt;] 14.77M 934KB/s in 44s2018-09-17 09:22:01 (342 KB/s) - ‘ksonnet.tar.gz’ saved [15491217/15491217]ks_0.11.0_linux_amd64/CHANGELOG.mdks_0.11.0_linux_amd64/CODE-OF-CONDUCT.mdks_0.11.0_linux_amd64/CONTRIBUTING.mdks_0.11.0_linux_amd64/LICENSEks_0.11.0_linux_amd64/README.mdks_0.11.0_linux_amd64/ksChecking kube-system status until all pods are running (2 not running)Checking kube-system status until all pods are running (3 not running)Before running install-kubeflow.sh, please 'export GITHUB_TOKEN=&lt;your token&gt;' 這邊提供GitHub Token是為了避免ksonnet部署反覆部署可能會造成GitHub限速問題，可以參考建立Token。 更換自己的Token1$ export GITHUB_TOKEN=972702e4fb348d3dba52f5dd3b99d7ae83e857ab kubeflow 部署在vm環境中執行kubeflow script，就可以快速完成kubeflow部署，而這邊會有Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.狀況就依照環境是否把所有應該生成的Pod建立完成，就執行結束。 1234567891011121314$ multipass@kubeflow:~$ /kubeflow/install-kubeflow.shnamespace/kubeflow createdINFO Using context \"microk8s\" from kubeconfig file \"/snap/microk8s/current/client.config\"INFO Creating environment \"default\" with namespace \"default\", pointing to cluster at address \"http://127.0.0.1:8080\"INFO Generating ksonnet-lib data at path '/home/multipass/my-kubeflow/lib/v1.11.1'INFO Retrieved 22 filesINFO Retrieved 5 filesINFO Retrieved 5 files···Checking kubeflow status until all pods are running (7 not running). Sleeping for 10 seconds.JupyterHub Port: 31808 以上就完成microk8s+kubeflow部署 部署完成後環境完成後檢查 Kubeflow 元件部署結果： 12345678910multipass@kubeflow:~$ kubectl -n kubeflow get po -o wideNAME READY STATUS RESTARTS AGE IP NODEambassador-68954d75f4-2n6lv 2/2 Running 4 4h 10.1.1.55 kubeflowambassador-68954d75f4-7dgvl 2/2 Running 4 4h 10.1.1.56 kubeflowambassador-68954d75f4-bgs8h 2/2 Running 5 4h 10.1.1.44 kubeflowmxnet-operator-f46557c4f-wmntn 1/1 Running 2 3h 10.1.1.52 kubeflowspartakus-volunteer-d54b65666-klrfk 1/1 Running 2 4h 10.1.1.45 kubeflowtf-hub-0 1/1 Running 2 4h 10.1.1.47 kubeflowtf-job-dashboard-784cdcbb4f-j2cjx 1/1 Running 2 4h 10.1.1.51 kubeflowtf-job-operator-85b46d47b7-xfwmp 1/1 Running 2 4h 10.1.1.48 kubeflow 運行JupyterHub測試透過查看vm狀態中，完成後連接 http://IP:Port，並輸入任意帳號密碼進行登入。 Spawner options可以自行配置Jupyter Notebook 資源狀態 Image: 預設會有多種映像檔可以使用其他參數可調整 CPU、Memory、GPU資源限制 按下Spawn kubeflow就會啟動一個pod配置資源給Jupyter notebook，部署過程時Image下載需要花一點時間。 配置過程中有時會後有狀況，可能需要多嘗試 完成後即可在透過Jupyter Notebook 編寫Model進行DL訓練 Kubernetes Addonsmicrok8s在Kubernetes安裝了一個準系統。這代表只要安裝和運行api-server、controller-manager、scheduler、kubelet，cni、kube-proxy。可以使用該microk8s.enable命令運行kube-dns和Dashboard等附加服務。 1microk8s.enable dns dashboard 使用該disable命令隨時禁用這些插件1microk8s.disable dashboard dns 12345678$ microk8s.kubectl cluster-infoKubernetes master is running at http://127.0.0.1:8080Heapster is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/heapster/proxyKubeDNS is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyGrafana is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxyInfluxDB is running at http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:http/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 停止並重新啟動microk8 (測試環境為Mac) 在 Linx 環境指令都是偷過snap enable/disable 來管理microk8s Mac\b\b環境可以透過multipass -h查詢可執行指令 暫時關閉vm運行1multipass stop kubeflow 啟動vm運行1multipass stop kubeflow 查看multipass目前vm狀態12Name State IPv4 Releasekubeflow STOPPED -- Ubuntu 18.04 LTS 刪除microk8s透過multipass list 檢查instances狀態 如果只下delete只有instances狀態刪除，是可以透過recover恢復已刪除的instances123multipass delete kubeflowName State IPv4 Releasekubeflow DELETED -- Not Available 所以我們要透過purge來清除永久清除所有已刪除的instances1multipass purge 相關文章:[Kubernetes] A local Kubernetes with microk8s[kubeflow] Microk8s for Kubeflow 本篇文章主介紹microk8s部署kubeflow，更多相關kubeflow 實作會在後續文章陸續介紹","categories":[{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/categories/kubeflow/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yylin1.github.io/tags/kubernetes/"},{"name":"microk8s","slug":"microk8s","permalink":"http://yylin1.github.io/tags/microk8s/"},{"name":"kubeflow","slug":"kubeflow","permalink":"http://yylin1.github.io/tags/kubeflow/"},{"name":"deploy","slug":"deploy","permalink":"http://yylin1.github.io/tags/deploy/"}]}]}